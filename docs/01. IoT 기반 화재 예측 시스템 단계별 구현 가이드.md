# IoT 기반 화재 예측 시스템 단계별 구현 가이드

## Phase 1: Isolation Forest로 기본 이상치 탐지 시스템 구축

### 1.1 시스템 아키텍처

```
MQTT Broker → Data Collector → Real-time Processing → Anomaly Detection → Alert System
```

### 1.2 데이터 수집 및 전처리

```python
import paho.mqtt.client as mqtt
import pandas as pd
import numpy as np
from datetime import datetime
import json
from collections import deque
from sklearn.ensemble import IsolationForest
from sklearn.preprocessing import StandardScaler
import joblib

class IoTDataCollector:
    def __init__(self, broker_address, topic_prefix="iot/+/data"):
        self.broker = broker_address
        self.topic = topic_prefix
        self.data_buffer = deque(maxlen=1000)  # 최근 1000개 데이터 보관
        self.scaler = StandardScaler()
        
    def on_message(self, client, userdata, message):
        """MQTT 메시지 수신 처리"""
        try:
            payload = json.loads(message.payload.decode())
            device_id = message.topic.split('/')[1]
            
            # 데이터 파싱
            data_point = {
                'timestamp': datetime.now(),
                'device_id': device_id,
                'voltage': payload.get('voltage', 0),
                'current': payload.get('current', 0),
                'leakage_current': payload.get('leakage_current', 0),
                'temperature': payload.get('temperature', 0),
                'co_gas': payload.get('co_gas', 0),
                'voc': payload.get('voc', 0),
                'power_factor': payload.get('power_factor', 0),
                'arc_count': payload.get('arc_count', 0)
            }
            
            self.data_buffer.append(data_point)
            self.process_data(data_point)
            
        except Exception as e:
            print(f"Error processing message: {e}")
    
    def process_data(self, data_point):
        """실시간 데이터 처리"""
        # 특징 엔지니어링
        features = self.extract_features(data_point)
        
        # 이상치 탐지
        if hasattr(self, 'isolation_model'):
            anomaly_score = self.detect_anomaly(features)
            if anomaly_score < -0.5:  # 이상치 임계값
                self.trigger_alert(data_point, anomaly_score)
```

### 1.3 특징 엔지니어링

```python
class FeatureEngineering:
    def __init__(self, window_size=10):
        self.window_size = window_size
        self.history = deque(maxlen=window_size)
    
    def extract_features(self, data_points):
        """시계열 특징 추출"""
        df = pd.DataFrame(data_points)
        
        features = {}
        
        # 기본 통계 특징
        for col in ['voltage', 'current', 'temperature', 'leakage_current']:
            if col in df.columns:
                features[f'{col}_mean'] = df[col].mean()
                features[f'{col}_std'] = df[col].std()
                features[f'{col}_max'] = df[col].max()
                features[f'{col}_min'] = df[col].min()
                features[f'{col}_range'] = features[f'{col}_max'] - features[f'{col}_min']
        
        # 변화율 특징
        if len(df) > 1:
            features['temp_change_rate'] = (df['temperature'].iloc[-1] - df['temperature'].iloc[0]) / len(df)
            features['current_change_rate'] = (df['current'].iloc[-1] - df['current'].iloc[0]) / len(df)
        
        # 복합 특징
        features['power'] = df['voltage'].mean() * df['current'].mean()
        features['resistance'] = df['voltage'].mean() / (df['current'].mean() + 1e-6)
        
        # 위험 지표
        features['arc_density'] = df['arc_count'].sum() / len(df)
        features['gas_level'] = df[['co_gas', 'voc']].max().max()
        
        return features
```

### 1.4 Isolation Forest 모델 구현

```python
class AnomalyDetector:
    def __init__(self, contamination=0.01):
        self.model = IsolationForest(
            contamination=contamination,
            random_state=42,
            n_estimators=100
        )
        self.scaler = StandardScaler()
        self.is_trained = False
        
    def train(self, training_data):
        """정상 데이터로 모델 학습"""
        # 특징 추출
        feature_extractor = FeatureEngineering()
        features_list = []
        
        for i in range(len(training_data) - 10):
            window_data = training_data[i:i+10]
            features = feature_extractor.extract_features(window_data)
            features_list.append(list(features.values()))
        
        X = np.array(features_list)
        
        # 정규화
        X_scaled = self.scaler.fit_transform(X)
        
        # 모델 학습
        self.model.fit(X_scaled)
        self.is_trained = True
        
        # 모델 저장
        joblib.dump(self.model, 'isolation_forest_model.pkl')
        joblib.dump(self.scaler, 'scaler.pkl')
        
    def predict_anomaly(self, features):
        """실시간 이상치 탐지"""
        if not self.is_trained:
            return 0
        
        # 특징 정규화
        features_array = np.array(list(features.values())).reshape(1, -1)
        features_scaled = self.scaler.transform(features_array)
        
        # 이상치 점수 계산 (-1: 이상치, 1: 정상)
        anomaly_score = self.model.decision_function(features_scaled)[0]
        
        return anomaly_score
```

### 1.5 실시간 모니터링 시스템

```python
class FirePredictionSystem:
    def __init__(self):
        self.collector = IoTDataCollector("localhost")
        self.detector = AnomalyDetector()
        self.alert_manager = AlertManager()
        
    def start_monitoring(self):
        """실시간 모니터링 시작"""
        # MQTT 클라이언트 설정
        client = mqtt.Client()
        client.on_message = self.on_message
        client.connect(self.collector.broker, 1883)
        client.subscribe("iot/+/data")
        
        # 모니터링 시작
        client.loop_forever()
        
    def on_message(self, client, userdata, message):
        """메시지 처리 및 이상치 탐지"""
        # 데이터 수집
        self.collector.on_message(client, userdata, message)
        
        # 윈도우 데이터로 특징 추출
        if len(self.collector.data_buffer) >= 10:
            recent_data = list(self.collector.data_buffer)[-10:]
            features = FeatureEngineering().extract_features(recent_data)
            
            # 이상치 탐지
            anomaly_score = self.detector.predict_anomaly(features)
            
            # 경보 처리
            if anomaly_score < -0.5:
                self.alert_manager.send_alert(
                    level="WARNING",
                    score=anomaly_score,
                    data=recent_data[-1],
                    features=features
                )
```

## Phase 2: LSTM 모델 추가로 시계열 예측 강화

### 2.1 LSTM 모델 설계

```python
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint

class LSTMPredictor:
    def __init__(self, sequence_length=30, n_features=8):
        self.sequence_length = sequence_length
        self.n_features = n_features
        self.model = self.build_model()
        
    def build_model(self):
        """LSTM 모델 구조 정의"""
        model = Sequential([
            LSTM(128, return_sequences=True, 
                 input_shape=(self.sequence_length, self.n_features)),
            Dropout(0.2),
            LSTM(64, return_sequences=True),
            Dropout(0.2),
            LSTM(32),
            Dropout(0.2),
            Dense(16, activation='relu'),
            Dense(1, activation='sigmoid')  # 0-1 사이의 화재 위험도
        ])
        
        model.compile(
            optimizer='adam',
            loss='binary_crossentropy',
            metrics=['accuracy', 'AUC']
        )
        
        return model
    
    def prepare_sequences(self, data, labels=None):
        """시계열 데이터를 LSTM 입력 형태로 변환"""
        sequences = []
        targets = []
        
        for i in range(len(data) - self.sequence_length):
            seq = data[i:i+self.sequence_length]
            sequences.append(seq)
            
            if labels is not None:
                # 향후 10분 내 화재 발생 여부
                target = labels[i+self.sequence_length:i+self.sequence_length+10].max()
                targets.append(target)
        
        return np.array(sequences), np.array(targets) if labels is not None else None
```

### 2.2 데이터 준비 및 학습

```python
class DataPreprocessor:
    def __init__(self):
        self.scaler = StandardScaler()
        self.feature_columns = ['voltage', 'current', 'leakage_current', 
                               'temperature', 'co_gas', 'voc', 
                               'power_factor', 'arc_count']
        
    def prepare_training_data(self, df, fire_events):
        """학습 데이터 준비"""
        # 특징 정규화
        features = df[self.feature_columns].values
        features_scaled = self.scaler.fit_transform(features)
        
        # 화재 라벨 생성 (화재 발생 10분 전부터 1로 표시)
        labels = np.zeros(len(df))
        for event_time in fire_events:
            event_idx = df[df['timestamp'] == event_time].index[0]
            start_idx = max(0, event_idx - 600)  # 10분 전 (1초 간격 가정)
            labels[start_idx:event_idx] = 1
        
        return features_scaled, labels
    
    def augment_fire_data(self, X, y):
        """화재 데이터 증강 (불균형 해결)"""
        fire_indices = np.where(y == 1)[0]
        normal_indices = np.where(y == 0)[0]
        
        # SMOTE 대신 간단한 오버샘플링
        n_fire = len(fire_indices)
        n_normal = len(normal_indices)
        
        if n_fire < n_normal * 0.3:  # 화재 데이터가 30% 미만인 경우
            # 화재 데이터 복제
            fire_X = X[fire_indices]
            fire_y = y[fire_indices]
            
            # 노이즈 추가하여 증강
            augmented_X = []
            augmented_y = []
            
            for _ in range(3):  # 3배 증강
                noise = np.random.normal(0, 0.01, fire_X.shape)
                augmented_X.append(fire_X + noise)
                augmented_y.append(fire_y)
            
            X_augmented = np.vstack([X] + augmented_X)
            y_augmented = np.hstack([y] + augmented_y)
            
            return X_augmented, y_augmented
        
        return X, y
```

### 2.3 통합 예측 시스템

```python
class IntegratedPredictionSystem:
    def __init__(self):
        self.isolation_forest = AnomalyDetector()
        self.lstm_predictor = LSTMPredictor()
        self.preprocessor = DataPreprocessor()
        self.prediction_buffer = deque(maxlen=30)
        
    def train_models(self, historical_data, fire_events):
        """모델 학습"""
        # Isolation Forest 학습
        print("Training Isolation Forest...")
        self.isolation_forest.train(historical_data)
        
        # LSTM 학습
        print("Preparing LSTM training data...")
        X, y = self.preprocessor.prepare_training_data(historical_data, fire_events)
        X_seq, y_seq = self.lstm_predictor.prepare_sequences(X, y)
        
        # 데이터 증강
        X_aug, y_aug = self.preprocessor.augment_fire_data(X_seq, y_seq)
        
        # 학습/검증 데이터 분할
        split_idx = int(len(X_aug) * 0.8)
        X_train, X_val = X_aug[:split_idx], X_aug[split_idx:]
        y_train, y_val = y_aug[:split_idx], y_aug[split_idx:]
        
        # LSTM 학습
        print("Training LSTM...")
        callbacks = [
            EarlyStopping(patience=10, restore_best_weights=True),
            ModelCheckpoint('lstm_model.h5', save_best_only=True)
        ]
        
        history = self.lstm_predictor.model.fit(
            X_train, y_train,
            epochs=50,
            batch_size=32,
            validation_data=(X_val, y_val),
            callbacks=callbacks,
            verbose=1
        )
        
        return history
    
    def predict_fire_risk(self, current_data):
        """실시간 화재 위험도 예측"""
        # 1. Isolation Forest 이상치 점수
        features = FeatureEngineering().extract_features(current_data[-10:])
        anomaly_score = self.isolation_forest.predict_anomaly(features)
        
        # 2. LSTM 예측 (충분한 데이터가 있을 때)
        lstm_risk = 0
        if len(self.prediction_buffer) >= 30:
            # 데이터 준비
            seq_data = np.array(list(self.prediction_buffer))
            seq_scaled = self.preprocessor.scaler.transform(seq_data)
            seq_input = seq_scaled.reshape(1, 30, 8)
            
            # LSTM 예측
            lstm_risk = self.lstm_predictor.model.predict(seq_input)[0][0]
        
        # 3. 종합 위험도 계산
        # Isolation Forest 점수를 0-1로 변환
        if_risk = 1 / (1 + np.exp(anomaly_score * 2))
        
        # 가중 평균
        if lstm_risk > 0:
            total_risk = 0.4 * if_risk + 0.6 * lstm_risk
        else:
            total_risk = if_risk
        
        return {
            'total_risk': total_risk,
            'isolation_forest_risk': if_risk,
            'lstm_risk': lstm_risk,
            'anomaly_score': anomaly_score
        }
```

## Phase 3: 앙상블 모델로 정확도 향상

### 3.1 다중 모델 앙상블 시스템

```python
import xgboost as xgb
from sklearn.ensemble import RandomForestClassifier
from tensorflow.keras.layers import GRU

class EnsembleFirePredictionSystem:
    def __init__(self):
        self.models = {
            'isolation_forest': AnomalyDetector(),
            'lstm': LSTMPredictor(sequence_length=30),
            'gru': self.build_gru_model(),
            'random_forest': RandomForestClassifier(n_estimators=100),
            'xgboost': xgb.XGBClassifier()
        }
        self.meta_model = self.build_meta_model()
        
    def build_gru_model(self):
        """GRU 모델 구축 (단기 예측용)"""
        model = Sequential([
            GRU(64, return_sequences=True, input_shape=(15, 8)),
            Dropout(0.2),
            GRU(32),
            Dense(16, activation='relu'),
            Dense(1, activation='sigmoid')
        ])
        
        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
        return model
    
    def build_meta_model(self):
        """메타 학습기 (스태킹)"""
        model = Sequential([
            Dense(10, activation='relu', input_shape=(5,)),  # 5개 모델의 예측값
            Dropout(0.3),
            Dense(5, activation='relu'),
            Dense(1, activation='sigmoid')
        ])
        
        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
        return model
    
    def extract_ensemble_features(self, data_window):
        """앙상블을 위한 다양한 특징 추출"""
        features = {}
        
        # 시계열 통계 특징
        df = pd.DataFrame(data_window)
        
        # 1. 기본 통계
        for col in df.select_dtypes(include=[np.number]).columns:
            features[f'{col}_mean'] = df[col].mean()
            features[f'{col}_std'] = df[col].std()
            features[f'{col}_skew'] = df[col].skew()
            features[f'{col}_kurt'] = df[col].kurtosis()
        
        # 2. 시계열 특징
        features['trend_strength'] = self.calculate_trend_strength(df['temperature'])
        features['seasonality'] = self.calculate_seasonality(df['current'])
        
        # 3. 교차 상관 특징
        features['temp_current_corr'] = df['temperature'].corr(df['current'])
        features['voltage_current_corr'] = df['voltage'].corr(df['current'])
        
        # 4. 주파수 도메인 특징 (FFT)
        for col in ['current', 'voltage']:
            fft_vals = np.fft.fft(df[col].values)
            features[f'{col}_fft_max'] = np.max(np.abs(fft_vals))
            features[f'{col}_fft_mean'] = np.mean(np.abs(fft_vals))
        
        # 5. 위험 지표
        features['max_temp'] = df['temperature'].max()
        features['arc_total'] = df['arc_count'].sum()
        features['gas_max'] = df[['co_gas', 'voc']].max().max()
        
        return features
    
    def predict_ensemble(self, data_sequence):
        """앙상블 예측"""
        predictions = {}
        
        # 1. Isolation Forest
        if_features = self.extract_ensemble_features(data_sequence[-10:])
        if_score = self.models['isolation_forest'].predict_anomaly(if_features)
        predictions['isolation_forest'] = 1 / (1 + np.exp(if_score * 2))
        
        # 2. LSTM (30 timesteps)
        if len(data_sequence) >= 30:
            lstm_input = self.prepare_lstm_input(data_sequence[-30:])
            predictions['lstm'] = self.models['lstm'].model.predict(lstm_input)[0][0]
        else:
            predictions['lstm'] = 0.5
        
        # 3. GRU (15 timesteps)
        if len(data_sequence) >= 15:
            gru_input = self.prepare_gru_input(data_sequence[-15:])
            predictions['gru'] = self.models['gru'].predict(gru_input)[0][0]
        else:
            predictions['gru'] = 0.5
        
        # 4. Random Forest
        rf_features = list(self.extract_ensemble_features(data_sequence[-20:]).values())
        predictions['random_forest'] = self.models['random_forest'].predict_proba([rf_features])[0][1]
        
        # 5. XGBoost
        predictions['xgboost'] = self.models['xgboost'].predict_proba([rf_features])[0][1]
        
        # 메타 모델로 최종 예측
        meta_input = np.array(list(predictions.values())).reshape(1, -1)
        final_prediction = self.meta_model.predict(meta_input)[0][0]
        
        return {
            'final_risk': final_prediction,
            'model_predictions': predictions,
            'confidence': self.calculate_confidence(predictions)
        }
    
    def calculate_confidence(self, predictions):
        """예측 신뢰도 계산"""
        pred_values = list(predictions.values())
        # 모델 간 일치도 기반 신뢰도
        std_dev = np.std(pred_values)
        confidence = 1 - (std_dev * 2)  # 표준편차가 작을수록 높은 신뢰도
        return max(0, min(1, confidence))
```

### 3.2 실시간 위험도 평가 시스템

```python
class RealTimeRiskAssessment:
    def __init__(self):
        self.ensemble_system = EnsembleFirePredictionSystem()
        self.risk_history = deque(maxlen=100)
        self.alert_levels = {
            'LOW': 0.3,
            'MEDIUM': 0.5,
            'HIGH': 0.7,
            'CRITICAL': 0.85
        }
        
    def assess_risk(self, data_stream):
        """실시간 위험도 평가"""
        # 앙상블 예측
        prediction = self.ensemble_system.predict_ensemble(data_stream)
        
        # 위험도 이력 업데이트
        self.risk_history.append(prediction['final_risk'])
        
        # 동적 임계값 조정
        dynamic_threshold = self.calculate_dynamic_threshold()
        
        # 위험 수준 결정
        risk_level = self.determine_risk_level(
            prediction['final_risk'], 
            dynamic_threshold
        )
        
        # 추가 위험 요소 체크
        additional_risks = self.check_additional_risks(data_stream)
        
        return {
            'risk_level': risk_level,
            'risk_score': prediction['final_risk'],
            'confidence': prediction['confidence'],
            'model_predictions': prediction['model_predictions'],
            'additional_risks': additional_risks,
            'recommended_action': self.get_recommended_action(risk_level, additional_risks)
        }
    
    def calculate_dynamic_threshold(self):
        """환경에 따른 동적 임계값 계산"""
        if len(self.risk_history) < 20:
            return self.alert_levels['MEDIUM']
        
        # 최근 위험도 패턴 분석
        recent_risks = list(self.risk_history)[-20:]
        mean_risk = np.mean(recent_risks)
        std_risk = np.std(recent_risks)
        
        # 평균 + 2*표준편차를 동적 임계값으로 사용
        dynamic_threshold = min(mean_risk + 2 * std_risk, 0.9)
        
        return dynamic_threshold
    
    def check_additional_risks(self, data_stream):
        """추가 위험 요소 확인"""
        risks = []
        recent_data = pd.DataFrame(data_stream[-10:])
        
        # 급격한 온도 상승
        if len(recent_data) > 1:
            temp_change = recent_data['temperature'].iloc[-1] - recent_data['temperature'].iloc[0]
            if temp_change > 10:  # 10도 이상 급상승
                risks.append({
                    'type': 'RAPID_TEMP_RISE',
                    'severity': 'HIGH',
                    'value': temp_change
                })
        
        # 아크 빈도
        arc_frequency = recent_data['arc_count'].sum()
        if arc_frequency > 5:
            risks.append({
                'type': 'HIGH_ARC_FREQUENCY',
                'severity': 'CRITICAL',
                'value': arc_frequency
            })
        
        # 가스 농도
        max_gas = recent_data[['co_gas', 'voc']].max().max()
        if max_gas > 100:  # ppm
            risks.append({
                'type': 'HIGH_GAS_CONCENTRATION',
                'severity': 'HIGH',
                'value': max_gas
            })
        
        return risks
```

## Phase 4: Edge-Cloud 하이브리드 시스템 구축

### 4.1 Edge 디바이스 경량 모델

```python
import tensorflow as tf
import tflite_runtime.interpreter as tflite

class EdgeLightweightModel:
    def __init__(self):
        self.model = self.build_lightweight_model()
        self.converter = tf.lite.TFLiteConverter.from_keras_model(self.model)
        
    def build_lightweight_model(self):
        """경량 CNN-LSTM 하이브리드 모델"""
        model = Sequential([
            # 1D CNN for feature extraction
            Conv1D(32, 3, activation='relu', input_shape=(10, 8)),
            MaxPooling1D(2),
            
            # Simplified LSTM
            LSTM(16, return_sequences=False),
            
            # Output
            Dense(8, activation='relu'),
            Dense(1, activation='sigmoid')
        ])
        
        model.compile(
            optimizer='adam',
            loss='binary_crossentropy',
            metrics=['accuracy']
        )
        
        return model
    
    def quantize_model(self):
        """모델 양자화 (8-bit)"""
        self.converter.optimizations = [tf.lite.Optimize.DEFAULT]
        self.converter.representative_dataset = self.representative_dataset_gen
        self.converter.target_spec.supported_ops = [
            tf.lite.OpsSet.TFLITE_BUILTINS_INT8
        ]
        self.converter.inference_input_type = tf.int8
        self.converter.inference_output_type = tf.int8
        
        tflite_model = self.converter.convert()
        
        # 모델 저장
        with open('edge_model.tflite', 'wb') as f:
            f.write(tflite_model)
        
        return tflite_model
    
    def edge_inference(self, input_data):
        """Edge에서 추론 실행"""
        # TFLite 인터프리터 로드
        interpreter = tflite.Interpreter(model_path='edge_model.tflite')
        interpreter.allocate_tensors()
        
        # 입력/출력 텐서 정보
        input_details = interpreter.get_input_details()
        output_details = interpreter.get_output_details()
        
        # 데이터 전처리 및 양자화
        input_scale, input_zero_point = input_details[0]['quantization']
        input_data = input_data / input_scale + input_zero_point
        input_data = input_data.astype(np.int8)
        
        # 추론 실행
        interpreter.set_tensor(input_details[0]['index'], input_data)
        interpreter.invoke()
        
        # 결과 역양자화
        output_data = interpreter.get_tensor(output_details[0]['index'])
        output_scale, output_zero_point = output_details[0]['quantization']
        output_data = (output_data - output_zero_point) * output_scale
        
        return output_data[0]
```

### 4.2 Edge-Cloud 통신 및 협업

```python
import asyncio
import aiohttp
from dataclasses import dataclass
import msgpack

@dataclass
class EdgeCloudMessage:
    timestamp: datetime
    device_id: str
    edge_risk_score: float
    raw_data: dict
    requires_cloud_analysis: bool

class EdgeCloudHybridSystem:
    def __init__(self, cloud_endpoint, device_id):
        self.cloud_endpoint = cloud_endpoint
        self.device_id = device_id
        self.edge_model = EdgeLightweightModel()
        self.data_buffer = deque(maxlen=100)
        self.cloud_sync_interval = 60  # 초
        
    async def process_edge_data(self, data_point):
        """Edge에서 1차 처리"""
        self.data_buffer.append(data_point)
        
        # Edge 모델로 빠른 추론
        if len(self.data_buffer) >= 10:
            edge_input = self.prepare_edge_input(list(self.data_buffer)[-10:])
            edge_risk = self.edge_model.edge_inference(edge_input)
            
            # 위험도에 따른 처리
            if edge_risk > 0.7:  # 고위험
                # 즉시 Cloud로 전송
                await self.send_to_cloud_immediate(data_point, edge_risk)
            elif edge_risk > 0.5:  # 중위험
                # 버퍼에 저장 후 주기적 전송
                await self.buffer_for_batch_send(data_point, edge_risk)
            
            return {
                'edge_risk': edge_risk,
                'action': 'IMMEDIATE' if edge_risk > 0.7 else 'BUFFERED'
            }
    
    async def send_to_cloud_immediate(self, data_point, edge_risk):
        """긴급 데이터 즉시 전송"""
        message = EdgeCloudMessage(
            timestamp=datetime.now(),
            device_id=self.device_id,
            edge_risk_score=edge_risk,
            raw_data=data_point,
            requires_cloud_analysis=True
        )
        
        async with aiohttp.ClientSession() as session:
            try:
                async with session.post(
                    f"{self.cloud_endpoint}/emergency",
                    data=msgpack.packb(message.__dict__),
                    headers={'Content-Type': 'application/msgpack'}
                ) as response:
                    cloud_result = await response.json()
                    await self.handle_cloud_response(cloud_result)
            except Exception as e:
                print(f"Cloud communication error: {e}")
                # 로컬 비상 조치
                await self.local_emergency_action(edge_risk)
    
    async def buffer_for_batch_send(self, data_point, edge_risk):
        """배치 전송을 위한 버퍼링"""
        if not hasattr(self, 'batch_buffer'):
            self.batch_buffer = []
        
        self.batch_buffer.append({
            'timestamp': datetime.now(),
            'data': data_point,
            'edge_risk': edge_risk
        })
        
        # 버퍼가 가득 차면 전송
        if len(self.batch_buffer) >= 50:
            await self.send_batch_to_cloud()
    
    async def send_batch_to_cloud(self):
        """배치 데이터 Cloud 전송"""
        if not hasattr(self, 'batch_buffer') or not self.batch_buffer:
            return
        
        async with aiohttp.ClientSession() as session:
            try:
                async with session.post(
                    f"{self.cloud_endpoint}/batch",
                    json={
                        'device_id': self.device_id,
                        'batch_data': self.batch_buffer
                    }
                ) as response:
                    result = await response.json()
                    self.batch_buffer.clear()
                    
                    # Cloud 분석 결과 처리
                    if result.get('high_risk_detected'):
                        await self.update_edge_model(result.get('model_update'))
            except Exception as e:
                print(f"Batch send error: {e}")
    
    def prepare_edge_input(self, data_window):
        """Edge 모델 입력 준비"""
        features = []
        for data in data_window:
            features.append([
                data['voltage'],
                data['current'],
                data['leakage_current'],
                data['temperature'],
                data['co_gas'],
                data['voc'],
                data['power_factor'],
                data['arc_count']
            ])
        return np.array(features).reshape(1, 10, 8)
```

### 4.3 Cloud 서버 구현

```python
from fastapi import FastAPI, BackgroundTasks
from fastapi.responses import JSONResponse
import uvicorn
from typing import List, Dict
import redis
from celery import Celery

app = FastAPI()
celery_app = Celery('fire_prediction', broker='redis://localhost:6379')
redis_client = redis.Redis(host='localhost', port=6379, db=0)

class CloudAnalysisServer:
    def __init__(self):
        self.ensemble_system = EnsembleFirePredictionSystem()
        self.device_states = {}  # 디바이스별 상태 추적
        
    @app.post("/emergency")
    async def emergency_analysis(self, message: Dict):
        """긴급 분석 요청 처리"""
        device_id = message['device_id']
        edge_risk = message['edge_risk_score']
        raw_data = message['raw_data']
        
        # 디바이스 상태 업데이트
        if device_id not in self.device_states:
            self.device_states[device_id] = deque(maxlen=1000)
        
        self.device_states[device_id].append(raw_data)
        
        # 심층 분석 수행
        cloud_analysis = await self.perform_deep_analysis(
            device_id, 
            list(self.device_states[device_id])
        )
        
        # 의사결정
        action = self.determine_action(edge_risk, cloud_analysis)
        
        # 알림 발송
        if action['severity'] >= 3:
            await self.send_alerts(device_id, action)
        
        return {
            'device_id': device_id,
            'cloud_risk': cloud_analysis['final_risk'],
            'action': action,
            'model_confidence': cloud_analysis['confidence']
        }
    
    async def perform_deep_analysis(self, device_id, device_data):
        """Cloud에서 심층 분석"""
        # 1. 앙상블 모델 예측
        ensemble_result = self.ensemble_system.predict_ensemble(device_data)
        
        # 2. 장기 패턴 분석
        long_term_patterns = await self.analyze_long_term_patterns(device_id)
        
        # 3. 인근 디바이스 상관 분석
        nearby_correlation = await self.analyze_nearby_devices(device_id)
        
        # 4. 환경 요인 고려
        environmental_factors = await self.get_environmental_factors(device_id)
        
        # 종합 분석
        final_risk = self.calculate_comprehensive_risk(
            ensemble_result,
            long_term_patterns,
            nearby_correlation,
            environmental_factors
        )
        
        return {
            'final_risk': final_risk,
            'ensemble_result': ensemble_result,
            'patterns': long_term_patterns,
            'correlation': nearby_correlation,
            'confidence': ensemble_result['confidence']
        }
    
    @celery_app.task
    def analyze_long_term_patterns(device_id):
        """장기 패턴 분석 (비동기)"""
        # Redis에서 장기 데이터 조회
        historical_data = redis_client.get(f"history:{device_id}")
        if not historical_data:
            return {'pattern': 'NO_DATA', 'risk_modifier': 1.0}
        
        # 패턴 분석
        data = pd.DataFrame(msgpack.unpackb(historical_data))
        
        # 주기적 패턴 감지
        patterns = {
            'daily_pattern': detect_daily_pattern(data),
            'weekly_pattern': detect_weekly_pattern(data),
            'anomaly_frequency': calculate_anomaly_frequency(data),
            'degradation_trend': detect_degradation_trend(data)
        }
        
        # 위험도 수정자 계산
        risk_modifier = 1.0
        if patterns['degradation_trend'] > 0.5:
            risk_modifier *= 1.5
        if patterns['anomaly_frequency'] > 0.1:
            risk_modifier *= 1.3
            
        return {
            'patterns': patterns,
            'risk_modifier': risk_modifier
        }
    
    @app.post("/batch")
    async def batch_analysis(self, batch_data: Dict, background_tasks: BackgroundTasks):
        """배치 데이터 분석"""
        device_id = batch_data['device_id']
        data_points = batch_data['batch_data']
        
        # 백그라운드에서 상세 분석
        background_tasks.add_task(
            self.process_batch_data,
            device_id,
            data_points
        )
        
        # 즉시 응답
        quick_analysis = self.quick_batch_analysis(data_points)
        
        return {
            'status': 'accepted',
            'quick_analysis': quick_analysis,
            'high_risk_detected': quick_analysis['max_risk'] > 0.6
        }
    
    async def process_batch_data(self, device_id, data_points):
        """배치 데이터 상세 처리"""
        # 데이터 저장
        for point in data_points:
            await self.store_data_point(device_id, point)
        
        # 모델 재학습 필요성 검토
        if await self.check_model_update_needed(device_id):
            await self.trigger_model_update(device_id)
        
        # 통계 업데이트
        await self.update_device_statistics(device_id, data_points)
```

### 4.4 통합 모니터링 대시보드

```python
class MonitoringDashboard:
    def __init__(self):
        self.devices = {}
        self.alerts = deque(maxlen=1000)
        
    def create_dashboard_api(self):
        """대시보드 API 엔드포인트"""
        
        @app.get("/dashboard/overview")
        async def get_overview():
            """전체 시스템 개요"""
            active_devices = len([d for d in self.devices.values() 
                                if d['last_seen'] > datetime.now() - timedelta(minutes=5)])
            
            risk_distribution = self.calculate_risk_distribution()
            recent_alerts = list(self.alerts)[-10:]
            
            return {
                'active_devices': active_devices,
                'total_devices': len(self.devices),
                'risk_distribution': risk_distribution,
                'recent_alerts': recent_alerts,
                'system_health': self.check_system_health()
            }
        
        @app.get("/dashboard/device/{device_id}")
        async def get_device_details(device_id: str):
            """특정 디바이스 상세 정보"""
            if device_id not in self.devices:
                return JSONResponse(status_code=404, content={"error": "Device not found"})
            
            device = self.devices[device_id]
            
            # 실시간 데이터
            current_data = await self.get_current_device_data(device_id)
            
            # 위험도 추이
            risk_history = await self.get_risk_history(device_id, hours=24)
            
            # 예측 정보
            predictions = await self.get_device_predictions(device_id)
            
            return {
                'device_info': device,
                'current_data': current_data,
                'risk_history': risk_history,
                'predictions': predictions,
                'maintenance_recommendations': self.get_maintenance_recommendations(device_id)
            }
        
        @app.websocket("/ws/{device_id}")
        async def websocket_endpoint(websocket: WebSocket, device_id: str):
            """실시간 데이터 스트리밍"""
            await websocket.accept()
            
            try:
                while True:
                    # 실시간 데이터 전송
                    data = await self.get_realtime_data(device_id)
                    await websocket.send_json(data)
                    await asyncio.sleep(1)
            except Exception as e:
                print(f"WebSocket error: {e}")
            finally:
                await websocket.close()
    
    def calculate_risk_distribution(self):
        """전체 디바이스의 위험도 분포"""
        risk_levels = {'low': 0, 'medium': 0, 'high': 0, 'critical': 0}
        
        for device in self.devices.values():
            risk = device.get('current_risk', 0)
            if risk < 0.3:
                risk_levels['low'] += 1
            elif risk < 0.5:
                risk_levels['medium'] += 1
            elif risk < 0.7:
                risk_levels['high'] += 1
            else:
                risk_levels['critical'] += 1
        
        return risk_levels
    
    def get_maintenance_recommendations(self, device_id):
        """유지보수 권장사항"""
        device = self.devices.get(device_id, {})
        recommendations = []
        
        # 센서별 상태 확인
        if device.get('temperature_trend', 0) > 0.1:
            recommendations.append({
                'type': 'COOLING_CHECK',
                'priority': 'HIGH',
                'description': '냉각 시스템 점검 필요'
            })
        
        if device.get('arc_frequency', 0) > 5:
            recommendations.append({
                'type': 'WIRING_INSPECTION',
                'priority': 'CRITICAL',
                'description': '배선 및 연결부 즉시 점검 필요'
            })
        
        if device.get('leakage_current_avg', 0) > 10:
            recommendations.append({
                'type': 'INSULATION_CHECK',
                'priority': 'MEDIUM',
                'description': '절연 상태 점검 권장'
            })
        
        return recommendations
```

### 4.5 시스템 배포 및 운영

```python
# docker-compose.yml
"""
version: '3.8'

services:
  edge-device:
    build: ./edge
    environment:
      - CLOUD_ENDPOINT=http://cloud-server:8000
      - DEVICE_ID=${DEVICE_ID}
    volumes:
      - ./models:/app/models
    restart: always
    
  cloud-server:
    build: ./cloud
    ports:
      - "8000:8000"
    environment:
      - REDIS_URL=redis://redis:6379
      - DATABASE_URL=postgresql://user:pass@db:5432/fire_prediction
    depends_on:
      - redis
      - db
      
  redis:
    image: redis:alpine
    ports:
      - "6379:6379"
      
  db:
    image: postgres:13
    environment:
      - POSTGRES_DB=fire_prediction
      - POSTGRES_USER=user
      - POSTGRES_PASSWORD=pass
    volumes:
      - postgres_data:/var/lib/postgresql/data
      
  monitoring:
    image: grafana/grafana
    ports:
      - "3000:3000"
    volumes:
      - ./grafana/dashboards:/etc/grafana/provisioning/dashboards
      
volumes:
  postgres_data:
"""

# 시스템 시작 스크립트
class SystemDeployment:
    def __init__(self):
        self.config = self.load_config()
        
    def deploy_system(self):
        """전체 시스템 배포"""
        # 1. 모델 학습 및 준비
        print("Training models...")
        self.train_all_models()
        
        # 2. Edge 모델 배포
        print("Deploying edge models...")
        self.deploy_edge_models()
        
        # 3. Cloud 서버 시작
        print("Starting cloud server...")
        self.start_cloud_server()
        
        # 4. 모니터링 시스템 구성
        print("Setting up monitoring...")
        self.setup_monitoring()
        
        print("System deployment complete!")
    
    def train_all_models(self):
        """모든 모델 학습"""
        # 데이터 로드
        training_data = pd.read_csv('historical_data.csv')
        fire_events = pd.read_csv('fire_events.csv')
        
        # Phase별 모델 학습
        system = IntegratedPredictionSystem()
        system.train_models(training_data, fire_events)
        
        # 모델 저장
        system.save_models('./models/')
    
    def deploy_edge_models(self):
        """Edge 모델 배포"""
        # 모델 경량화
        edge_model = EdgeLightweightModel()
        edge_model.quantize_model()
        
        # 각 Edge 디바이스로 전송
        devices = self.config['edge_devices']
        for device in devices:
            self.send_model_to_device(device, 'edge_model.tflite')
    
    def start_cloud_server(self):
        """Cloud 서버 시작"""
        import subprocess
        subprocess.Popen(['uvicorn', 'cloud_server:app', '--host', '0.0.0.0', '--port', '8000'])
    
    def setup_monitoring(self):
        """모니터링 설정"""
        # Grafana 대시보드 구성
        dashboard_config = {
            'panels': [
                {'title': 'System Overview', 'type': 'graph'},
                {'title': 'Risk Distribution', 'type': 'piechart'},
                {'title': 'Alert History', 'type': 'table'},
                {'title': 'Device Status', 'type': 'heatmap'}
            ]
        }
        
        # Prometheus 메트릭 설정
        self.setup_prometheus_metrics()

if __name__ == "__main__":
    deployment = SystemDeployment()
    deployment.deploy_system()
"""
```

## 마무리 및 운영 가이드

### 성능 모니터링
- Edge 응답 시간: < 100ms
- Cloud 분석 시간: < 1초
- 전체 시스템 지연: < 2초

### 유지보수 체크리스트
1. 일일: 시스템 로그 확인, 알림 검토
2. 주간: 모델 성능 메트릭 분석, Edge 디바이스 상태 점검
3. 월간: 모델 재학습, 시스템 업데이트

### 확장 고려사항
- 디바이스 수 증가에 따른 Cloud 서버 스케일링
- 지역별 Edge 서버 클러스터링
- 실시간 모델 업데이트 파이프라인