# 화재 예측 앙상블 모델 5종 상세 설명

## 1. Isolation Forest (IF) - 격리 숲

### 기본 원리
Isolation Forest는 "이상치는 정상 데이터보다 격리하기 쉽다"는 원리에 기반합니다.

```python
# 핵심 개념 시각화
import numpy as np
import matplotlib.pyplot as plt
from sklearn.ensemble import IsolationForest

# 정상 데이터와 이상치 생성
np.random.seed(42)
normal_data = np.random.randn(100, 2)
anomalies = np.random.uniform(low=-4, high=4, size=(10, 2))
X = np.vstack([normal_data, anomalies])

# Isolation Forest 학습
clf = IsolationForest(contamination=0.1, random_state=42)
predictions = clf.fit_predict(X)
```

### 작동 방식

1. **격리 트리 구축**
   - 데이터 공간을 무작위로 분할
   - 각 데이터 포인트를 격리할 때까지 반복
   - 이상치는 적은 분할로 격리됨

2. **이상 점수 계산**
   ```python
   def isolation_score(path_length, n_samples):
       """격리 경로 길이를 기반으로 이상 점수 계산"""
       # 평균 경로 길이
       c_n = 2 * (np.log(n_samples - 1) + 0.5772) - 2 * (n_samples - 1) / n_samples
       
       # 이상 점수 (0~1, 1에 가까울수록 이상치)
       score = 2 ** (-path_length / c_n)
       return score
   ```

3. **화재 예측에서의 활용**
   ```python
   class FireAnomalyDetector:
       def __init__(self):
           self.model = IsolationForest(
               n_estimators=100,      # 트리 개수
               contamination=0.01,    # 예상 이상치 비율
               max_samples='auto',    # 서브샘플링 크기
               random_state=42
           )
       
       def detect_fire_anomaly(self, sensor_data):
           """센서 데이터에서 이상 패턴 감지"""
           features = self.extract_features(sensor_data)
           
           # -1: 이상치, 1: 정상
           prediction = self.model.predict([features])[0]
           
           # 이상 점수 (낮을수록 이상)
           anomaly_score = self.model.score_samples([features])[0]
           
           return {
               'is_anomaly': prediction == -1,
               'anomaly_score': anomaly_score,
               'fire_risk': self.convert_to_fire_risk(anomaly_score)
           }
   ```

### 장단점
- **장점**: 
  - 매우 빠른 학습 및 예측
  - 고차원 데이터에서도 효과적
  - 정상 데이터만으로 학습 가능
- **단점**: 
  - 시계열 패턴 고려 못함
  - 국소적 이상치 탐지 어려움

---

## 2. LSTM (Long Short-Term Memory)

### 기본 원리
LSTM은 장기 의존성을 학습할 수 있는 특별한 RNN 구조입니다.

### LSTM 셀 구조

```python
class LSTMCell:
    """LSTM 셀의 내부 작동 원리"""
    def __init__(self, input_size, hidden_size):
        # 게이트별 가중치
        self.W_f = np.random.randn(hidden_size, input_size + hidden_size)  # Forget gate
        self.W_i = np.random.randn(hidden_size, input_size + hidden_size)  # Input gate
        self.W_c = np.random.randn(hidden_size, input_size + hidden_size)  # Cell gate
        self.W_o = np.random.randn(hidden_size, input_size + hidden_size)  # Output gate
        
    def forward(self, x_t, h_prev, c_prev):
        """LSTM 순전파"""
        # 입력과 이전 은닉 상태 결합
        combined = np.concatenate([x_t, h_prev])
        
        # 1. Forget Gate: 어떤 정보를 잊을지 결정
        f_t = sigmoid(np.dot(self.W_f, combined))
        
        # 2. Input Gate: 어떤 새 정보를 저장할지 결정
        i_t = sigmoid(np.dot(self.W_i, combined))
        c_tilde = np.tanh(np.dot(self.W_c, combined))
        
        # 3. Cell State 업데이트
        c_t = f_t * c_prev + i_t * c_tilde
        
        # 4. Output Gate: 출력할 정보 결정
        o_t = sigmoid(np.dot(self.W_o, combined))
        h_t = o_t * np.tanh(c_t)
        
        return h_t, c_t
```

### 화재 예측을 위한 LSTM 구현

```python
import tensorflow as tf
from tensorflow.keras import layers, models

class FirePredictionLSTM:
    def __init__(self, sequence_length=30, n_features=8):
        self.sequence_length = sequence_length
        self.n_features = n_features
        self.model = self.build_model()
        
    def build_model(self):
        """시계열 화재 예측 LSTM 모델"""
        model = models.Sequential([
            # 첫 번째 LSTM 층: 시계열 패턴 학습
            layers.LSTM(128, 
                       return_sequences=True,
                       input_shape=(self.sequence_length, self.n_features)),
            layers.Dropout(0.2),
            
            # 두 번째 LSTM 층: 고수준 패턴 추출
            layers.LSTM(64, return_sequences=False),
            layers.Dropout(0.2),
            
            # 출력층
            layers.Dense(32, activation='relu'),
            layers.Dense(1, activation='sigmoid')  # 화재 확률
        ])
        
        model.compile(
            optimizer='adam',
            loss='binary_crossentropy',
            metrics=['accuracy']
        )
        
        return model
    
    def prepare_fire_data(self, sensor_data):
        """센서 데이터를 LSTM 입력 형태로 변환"""
        sequences = []
        
        # 슬라이딩 윈도우로 시퀀스 생성
        for i in range(len(sensor_data) - self.sequence_length):
            sequence = sensor_data[i:i+self.sequence_length]
            sequences.append(sequence)
        
        return np.array(sequences)
```

### LSTM의 화재 패턴 학습

```python
# LSTM이 학습하는 화재 전조 패턴 예시
def visualize_lstm_patterns():
    """LSTM이 포착하는 시계열 패턴"""
    
    # 1. 온도 상승 패턴
    temperature_pattern = {
        'normal': [20, 21, 20, 22, 21],  # 정상 변동
        'pre_fire': [20, 22, 25, 30, 38]  # 화재 전 급상승
    }
    
    # 2. 전류 이상 패턴
    current_pattern = {
        'normal': [10, 11, 10, 9, 10],    # 안정적
        'pre_fire': [10, 12, 8, 15, 20]   # 불안정
    }
    
    # LSTM은 이러한 시간적 변화를 학습
    return temperature_pattern, current_pattern
```

### 장단점
- **장점**: 
  - 장기 시계열 패턴 학습
  - 복잡한 시간적 의존성 포착
  - 다변량 시계열 처리 가능
- **단점**: 
  - 학습 시간이 길고 계산량 많음
  - 많은 학습 데이터 필요
  - 과적합 위험

---

## 3. GRU (Gated Recurrent Unit)

### 기본 원리
GRU는 LSTM의 간소화된 버전으로, 더 적은 파라미터로 유사한 성능을 달성합니다.

### GRU 구조

```python
class GRUCell:
    """GRU 셀의 작동 원리"""
    def __init__(self, input_size, hidden_size):
        # GRU는 LSTM보다 적은 게이트 사용
        self.W_z = np.random.randn(hidden_size, input_size + hidden_size)  # Update gate
        self.W_r = np.random.randn(hidden_size, input_size + hidden_size)  # Reset gate
        self.W_h = np.random.randn(hidden_size, input_size + hidden_size)  # Hidden state
        
    def forward(self, x_t, h_prev):
        """GRU 순전파"""
        combined = np.concatenate([x_t, h_prev])
        
        # 1. Update Gate: 이전 정보를 얼마나 유지할지
        z_t = sigmoid(np.dot(self.W_z, combined))
        
        # 2. Reset Gate: 이전 정보를 얼마나 잊을지
        r_t = sigmoid(np.dot(self.W_r, combined))
        
        # 3. 새로운 후보 은닉 상태
        combined_reset = np.concatenate([x_t, r_t * h_prev])
        h_tilde = np.tanh(np.dot(self.W_h, combined_reset))
        
        # 4. 최종 은닉 상태
        h_t = (1 - z_t) * h_prev + z_t * h_tilde
        
        return h_t
```

### 화재 예측용 GRU 구현

```python
class FirePredictionGRU:
    def __init__(self, sequence_length=15):
        """단기 예측에 최적화된 GRU 모델"""
        self.model = models.Sequential([
            # GRU는 LSTM보다 빠른 학습과 예측
            layers.GRU(64, 
                      return_sequences=True,
                      input_shape=(sequence_length, 8)),
            layers.Dropout(0.2),
            
            layers.GRU(32, return_sequences=False),
            
            layers.Dense(16, activation='relu'),
            layers.Dense(1, activation='sigmoid')
        ])
        
        self.model.compile(
            optimizer='adam',
            loss='binary_crossentropy',
            metrics=['accuracy']
        )
    
    def predict_short_term(self, recent_data):
        """5-15분 단기 화재 위험 예측"""
        # GRU는 짧은 시퀀스에서 효율적
        prediction = self.model.predict(recent_data)
        return prediction[0][0]
```

### LSTM vs GRU 비교

```python
def compare_lstm_gru():
    """화재 예측에서 LSTM과 GRU 비교"""
    
    comparison = {
        'LSTM': {
            'parameters': '4 * (hidden_size * (input_size + hidden_size + 1))',
            'gates': 4,  # forget, input, cell, output
            'memory': 'Cell state + Hidden state',
            'best_for': '장기 패턴 (30분-1시간)',
            'training_time': 1.0  # 상대적
        },
        'GRU': {
            'parameters': '3 * (hidden_size * (input_size + hidden_size + 1))',
            'gates': 2,  # update, reset
            'memory': 'Hidden state only',
            'best_for': '단기 패턴 (5-15분)',
            'training_time': 0.7  # 상대적으로 빠름
        }
    }
    
    return comparison
```

### 장단점
- **장점**: 
  - LSTM보다 빠른 학습
  - 적은 파라미터로 효율적
  - 단기 예측에 우수
- **단점**: 
  - 매우 긴 시퀀스에서는 LSTM보다 성능 저하
  - 복잡한 장기 패턴 학습 어려움

---

## 4. Random Forest (RF) - 랜덤 포레스트

### 기본 원리
Random Forest는 여러 개의 결정 트리를 앙상블하여 예측하는 방법입니다.

### 작동 방식

```python
class RandomForestPrinciple:
    """Random Forest 기본 원리"""
    
    def __init__(self, n_trees=100):
        self.n_trees = n_trees
        self.trees = []
        
    def build_tree(self, X, y):
        """개별 결정 트리 구축"""
        # 1. Bootstrap 샘플링 (복원 추출)
        n_samples = X.shape[0]
        bootstrap_indices = np.random.choice(n_samples, n_samples, replace=True)
        X_bootstrap = X[bootstrap_indices]
        y_bootstrap = y[bootstrap_indices]
        
        # 2. 특징 무작위 선택
        n_features = X.shape[1]
        # 각 분할에서 sqrt(n_features)개만 고려
        max_features = int(np.sqrt(n_features))
        
        # 3. 트리 학습
        tree = DecisionTree(max_features=max_features)
        tree.fit(X_bootstrap, y_bootstrap)
        
        return tree
    
    def predict(self, X):
        """앙상블 예측"""
        # 모든 트리의 예측을 평균
        predictions = []
        for tree in self.trees:
            predictions.append(tree.predict(X))
        
        # 회귀: 평균, 분류: 투표
        return np.mean(predictions, axis=0)
```

### 화재 예측용 Random Forest

```python
from sklearn.ensemble import RandomForestClassifier

class FireDetectionRF:
    def __init__(self):
        self.model = RandomForestClassifier(
            n_estimators=100,      # 트리 개수
            max_depth=20,          # 트리 깊이 제한
            min_samples_split=5,   # 분할 최소 샘플
            max_features='sqrt',   # 특징 선택 전략
            random_state=42
        )
        
    def extract_statistical_features(self, sensor_window):
        """통계적 특징 추출"""
        features = {}
        
        # 각 센서별 통계량
        for sensor in ['temperature', 'current', 'voltage']:
            data = sensor_window[sensor]
            features.update({
                f'{sensor}_mean': np.mean(data),
                f'{sensor}_std': np.std(data),
                f'{sensor}_max': np.max(data),
                f'{sensor}_min': np.min(data),
                f'{sensor}_range': np.max(data) - np.min(data),
                f'{sensor}_skew': scipy.stats.skew(data),
                f'{sensor}_kurtosis': scipy.stats.kurtosis(data)
            })
        
        # 교차 특징
        features['power'] = features['voltage_mean'] * features['current_mean']
        features['resistance'] = features['voltage_mean'] / (features['current_mean'] + 1e-6)
        
        return features
    
    def predict_fire_risk(self, sensor_data):
        """화재 위험도 예측"""
        # 특징 추출
        features = self.extract_statistical_features(sensor_data)
        feature_vector = list(features.values())
        
        # 예측 확률
        fire_probability = self.model.predict_proba([feature_vector])[0][1]
        
        # 특징 중요도
        feature_importance = dict(zip(features.keys(), 
                                    self.model.feature_importances_))
        
        return {
            'fire_risk': fire_probability,
            'important_features': sorted(feature_importance.items(), 
                                       key=lambda x: x[1], 
                                       reverse=True)[:5]
        }
```

### 특징 중요도 분석

```python
def analyze_feature_importance():
    """Random Forest의 특징 중요도 분석"""
    
    # 화재 예측에서 중요한 특징들
    feature_importance = {
        'temperature_max': 0.25,      # 최고 온도
        'arc_count': 0.20,           # 아크 발생 횟수
        'current_std': 0.15,         # 전류 변동성
        'temperature_change_rate': 0.12,  # 온도 상승률
        'co_gas_level': 0.10,        # CO 가스 농도
        'power_factor': 0.08,        # 역률
        'voltage_stability': 0.05,   # 전압 안정성
        'others': 0.05
    }
    
    return feature_importance
```

### 장단점
- **장점**: 
  - 과적합에 강함
  - 특징 중요도 제공
  - 병렬 처리 가능
  - 비선형 관계 포착
- **단점**: 
  - 시계열 패턴 직접 학습 불가
  - 예측 설명이 어려움
  - 메모리 사용량 많음

---

## 5. XGBoost (eXtreme Gradient Boosting)

### 기본 원리
XGBoost는 그래디언트 부스팅을 최적화한 알고리즘으로, 순차적으로 약한 학습기를 추가합니다.

### 부스팅 원리

```python
class GradientBoostingPrinciple:
    """그래디언트 부스팅 기본 원리"""
    
    def __init__(self, n_estimators=100, learning_rate=0.1):
        self.n_estimators = n_estimators
        self.learning_rate = learning_rate
        self.trees = []
        
    def fit(self, X, y):
        """부스팅 학습 과정"""
        # 1. 초기 예측 (평균값)
        F_0 = np.mean(y)
        F_m = np.full(len(y), F_0)
        
        for m in range(self.n_estimators):
            # 2. 잔차(residual) 계산
            residuals = y - F_m
            
            # 3. 잔차를 학습하는 트리 생성
            tree = DecisionTree(max_depth=3)
            tree.fit(X, residuals)
            
            # 4. 예측 업데이트
            predictions = tree.predict(X)
            F_m = F_m + self.learning_rate * predictions
            
            self.trees.append(tree)
    
    def predict(self, X):
        """부스팅 예측"""
        predictions = np.zeros(len(X))
        
        for tree in self.trees:
            predictions += self.learning_rate * tree.predict(X)
        
        return predictions
```

### XGBoost의 최적화 기법

```python
import xgboost as xgb

class FirePredictionXGBoost:
    def __init__(self):
        self.model = xgb.XGBClassifier(
            n_estimators=100,
            max_depth=6,
            learning_rate=0.1,
            objective='binary:logistic',
            
            # XGBoost 특화 파라미터
            gamma=0.1,              # 분할을 위한 최소 손실 감소
            subsample=0.8,          # 각 트리에 사용할 샘플 비율
            colsample_bytree=0.8,   # 각 트리에 사용할 특징 비율
            
            # 정규화 파라미터
            reg_alpha=0.1,          # L1 정규화
            reg_lambda=1.0,         # L2 정규화
            
            # 성능 최적화
            tree_method='hist',     # 히스토그램 기반 분할
            enable_categorical=True  # 범주형 변수 지원
        )
    
    def train_with_optimization(self, X_train, y_train, X_val, y_val):
        """최적화된 학습"""
        # Early Stopping 사용
        self.model.fit(
            X_train, y_train,
            eval_set=[(X_val, y_val)],
            early_stopping_rounds=10,
            verbose=False
        )
        
        # 특징 중요도
        feature_importance = self.model.feature_importances_
        
        return feature_importance
```

### XGBoost의 화재 예측 활용

```python
class XGBoostFireAnalyzer:
    def __init__(self):
        self.model = None
        self.feature_names = None
        
    def create_interaction_features(self, data):
        """상호작용 특징 생성"""
        features = {}
        
        # 기본 특징
        base_features = ['temperature', 'current', 'voltage', 'co_gas']
        
        # 1차 상호작용
        for i, feat1 in enumerate(base_features):
            for feat2 in base_features[i+1:]:
                # 곱셈 상호작용
                features[f'{feat1}_x_{feat2}'] = data[feat1] * data[feat2]
                # 비율 상호작용
                features[f'{feat1}_div_{feat2}'] = data[feat1] / (data[feat2] + 1e-6)
        
        # 도메인 특화 특징
        features['thermal_load'] = data['current']**2 * data['resistance']
        features['power_quality'] = abs(data['power_factor'] - 1.0)
        
        return features
    
    def explain_prediction(self, instance):
        """SHAP를 활용한 예측 설명"""
        import shap
        
        # SHAP 설명자 생성
        explainer = shap.TreeExplainer(self.model)
        shap_values = explainer.shap_values(instance)
        
        # 특징별 기여도
        feature_contributions = dict(zip(self.feature_names, shap_values[0]))
        
        return sorted(feature_contributions.items(), 
                     key=lambda x: abs(x[1]), 
                     reverse=True)
```

### XGBoost vs Random Forest

```python
def compare_xgboost_rf():
    """화재 예측에서 XGBoost와 RF 비교"""
    
    comparison = {
        'Training': {
            'XGBoost': '순차적 (직렬)',
            'RandomForest': '병렬'
        },
        'Base Learners': {
            'XGBoost': '약한 학습기 (shallow trees)',
            'RandomForest': '강한 학습기 (deep trees)'
        },
        'Learning': {
            'XGBoost': '이전 오류에서 학습',
            'RandomForest': '독립적 학습'
        },
        'Performance': {
            'XGBoost': '일반적으로 더 높은 정확도',
            'RandomForest': '안정적이고 robust'
        },
        'Speed': {
            'XGBoost': '최적화로 빠름',
            'RandomForest': '병렬화로 빠름'
        }
    }
    
    return comparison
```

### 장단점
- **장점**: 
  - 매우 높은 예측 정확도
  - 빠른 학습 및 예측
  - 과적합 방지 기능 내장
  - 특징 중요도 제공
- **단점**: 
  - 하이퍼파라미터 튜닝 복잡
  - 시계열 직접 처리 불가
  - 메모리 사용량 많음

---

## 앙상블 통합 전략

### 모델별 강점 활용

```python
class EnsembleStrategy:
    def __init__(self):
        self.models = {
            'isolation_forest': {'weight': 0.15, 'type': 'anomaly'},
            'lstm': {'weight': 0.30, 'type': 'temporal'},
            'gru': {'weight': 0.20, 'type': 'temporal'},
            'random_forest': {'weight': 0.20, 'type': 'feature'},
            'xgboost': {'weight': 0.15, 'type': 'feature'}
        }
    
    def weighted_ensemble(self, predictions):
        """가중 앙상블"""
        weighted_sum = 0
        total_weight = 0
        
        for model_name, pred in predictions.items():
            weight = self.models[model_name]['weight']
            weighted_sum += weight * pred
            total_weight += weight
        
        return weighted_sum / total_weight
    
    def dynamic_weighting(self, predictions, context):
        """상황별 동적 가중치"""
        weights = {}
        
        # 시계열 변동이 큰 경우
        if context['temporal_variance'] > 0.5:
            weights['lstm'] = 0.4
            weights['gru'] = 0.3
        else:
            weights['random_forest'] = 0.3
            weights['xgboost'] = 0.3
        
        # 이상치가 감지된 경우
        if context['anomaly_detected']:
            weights['isolation_forest'] = 0.3
        
        return self.apply_weights(predictions, weights)
```

### 메타 학습기

```python
class MetaLearner:
    """스태킹 앙상블을 위한 메타 학습기"""
    
    def __init__(self):
        self.meta_model = xgb.XGBClassifier(
            n_estimators=50,
            max_depth=3,
            learning_rate=0.05
        )
    
    def prepare_meta_features(self, base_predictions):
        """메타 특징 생성"""
        features = []
        
        # 기본 예측값
        features.extend(base_predictions.values())
        
        # 예측값 통계
        pred_values = list(base_predictions.values())
        features.append(np.mean(pred_values))
        features.append(np.std(pred_values))
        features.append(np.max(pred_values))
        features.append(np.min(pred_values))
        
        # 모델 간 일치도
        features.append(self.calculate_agreement(pred_values))
        
        return features
    
    def calculate_agreement(self, predictions):
        """모델 간 일치도 계산"""
        # 예측값을 이진화 (0.5 기준)
        binary_preds = [1 if p > 0.5 else 0 for p in predictions]
        
        # 일치하는 모델 수 / 전체 모델 수
        agreement = sum(binary_preds) / len(binary_preds)
        
        # 완전 일치(0 또는 1)에 가까울수록 높은 점수
        return 1 - abs(agreement - 0.5) * 2
```

이렇게 5개의 모델은 각각 다른 관점에서 화재 위험을 평가합니다:

1. **Isolation Forest**: 비정상적인 센서 값 조합 감지
2. **LSTM**: 장기 시계열 패턴에서 화재 전조 학습
3. **GRU**: 단기 급격한 변화 포착
4. **Random Forest**: 다양한 특징 조합의 중요도 평가
5. **XGBoost**: 복잡한 비선형 관계 학습

이들을 앙상블하면 각 모델의 약점을 보완하고 강점을 결합하여 더 정확하고 신뢰할 수 있는 화재 예측이 가능합니다.