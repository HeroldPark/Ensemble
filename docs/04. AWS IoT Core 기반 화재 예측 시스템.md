# AWS IoT Core ê¸°ë°˜ í™”ì¬ ì˜ˆì¸¡ ì‹œìŠ¤í…œ - Part 1

## ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜ ê°œìš”

```
IoT Devices â†’ AWS IoT Core â†’ AWS Lambda â†’ Amazon Kinesis â†’ 
Real-time Processing â†’ Machine Learning â†’ Alert System
```

## 1. AWS IoT Core ì„¤ì •

### 1.1 IoT Thing ë“±ë¡ ë° ì •ì±… ì„¤ì •

#### IoT ì •ì±… (JSON)
```json
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Action": [
                "iot:Connect",
                "iot:Publish",
                "iot:Subscribe",
                "iot:Receive"
            ],
            "Resource": [
                "arn:aws:iot:region:account:client/device-*",
                "arn:aws:iot:region:account:topic/fire-prediction/device/*/data",
                "arn:aws:iot:region:account:topic/fire-prediction/alerts/*"
            ]
        }
    ]
}
```

#### IoT Rule ì„¤ì •
```sql
-- IoT Rule SQL
SELECT 
    deviceId,
    idx,
    status,
    volt,
    voltLow,
    ct as current,
    zct as leakage_current,
    rzct as resistive_leakage,
    pwr as power,
    pcbTemp,
    sensorTemp,
    co,
    voc,
    pFct as power_factor,
    aArc as arc_count,
    timestamp() as timestamp
FROM 'fire-prediction/device/+/data'
WHERE status < 4096
```

### 1.2 Discrete ë°ì´í„° íŒŒì„œ

```python
import json
import boto3
import numpy as np
from datetime import datetime
import os
import logging

logger = logging.getLogger()
logger.setLevel(logging.INFO)

class DiscreteDataProcessor:
    def __init__(self):
        self.dynamodb = boto3.resource('dynamodb')
        self.kinesis = boto3.client('kinesis')
        self.sns = boto3.client('sns')
        
        # í…Œì´ë¸” ì°¸ì¡°
        self.raw_data_table = self.dynamodb.Table('FirePrediction-RawData')
        self.device_state_table = self.dynamodb.Table('FirePrediction-DeviceState')
        
    def parse_discrete_data(self, iot_message):
        """Discrete ë°ì´í„° íŒŒì‹± ë° ì •ê·œí™”"""
        try:
            data = {
                'device_id': iot_message['deviceId'],
                'idx': iot_message['idx'],
                'timestamp': iot_message['timestamp'],
                'status_bits': iot_message['status'],
                
                # ì„¼ì„œ ë°ì´í„° (Discrete í´ë˜ìŠ¤ í•„ë“œ ë§¤í•‘)
                'voltage': iot_message['volt'],
                'voltage_low': iot_message['voltLow'],
                'current': iot_message['current'],
                'leakage_current': iot_message['leakage_current'],
                'resistive_leakage': iot_message['resistive_leakage'],
                'power': iot_message['power'],
                'pcb_temperature': iot_message['pcbTemp'],
                'sensor_temperature': iot_message['sensorTemp'],
                'co_gas': iot_message['co'],
                'voc': iot_message['voc'],
                'power_factor': iot_message['power_factor'],
                'arc_count': iot_message['arc_count']
            }
            
            # ìƒíƒœ ë¹„íŠ¸ ë¶„ì„
            data['alarm_status'] = self.parse_status_bits(iot_message['status'])
            
            # ìœ„í—˜ ì§€í‘œ ê³„ì‚°
            data['risk_indicators'] = self.calculate_risk_indicators(data)
            
            return data
            
        except Exception as e:
            logger.error(f"Data parsing error: {e}")
            return None
    
    def parse_status_bits(self, status):
        """ìƒíƒœ ë¹„íŠ¸ ë¶„ì„ (Discrete í´ë˜ìŠ¤ ì£¼ì„ ê¸°ë°˜)"""
        # Discrete í´ë˜ìŠ¤ì˜ status í•„ë“œ ì£¼ì„:
        # ê³¼ì „ì••(1), ì •ì „(2), ê³¼ì „ë¥˜(3), ëˆ„ì „(ì´í•©)(4), ëˆ„ì „(ì €í•­ì„±)(5), 
        # ì „ë ¥ëŸ‰(6), ê¸°íŒì˜¨ë„(7), ì„¼ì„œì˜¨ë„(8), CO(9), VOC(10), ì—­ë¥ (11), ì•„í¬(12)
        
        alarms = {
            'overvoltage': bool(status & (1 << 0)),        # bit 0
            'power_outage': bool(status & (1 << 1)),       # bit 1  
            'overcurrent': bool(status & (1 << 2)),        # bit 2
            'total_leakage': bool(status & (1 << 3)),      # bit 3
            'resistive_leakage': bool(status & (1 << 4)),  # bit 4
            'power_anomaly': bool(status & (1 << 5)),      # bit 5
            'pcb_temp_alarm': bool(status & (1 << 6)),     # bit 6
            'sensor_temp_alarm': bool(status & (1 << 7)),  # bit 7
            'co_alarm': bool(status & (1 << 8)),           # bit 8
            'voc_alarm': bool(status & (1 << 9)),          # bit 9
            'power_factor_alarm': bool(status & (1 << 10)), # bit 10
            'arc_alarm': bool(status & (1 << 11)),         # bit 11
            'communication_error': status >= 4096          # í†µì‹ ì¥ì• 
        }
        
        alarms['total_alarm_count'] = sum(1 for k, v in alarms.items() 
                                        if v and k != 'communication_error')
        return alarms
    
    def calculate_risk_indicators(self, data):
        """í™”ì¬ ìœ„í—˜ ì§€í‘œ ê³„ì‚°"""
        indicators = {}
        
        # ì˜¨ë„ ìœ„í—˜ë„ (Discrete ì£¼ì„ ê¸°ì¤€: 60Â°C)
        max_temp = max(data['pcb_temperature'], data['sensor_temperature'])
        if max_temp > 60:
            indicators['temperature_risk'] = 'CRITICAL'
            temp_score = 1.0
        elif max_temp > 50:
            indicators['temperature_risk'] = 'HIGH'
            temp_score = 0.7
        elif max_temp > 40:
            indicators['temperature_risk'] = 'MEDIUM'
            temp_score = 0.4
        else:
            indicators['temperature_risk'] = 'LOW'
            temp_score = 0.1
        
        # ì „ê¸°ì  ìœ„í—˜ë„
        electrical_risk_score = 0
        if data['alarm_status']['overvoltage']:
            electrical_risk_score += 0.15
        if data['alarm_status']['overcurrent']:
            electrical_risk_score += 0.20  # ê³ ì „ë¥˜(100A ê¸°ì¤€)
        if data['alarm_status']['total_leakage']:
            electrical_risk_score += 0.25  # ëˆ„ì „(8mA ê¸°ì¤€)
        if data['arc_count'] > 0:
            electrical_risk_score += min(0.3, data['arc_count'] * 0.05)
        
        # ê°€ìŠ¤ ìœ„í—˜ë„ (Discrete ì£¼ì„ ê¸°ì¤€)
        gas_risk_score = 0
        if data['co_gas'] > 17:  # 17ppm ì´ˆê³¼
            gas_risk_score += 0.2
        if data['voc'] > 400:    # 400Î¼g/mÂ³ ì´ˆê³¼
            gas_risk_score += 0.15
        
        # ì¢…í•© ìœ„í—˜ë„ (0-1 ìŠ¤ì¼€ì¼)
        total_risk = min(1.0, temp_score * 0.4 + electrical_risk_score + gas_risk_score)
        
        indicators.update({
            'electrical_risk_score': electrical_risk_score,
            'gas_risk_score': gas_risk_score,
            'temperature_score': temp_score,
            'total_risk_score': total_risk
        })
        
        return indicators

    def validate_discrete_data(self, data):
        """Discrete ë°ì´í„° ìœ íš¨ì„± ê²€ì¦"""
        validation_results = {
            'is_valid': True,
            'warnings': [],
            'errors': []
        }
        
        # verifySts í•„ë“œê°€ ìˆë‹¤ë©´ statusì™€ ë¹„êµ ê²€ì¦
        if 'verifySts' in data:
            if data['verifySts'] != data['status_bits']:
                validation_results['warnings'].append(
                    f"Status verification mismatch: {data['status_bits']} vs {data['verifySts']}"
                )
        
        # ì„¼ì„œ ê°’ ë²”ìœ„ ê²€ì¦
        if data['voltage'] < 0 or data['voltage'] > 300:
            validation_results['errors'].append(f"Voltage out of range: {data['voltage']}")
        
        if data['current'] < 0 or data['current'] > 200:
            validation_results['errors'].append(f"Current out of range: {data['current']}")
        
        if data['pcb_temperature'] < -10 or data['pcb_temperature'] > 100:
            validation_results['errors'].append(f"PCB temperature out of range: {data['pcb_temperature']}")
        
        if data['sensor_temperature'] < -10 or data['sensor_temperature'] > 100:
            validation_results['errors'].append(f"Sensor temperature out of range: {data['sensor_temperature']}")
        
        if validation_results['errors']:
            validation_results['is_valid'] = False
        
        return validation_results
```

### 1.3 DynamoDB í…Œì´ë¸” ì„¤ê³„

```yaml
# CloudFormation í…œí”Œë¦¿
Resources:
  FirePredictionRawDataTable:
    Type: AWS::DynamoDB::Table
    Properties:
      TableName: FirePrediction-RawData
      BillingMode: PAY_PER_REQUEST
      AttributeDefinitions:
        - AttributeName: device_id
          AttributeType: S
        - AttributeName: timestamp
          AttributeType: S
      KeySchema:
        - AttributeName: device_id
          KeyType: HASH
        - AttributeName: timestamp
          KeyType: RANGE
      TimeToLiveSpecification:
        AttributeName: ttl
        Enabled: true
      StreamSpecification:
        StreamViewType: NEW_AND_OLD_IMAGES

  FirePredictionDeviceStateTable:
    Type: AWS::DynamoDB::Table
    Properties:
      TableName: FirePrediction-DeviceState
      BillingMode: PAY_PER_REQUEST
      AttributeDefinitions:
        - AttributeName: device_id
          AttributeType: S
      KeySchema:
        - AttributeName: device_id
          KeyType: HASH

  FirePredictionAlertsTable:
    Type: AWS::DynamoDB::Table
    Properties:
      TableName: FirePrediction-Alerts
      BillingMode: PAY_PER_REQUEST
      AttributeDefinitions:
        - AttributeName: device_id
          AttributeType: S
        - AttributeName: alert_timestamp
          AttributeType: S
      KeySchema:
        - AttributeName: device_id
          KeyType: HASH
        - AttributeName: alert_timestamp
          KeyType: RANGE
      TimeToLiveSpecification:
        AttributeName: ttl
        Enabled: true
```

### 1.4 DynamoDB í•¸ë“¤ëŸ¬ êµ¬í˜„

```python
from boto3.dynamodb.conditions import Key
from decimal import Decimal
import json

class DynamoDBHandler:
    def __init__(self):
        self.dynamodb = boto3.resource('dynamodb')
        self.raw_data_table = self.dynamodb.Table('FirePrediction-RawData')
        self.device_state_table = self.dynamodb.Table('FirePrediction-DeviceState')
        self.alerts_table = self.dynamodb.Table('FirePrediction-Alerts')
    
    def save_raw_data(self, processed_data):
        """ì›ì‹œ ë°ì´í„° ì €ì¥"""
        try:
            # TTL ì„¤ì • (30ì¼ í›„ ìë™ ì‚­ì œ)
            ttl = int((datetime.now().timestamp() + (30 * 24 * 60 * 60)))
            
            # Decimal ë³€í™˜ (DynamoDB ìš”êµ¬ì‚¬í•­)
            item = self.convert_to_dynamodb_format({
                'device_id': str(processed_data['device_id']),
                'timestamp': processed_data['timestamp'],
                'ttl': ttl,
                'idx': processed_data['idx'],
                'status_bits': processed_data['status_bits'],
                'voltage': processed_data['voltage'],
                'voltage_low': processed_data['voltage_low'],
                'current': processed_data['current'],
                'leakage_current': processed_data['leakage_current'],
                'resistive_leakage': processed_data['resistive_leakage'],
                'power': processed_data['power'],
                'pcb_temperature': processed_data['pcb_temperature'],
                'sensor_temperature': processed_data['sensor_temperature'],
                'co_gas': processed_data['co_gas'],
                'voc': processed_data['voc'],
                'power_factor': processed_data['power_factor'],
                'arc_count': processed_data['arc_count'],
                'alarm_status': processed_data['alarm_status'],
                'risk_indicators': processed_data['risk_indicators']
            })
            
            self.raw_data_table.put_item(Item=item)
            return True
            
        except Exception as e:
            logger.error(f"DynamoDB save error: {e}")
            return False
    
    def convert_to_dynamodb_format(self, data):
        """Python ë°ì´í„°ë¥¼ DynamoDB í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
        if isinstance(data, dict):
            return {k: self.convert_to_dynamodb_format(v) for k, v in data.items()}
        elif isinstance(data, list):
            return [self.convert_to_dynamodb_format(item) for item in data]
        elif isinstance(data, float):
            return Decimal(str(data))
        elif isinstance(data, int) and not isinstance(data, bool):
            return data
        else:
            return data
    
    def update_device_state(self, device_id, current_data, analysis_result):
        """ë””ë°”ì´ìŠ¤ ìƒíƒœ ì—…ë°ì´íŠ¸"""
        try:
            response = self.device_state_table.update_item(
                Key={'device_id': str(device_id)},
                UpdateExpression="""
                    SET 
                        last_seen = :timestamp,
                        current_risk_level = :risk_level,
                        current_risk_score = :risk_score,
                        last_temperature = :temp,
                        last_arc_count = :arc,
                        total_alarms = if_not_exists(total_alarms, :zero) + :alarm_count,
                        last_status_bits = :status,
                        last_idx = :idx
                """,
                ExpressionAttributeValues={
                    ':timestamp': current_data['timestamp'],
                    ':risk_level': analysis_result.get('risk_level', 'UNKNOWN'),
                    ':risk_score': Decimal(str(analysis_result.get('risk_score', 0))),
                    ':temp': Decimal(str(max(current_data['pcb_temperature'], 
                                           current_data['sensor_temperature']))),
                    ':arc': current_data['arc_count'],
                    ':alarm_count': current_data['alarm_status']['total_alarm_count'],
                    ':status': current_data['status_bits'],
                    ':idx': current_data['idx'],
                    ':zero': 0
                },
                ReturnValues='ALL_NEW'
            )
            
            return response['Attributes']
            
        except Exception as e:
            logger.error(f"Device state update error: {e}")
            return None
    
    def get_device_history(self, device_id, hours=24):
        """ë””ë°”ì´ìŠ¤ ì´ë ¥ ì¡°íšŒ"""
        try:
            from datetime import timedelta
            
            start_time = (datetime.now() - timedelta(hours=hours)).isoformat()
            
            response = self.raw_data_table.query(
                KeyConditionExpression=Key('device_id').eq(str(device_id)) & 
                                     Key('timestamp').gte(start_time),
                ScanIndexForward=False,  # ìµœì‹  ìˆœ
                Limit=1000
            )
            
            return response['Items']
            
        except Exception as e:
            logger.error(f"History query error: {e}")
            return []
    
    def save_alert(self, device_id, alert_data):
        """ì•Œë¦¼ ì €ì¥"""
        try:
            ttl = int((datetime.now().timestamp() + (90 * 24 * 60 * 60)))  # 90ì¼
            
            item = self.convert_to_dynamodb_format({
                'device_id': str(device_id),
                'alert_timestamp': datetime.now().isoformat(),
                'ttl': ttl,
                'alert_level': alert_data['level'],
                'alert_type': alert_data['type'],
                'risk_score': alert_data['risk_score'],
                'message': alert_data['message'],
                'sensor_data': alert_data.get('sensor_data', {}),
                'resolved': False
            })
            
            self.alerts_table.put_item(Item=item)
            return True
            
        except Exception as e:
            logger.error(f"Alert save error: {e}")
            return False
```


# AWS IoT Core í™”ì¬ ì˜ˆì¸¡ ì‹œìŠ¤í…œ - Part 2: Lambda ì²˜ë¦¬ ë° ì‹¤ì‹œê°„ ë¶„ì„

## 2. Lambda í•¨ìˆ˜ë¥¼ í†µí•œ ì‹¤ì‹œê°„ ë°ì´í„° ì²˜ë¦¬

### 2.1 ë©”ì¸ Lambda í•¸ë“¤ëŸ¬

```python
import json
import boto3
import numpy as np
from datetime import datetime
import os
from sklearn.ensemble import IsolationForest
import joblib
import logging

logger = logging.getLogger()
logger.setLevel(logging.INFO)

def lambda_handler(event, context):
    """ë©”ì¸ Lambda í•¸ë“¤ëŸ¬ - IoT Ruleì—ì„œ íŠ¸ë¦¬ê±°"""
    processor = DiscreteDataProcessor()
    analyzer = RealTimeAnalyzer(processor)
    
    try:
        # IoT ë©”ì‹œì§€ íŒŒì‹±
        iot_data = event if isinstance(event, dict) else json.loads(event)
        logger.info(f"Processing data for device: {iot_data.get('deviceId')}")
        
        # ë°ì´í„° ì²˜ë¦¬
        processed_data = processor.parse_discrete_data(iot_data)
        if not processed_data:
            return {'statusCode': 400, 'body': 'Data processing failed'}
        
        # ë°ì´í„° ìœ íš¨ì„± ê²€ì¦
        validation = processor.validate_discrete_data(processed_data)
        if not validation['is_valid']:
            logger.warning(f"Invalid data: {validation['errors']}")
            # ìœ íš¨í•˜ì§€ ì•Šì€ ë°ì´í„°ë„ ë¡œê¹…ì„ ìœ„í•´ ì €ì¥
        
        # DynamoDBì— ì›ì‹œ ë°ì´í„° ì €ì¥
        save_success = processor.dynamo_handler.save_raw_data(processed_data)
        if not save_success:
            logger.error("Failed to save raw data")
        
        # ì‹¤ì‹œê°„ ë¶„ì„
        analysis_result = analyzer.perform_realtime_analysis(processed_data)
        
        # ë””ë°”ì´ìŠ¤ ìƒíƒœ ì—…ë°ì´íŠ¸
        processor.dynamo_handler.update_device_state(
            processed_data['device_id'], 
            processed_data, 
            analysis_result
        )
        
        # Kinesisë¡œ ìŠ¤íŠ¸ë¦¬ë° ë°ì´í„° ì „ì†¡
        kinesis_success = processor.send_to_kinesis(processed_data, analysis_result)
        
        # ìœ„í—˜ë„ì— ë”°ë¥¸ ì¦‰ì‹œ ì¡°ì¹˜
        if analysis_result['risk_level'] in ['HIGH', 'CRITICAL']:
            processor.handle_high_risk_event(processed_data, analysis_result)
        
        return {
            'statusCode': 200,
            'body': json.dumps({
                'device_id': processed_data['device_id'],
                'risk_level': analysis_result['risk_level'],
                'risk_score': float(analysis_result['risk_score']),
                'validation': validation,
                'kinesis_sent': kinesis_success
            })
        }
        
    except Exception as e:
        logger.error(f"Lambda execution error: {e}")
        return {'statusCode': 500, 'body': f'Error: {str(e)}'}

class RealTimeAnalyzer:
    def __init__(self, processor):
        self.processor = processor
        self.dynamo_handler = processor.dynamo_handler
        
    def perform_realtime_analysis(self, data):
        """ì‹¤ì‹œê°„ ìœ„í—˜ë„ ë¶„ì„"""
        # 1. ê¸°ë³¸ ê·œì¹™ ê¸°ë°˜ ë¶„ì„
        rule_based_risk = self.rule_based_analysis(data)
        
        # 2. ë””ë°”ì´ìŠ¤ ì´ë ¥ ê¸°ë°˜ ë¶„ì„
        historical_context = self.get_historical_context(data['device_id'])
        
        # 3. ìƒíƒœ ë¹„íŠ¸ ê¸°ë°˜ ë¶„ì„
        status_risk = self.analyze_status_bits(data['alarm_status'])
        
        # 4. ì„¼ì„œ íŒ¨í„´ ë¶„ì„
        pattern_risk = self.analyze_sensor_patterns(data)
        
        # ì¢…í•© ìœ„í—˜ë„ ê³„ì‚°
        final_risk = self.calculate_final_risk(
            rule_based_risk, 
            historical_context, 
            status_risk, 
            pattern_risk
        )
        
        # ìœ„í—˜ ìˆ˜ì¤€ ê²°ì •
        risk_level = self.determine_risk_level(final_risk)
        
        return {
            'risk_level': risk_level,
            'risk_score': final_risk,
            'rule_based_risk': rule_based_risk,
            'status_risk': status_risk,
            'pattern_risk': pattern_risk,
            'confidence': self.calculate_confidence(rule_based_risk, status_risk),
            'analysis_timestamp': datetime.now().isoformat()
        }
    
    def rule_based_analysis(self, data):
        """ê·œì¹™ ê¸°ë°˜ ìœ„í—˜ë„ ë¶„ì„ (Discrete í•„ë“œ ê¸°ë°˜)"""
        risk_score = 0
        
        # ì˜¨ë„ ê¸°ë°˜ ìœ„í—˜ë„ (Discrete ì£¼ì„: 60Â°C ê¸°ì¤€)
        max_temp = max(data['pcb_temperature'], data['sensor_temperature'])
        if max_temp > 60:
            risk_score += 0.4
        elif max_temp > 50:
            risk_score += 0.3
        elif max_temp > 40:
            risk_score += 0.1
        
        # ì „ì•• ì´ìƒ (Discrete ì£¼ì„: 234V ì´ìƒ ê³ ì „ì••, 190V ì´í•˜ ì €ì „ì••)
        if data['voltage'] > 234 or data['voltage_low'] < 190:
            risk_score += 0.15
        
        # ì „ë¥˜ ì´ìƒ (Discrete ì£¼ì„: 100A ê¸°ì¤€)
        if data['current'] > 100:
            risk_score += 0.2
        
        # ëˆ„ì „ (Discrete ì£¼ì„: 8mA ê¸°ì¤€)
        if data['leakage_current'] > 8 or data['resistive_leakage'] > 8:
            risk_score += 0.25
        
        # ì•„í¬ ë°œìƒ
        if data['arc_count'] > 0:
            risk_score += min(0.2, data['arc_count'] * 0.02)
        
        # ê°€ìŠ¤ ë†ë„ (Discrete ì£¼ì„ ê¸°ì¤€)
        if data['co_gas'] > 17:  # 17ppm
            risk_score += 0.15
        if data['voc'] > 400:    # 400Î¼g/mÂ³
            risk_score += 0.1
        
        # ì—­ë¥  ì´ìƒ (Discrete ì£¼ì„: 90% ê¸°ì¤€)
        if data['power_factor'] < 0.9:
            risk_score += 0.05
        
        return min(1.0, risk_score)
    
    def analyze_status_bits(self, alarm_status):
        """ìƒíƒœ ë¹„íŠ¸ ê¸°ë°˜ ìœ„í—˜ë„ ë¶„ì„"""
        risk_score = 0
        
        # ê° ì•ŒëŒë³„ ê°€ì¤‘ì¹˜
        alarm_weights = {
            'overvoltage': 0.1,
            'power_outage': 0.15,
            'overcurrent': 0.2,
            'total_leakage': 0.25,
            'resistive_leakage': 0.2,
            'power_anomaly': 0.1,
            'pcb_temp_alarm': 0.3,
            'sensor_temp_alarm': 0.3,
            'co_alarm': 0.2,
            'voc_alarm': 0.15,
            'power_factor_alarm': 0.05,
            'arc_alarm': 0.25
        }
        
        for alarm, weight in alarm_weights.items():
            if alarm_status.get(alarm, False):
                risk_score += weight
        
        # ë‹¤ì¤‘ ì•ŒëŒ ë°œìƒ ì‹œ ì¶”ê°€ ê°€ì¤‘ì¹˜
        if alarm_status['total_alarm_count'] > 2:
            risk_score += 0.1 * (alarm_status['total_alarm_count'] - 2)
        
        # í†µì‹  ì˜¤ë¥˜ ì²˜ë¦¬
        if alarm_status.get('communication_error', False):
            return 0.8  # í†µì‹  ì˜¤ë¥˜ëŠ” ë³„ë„ ì²˜ë¦¬
        
        return min(1.0, risk_score)
    
    def analyze_sensor_patterns(self, data):
        """ì„¼ì„œ ë°ì´í„° íŒ¨í„´ ë¶„ì„"""
        pattern_risk = 0
        
        # ì˜¨ë„ ë¶ˆê· í˜• (PCBì™€ ì„¼ì„œ ì˜¨ë„ ì°¨ì´)
        temp_diff = abs(data['pcb_temperature'] - data['sensor_temperature'])
        if temp_diff > 20:
            pattern_risk += 0.15
        elif temp_diff > 10:
            pattern_risk += 0.05
        
        # ì „ë ¥ ê³„ì‚° ë° ì´ìƒ íƒì§€
        calculated_power = data['voltage'] * data['current'] * data['power_factor']
        if abs(calculated_power - data['power']) > calculated_power * 0.2:
            pattern_risk += 0.1
        
        # ì €í•­ ê³„ì‚° (ì˜´ì˜ ë²•ì¹™)
        if data['current'] > 0.1:  # ì „ë¥˜ê°€ ìˆì„ ë•Œë§Œ
            resistance = data['voltage'] / data['current']
            if resistance < 1 or resistance > 1000:  # ë¹„ì •ìƒì ì¸ ì €í•­ê°’
                pattern_risk += 0.1
        
        return min(1.0, pattern_risk)
    
    def get_historical_context(self, device_id):
        """ë””ë°”ì´ìŠ¤ ì´ë ¥ ì»¨í…ìŠ¤íŠ¸ ë¶„ì„"""
        try:
            # ìµœê·¼ 1ì‹œê°„ ë°ì´í„° ì¡°íšŒ
            recent_data = self.dynamo_handler.get_device_history(device_id, hours=1)
            
            if len(recent_data) < 5:
                return {'trend': 'INSUFFICIENT_DATA', 'risk_modifier': 1.0}
            
            # ìœ„í—˜ë„ íŠ¸ë Œë“œ ë¶„ì„
            risk_scores = []
            temperatures = []
            arc_counts = []
            
            for item in recent_data[:20]:  # ìµœê·¼ 20ê°œ ë°ì´í„°
                if 'risk_indicators' in item:
                    risk_scores.append(float(item['risk_indicators']['total_risk_score']))
                temperatures.append(float(item['pcb_temperature']))
                arc_counts.append(int(item['arc_count']))
            
            context = {
                'recent_data_count': len(recent_data),
                'avg_temperature': np.mean(temperatures),
                'temp_trend': self.calculate_trend(temperatures),
                'total_arcs_hour': sum(arc_counts),
                'avg_risk_score': np.mean(risk_scores) if risk_scores else 0.5
            }
            
            # ìœ„í—˜ë„ ìˆ˜ì •ì ê³„ì‚°
            risk_modifier = 1.0
            if context['temp_trend'] > 0.1:  # ì˜¨ë„ ìƒìŠ¹ íŠ¸ë Œë“œ
                risk_modifier += 0.2
            if context['total_arcs_hour'] > 10:  # ì•„í¬ ë¹ˆë°œ
                risk_modifier += 0.3
            if context['avg_risk_score'] > 0.6:  # ì§€ì†ì  ê³ ìœ„í—˜
                risk_modifier += 0.1
            
            context['risk_modifier'] = min(2.0, risk_modifier)
            
            return context
            
        except Exception as e:
            logger.error(f"Historical context analysis error: {e}")
            return {'trend': 'ERROR', 'risk_modifier': 1.0}
    
    def calculate_trend(self, values):
        """ì„ í˜• íŠ¸ë Œë“œ ê³„ì‚°"""
        if len(values) < 3:
            return 0
        
        # ë‹¨ìˆœ ì„ í˜• íšŒê·€ë¡œ ê¸°ìš¸ê¸° ê³„ì‚°
        n = len(values)
        x = np.arange(n)
        
        slope = np.polyfit(x, values, 1)[0]
        return slope
    
    def calculate_final_risk(self, rule_risk, historical_context, status_risk, pattern_risk):
        """ì¢…í•© ìœ„í—˜ë„ ê³„ì‚°"""
        # ê¸°ë³¸ ê°€ì¤‘ í‰ê· 
        base_risk = (
            rule_risk * 0.4 +
            status_risk * 0.3 +
            pattern_risk * 0.3
        )
        
        # ì´ë ¥ ì»¨í…ìŠ¤íŠ¸ ì ìš©
        risk_modifier = historical_context.get('risk_modifier', 1.0)
        final_risk = base_risk * risk_modifier
        
        return min(1.0, final_risk)
    
    def determine_risk_level(self, risk_score):
        """ìœ„í—˜ ìˆ˜ì¤€ ê²°ì •"""
        if risk_score >= 0.8:
            return 'CRITICAL'
        elif risk_score >= 0.6:
            return 'HIGH'
        elif risk_score >= 0.4:
            return 'MEDIUM'
        elif risk_score >= 0.2:
            return 'LOW'
        else:
            return 'NORMAL'
    
    def calculate_confidence(self, rule_risk, status_risk):
        """ì˜ˆì¸¡ ì‹ ë¢°ë„ ê³„ì‚°"""
        # ê·œì¹™ ê¸°ë°˜ê³¼ ìƒíƒœ ê¸°ë°˜ ìœ„í—˜ë„ì˜ ì¼ì¹˜ë„
        diff = abs(rule_risk - status_risk)
        confidence = 1.0 - (diff * 0.5)
        return max(0.5, min(1.0, confidence))

class AlertManager:
    def __init__(self):
        self.sns = boto3.client('sns')
        self.dynamo_handler = DynamoDBHandler()
        
    def handle_high_risk_event(self, data, analysis_result):
        """ê³ ìœ„í—˜ ì´ë²¤íŠ¸ ì²˜ë¦¬"""
        try:
            device_id = data['device_id']
            risk_level = analysis_result['risk_level']
            risk_score = analysis_result['risk_score']
            
            # ì•Œë¦¼ ë°ì´í„° êµ¬ì„±
            alert_data = {
                'level': risk_level,
                'type': self.determine_alert_type(data, analysis_result),
                'risk_score': risk_score,
                'message': self.generate_alert_message(data, analysis_result),
                'sensor_data': {
                    'temperature': max(data['pcb_temperature'], data['sensor_temperature']),
                    'voltage': data['voltage'],
                    'current': data['current'],
                    'arc_count': data['arc_count'],
                    'co_gas': data['co_gas'],
                    'voc': data['voc']
                },
                'recommendations': self.generate_recommendations(data, analysis_result)
            }
            
            # ì•Œë¦¼ ì €ì¥
            self.dynamo_handler.save_alert(device_id, alert_data)
            
            # SNS ì•Œë¦¼ ë°œì†¡
            if risk_level == 'CRITICAL':
                self.send_critical_alert(device_id, alert_data)
            elif risk_level == 'HIGH':
                self.send_high_risk_alert(device_id, alert_data)
            
            return True
            
        except Exception as e:
            logger.error(f"Alert handling error: {e}")
            return False
    
    def determine_alert_type(self, data, analysis_result):
        """ì•Œë¦¼ ìœ í˜• ê²°ì •"""
        alarm_status = data['alarm_status']
        
        if alarm_status['pcb_temp_alarm'] or alarm_status['sensor_temp_alarm']:
            return 'TEMPERATURE_CRITICAL'
        elif alarm_status['arc_alarm'] or data['arc_count'] > 0:
            return 'ARC_DETECTED'
        elif alarm_status['total_leakage'] or alarm_status['resistive_leakage']:
            return 'ELECTRICAL_LEAKAGE'
        elif alarm_status['overcurrent']:
            return 'OVERCURRENT'
        elif alarm_status['co_alarm'] or alarm_status['voc_alarm']:
            return 'GAS_DETECTION'
        else:
            return 'GENERAL_RISK'
    
    def generate_alert_message(self, data, analysis_result):
        """ì•Œë¦¼ ë©”ì‹œì§€ ìƒì„±"""
        device_id = data['device_id']
        risk_level = analysis_result['risk_level']
        risk_score = analysis_result['risk_score']
        
        message = f"ğŸš¨ í™”ì¬ ìœ„í—˜ ê°ì§€ - ë””ë°”ì´ìŠ¤ {device_id}\n"
        message += f"ìœ„í—˜ë„: {risk_level} ({risk_score:.2f})\n"
        message += f"ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n"
        
        # ì£¼ìš” ìœ„í—˜ ìš”ì†Œ
        if data['alarm_status']['pcb_temp_alarm'] or data['alarm_status']['sensor_temp_alarm']:
            max_temp = max(data['pcb_temperature'], data['sensor_temperature'])
            message += f"âš ï¸ ê³ ì˜¨ ê°ì§€: {max_temp:.1f}Â°C\n"
        
        if data['arc_count'] > 0:
            message += f"âš¡ ì•„í¬ ë°œìƒ: {data['arc_count']}íšŒ\n"
        
        if data['alarm_status']['total_leakage']:
            message += f"ğŸ’§ ëˆ„ì „ ê°ì§€: {data['leakage_current']:.1f}mA\n"
        
        if data['co_gas'] > 17:
            message += f"â˜ï¸ CO ê°€ìŠ¤: {data['co_gas']:.1f}ppm\n"
        
        if data['voc'] > 400:
            message += f"ğŸŒ«ï¸ VOC: {data['voc']:.1f}Î¼g/mÂ³\n"
        
        return message
    
    def generate_recommendations(self, data, analysis_result):
        """ê¶Œì¥ ì¡°ì¹˜ì‚¬í•­ ìƒì„±"""
        recommendations = []
        
        if data['alarm_status']['pcb_temp_alarm'] or data['alarm_status']['sensor_temp_alarm']:
            recommendations.append("ì¦‰ì‹œ ëƒ‰ê° ì¡°ì¹˜ ë° í™˜ê¸° í™•ì¸")
            recommendations.append("ì „ì› ì°¨ë‹¨ ê²€í† ")
        
        if data['arc_count'] > 0:
            recommendations.append("ë°°ì„  ì—°ê²°ë¶€ ì ê²€")
            recommendations.append("ì ˆì—°ì²´ ìƒíƒœ í™•ì¸")
        
        if data['alarm_status']['total_leakage']:
            recommendations.append("ëˆ„ì „ ì°¨ë‹¨ê¸° ì‘ë™ í™•ì¸")
            recommendations.append("ì ‘ì§€ ìƒíƒœ ì ê²€")
        
        if data['co_gas'] > 17 or data['voc'] > 400:
            recommendations.append("ì¦‰ì‹œ í™˜ê¸° ì‹¤ì‹œ")
            recommendations.append("ê°€ìŠ¤ ëˆ„ì¶œì› í™•ì¸")
        
        if analysis_result['risk_level'] == 'CRITICAL':
            recommendations.append("ğŸš¨ ì¦‰ì‹œ í˜„ì¥ ì ê²€ í•„ìš”")
            recommendations.append("í•„ìš”ì‹œ ì „ì› ì°¨ë‹¨ ê³ ë ¤")
        
        return recommendations
    
    def send_critical_alert(self, device_id, alert_data):
        """ê¸´ê¸‰ ì•Œë¦¼ ë°œì†¡"""
        try:
            topic_arn = os.environ.get('CRITICAL_ALERT_TOPIC_ARN')
            if not topic_arn:
                logger.warning("Critical alert topic ARN not configured")
                return
            
            message = {
                'default': alert_data['message'],
                'sms': f"ğŸš¨ê¸´ê¸‰ğŸš¨ í™”ì¬ìœ„í—˜ ë””ë°”ì´ìŠ¤{device_id} ìœ„í—˜ë„:{alert_data['risk_score']:.2f}",
                'email': self.format_email_alert(device_id, alert_data)
            }
            
            self.sns.publish(
                TopicArn=topic_arn,
                Message=json.dumps(message),
                MessageStructure='json',
                Subject=f"ğŸš¨ CRITICAL: í™”ì¬ ìœ„í—˜ ê°ì§€ - ë””ë°”ì´ìŠ¤ {device_id}"
            )
            
        except Exception as e:
            logger.error(f"Critical alert send error: {e}")
    
    def send_high_risk_alert(self, device_id, alert_data):
        """ê³ ìœ„í—˜ ì•Œë¦¼ ë°œì†¡"""
        try:
            topic_arn = os.environ.get('HIGH_RISK_ALERT_TOPIC_ARN')
            if not topic_arn:
                logger.warning("High risk alert topic ARN not configured")
                return
            
            self.sns.publish(
                TopicArn=topic_arn,
                Message=alert_data['message'],
                Subject=f"âš ï¸ HIGH RISK: í™”ì¬ ìœ„í—˜ ì¦ê°€ - ë””ë°”ì´ìŠ¤ {device_id}"
            )
            
        except Exception as e:
            logger.error(f"High risk alert send error: {e}")
    
    def format_email_alert(self, device_id, alert_data):
        """ì´ë©”ì¼ ì•Œë¦¼ í¬ë§·"""
        html_content = f"""
        <html>
        <body>
            <h2 style="color: red;">ğŸš¨ í™”ì¬ ìœ„í—˜ ê°ì§€ ì•Œë¦¼</h2>
            <p><strong>ë””ë°”ì´ìŠ¤ ID:</strong> {device_id}</p>
            <p><strong>ìœ„í—˜ ìˆ˜ì¤€:</strong> {alert_data['level']}</p>
            <p><strong>ìœ„í—˜ ì ìˆ˜:</strong> {alert_data['risk_score']:.2f}</p>
            <p><strong>ê°ì§€ ì‹œê°„:</strong> {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>
            
            <h3>ì„¼ì„œ ë°ì´í„°</h3>
            <ul>
                <li>ì˜¨ë„: {alert_data['sensor_data']['temperature']:.1f}Â°C</li>
                <li>ì „ì••: {alert_data['sensor_data']['voltage']:.1f}V</li>
                <li>ì „ë¥˜: {alert_data['sensor_data']['current']:.1f}A</li>
                <li>ì•„í¬ íšŸìˆ˜: {alert_data['sensor_data']['arc_count']}</li>
                <li>CO ê°€ìŠ¤: {alert_data['sensor_data']['co_gas']:.1f}ppm</li>
                <li>VOC: {alert_data['sensor_data']['voc']:.1f}Î¼g/mÂ³</li>
            </ul>
            
            <h3>ê¶Œì¥ ì¡°ì¹˜ì‚¬í•­</h3>
            <ul>
        """
        
        for recommendation in alert_data['recommendations']:
            html_content += f"<li>{recommendation}</li>"
        
        html_content += """
            </ul>
            <p style="color: red; font-weight: bold;">
                ê¸´ê¸‰í•œ ì¡°ì¹˜ê°€ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì¦‰ì‹œ í˜„ì¥ì„ í™•ì¸í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤.
            </p>
        </body>
        </html>
        """
        
        return html_content

# DiscreteDataProcessorì— ì¶”ê°€ ë©”ì„œë“œë“¤
class DiscreteDataProcessor:
    def __init__(self):
        self.dynamodb = boto3.resource('dynamodb')
        self.kinesis = boto3.client('kinesis')
        self.sns = boto3.client('sns')
        
        # í•¸ë“¤ëŸ¬ë“¤
        self.dynamo_handler = DynamoDBHandler()
        self.alert_manager = AlertManager()
        
        # Kinesis ìŠ¤íŠ¸ë¦¼ëª…
        self.stream_name = os.environ.get('KINESIS_STREAM_NAME', 'fire-prediction-stream')
    
    def send_to_kinesis(self, processed_data, analysis_result):
        """Kinesisë¡œ ë°ì´í„° ì „ì†¡"""
        try:
            record = {
                'device_id': processed_data['device_id'],
                'timestamp': processed_data['timestamp'],
                'sensor_data': {
                    'voltage': processed_data['voltage'],
                    'current': processed_data['current'],
                    'temperature': max(processed_data['pcb_temperature'], 
                                     processed_data['sensor_temperature']),
                    'leakage_current': processed_data['leakage_current'],
                    'co_gas': processed_data['co_gas'],
                    'voc': processed_data['voc'],
                    'power_factor': processed_data['power_factor'],
                    'arc_count': processed_data['arc_count']
                },
                'risk_analysis': analysis_result,
                'alarm_status': processed_data['alarm_status'],
                'discrete_idx': processed_data['idx']
            }
            
            response = self.kinesis.put_record(
                StreamName=self.stream_name,
                Data=json.dumps(record, default=str),
                PartitionKey=str(processed_data['device_id'])
            )
            
            logger.info(f"Data sent to Kinesis: {response['SequenceNumber']}")
            return True
            
        except Exception as e:
            logger.error(f"Kinesis send error: {e}")
            return False
    
    def handle_high_risk_event(self, processed_data, analysis_result):
        """ê³ ìœ„í—˜ ì´ë²¤íŠ¸ ì²˜ë¦¬"""
        return self.alert_manager.handle_high_risk_event(processed_data, analysis_result)

# AWS IoT Core í™”ì¬ ì˜ˆì¸¡ ì‹œìŠ¤í…œ - Part 3: Kinesis ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬ ë° ê³ ê¸‰ ë¶„ì„

## 3. Amazon Kinesisë¥¼ í†µí•œ ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬

### 3.1 Kinesis Data Streams ì„¤ì •

```yaml
# CloudFormation í…œí”Œë¦¿
Resources:
  FirePredictionKinesisStream:
    Type: AWS::Kinesis::Stream
    Properties:
      Name: fire-prediction-stream
      ShardCount: 2
      RetentionPeriodHours: 24
      
  KinesisAnalyticsApplication:
    Type: AWS::KinesisAnalytics::Application
    Properties:
      ApplicationName: fire-prediction-analytics
      ApplicationDescription: "Real-time fire risk analytics"
      Inputs:
        - NamePrefix: "SOURCE_SQL_STREAM"
          KinesisStreamsInput:
            ResourceARN: !GetAtt FirePredictionKinesisStream.Arn
            RoleARN: !GetAtt KinesisAnalyticsRole.Arn
          InputSchema:
            RecordColumns:
              - Name: device_id
                SqlType: VARCHAR(32)
                Mapping: $.device_id
              - Name: timestamp
                SqlType: TIMESTAMP
                Mapping: $.timestamp
              - Name: temperature
                SqlType: DOUBLE
                Mapping: $.sensor_data.temperature
              - Name: current
                SqlType: DOUBLE
                Mapping: $.sensor_data.current
              - Name: voltage
                SqlType: DOUBLE
                Mapping: $.sensor_data.voltage
              - Name: arc_count
                SqlType: INTEGER
                Mapping: $.sensor_data.arc_count
              - Name: risk_score
                SqlType: DOUBLE
                Mapping: $.risk_analysis.risk_score
            RecordFormat:
              RecordFormatType: JSON
```

### 3.2 Kinesis Data Processor Lambda

```python
import boto3
import json
from datetime import datetime, timedelta
import base64
import numpy as np
from collections import defaultdict
import logging

logger = logging.getLogger()
logger.setLevel(logging.INFO)

class KinesisDataProcessor:
    def __init__(self):
        self.sns = boto3.client('sns')
        self.cloudwatch = boto3.client('cloudwatch')
        self.dynamodb = boto3.resource('dynamodb')
        self.predictions_table = self.dynamodb.Table('FirePrediction-Predictions')
        self.s3 = boto3.client('s3')
        
        # ì§‘ê³„ìš© ì„ì‹œ ìŠ¤í† ë¦¬ì§€
        self.batch_aggregations = defaultdict(list)
        
    def lambda_handler(self, event, context):
        """Kinesis íŠ¸ë¦¬ê±° Lambda í•¨ìˆ˜"""
        try:
            processed_records = 0
            high_risk_devices = []
            device_aggregations = defaultdict(list)
            
            # ë°°ì¹˜ ë ˆì½”ë“œ ì²˜ë¦¬
            for record in event['Records']:
                # Kinesis ë ˆì½”ë“œ ë””ì½”ë”©
                payload = json.loads(base64.b64decode(record['kinesis']['data']))
                
                # ë°ì´í„° ì²˜ë¦¬
                result = self.process_kinesis_record(payload)
                device_aggregations[payload['device_id']].append(result)
                
                if result['action_required']:
                    high_risk_devices.append(result)
                
                processed_records += 1
            
            # ë””ë°”ì´ìŠ¤ë³„ ë°°ì¹˜ ë¶„ì„
            batch_analysis_results = self.perform_batch_analysis(device_aggregations)
            
            # ë°°ì¹˜ ì²˜ë¦¬ ê²°ê³¼ ë¡œê¹…
            self.log_batch_metrics(processed_records, len(high_risk_devices))
            
            # ê³ ìœ„í—˜ ë””ë°”ì´ìŠ¤ì— ëŒ€í•œ ì¶”ê°€ ì¡°ì¹˜
            if high_risk_devices:
                self.handle_high_risk_batch(high_risk_devices)
            
            # S3ì— ë°°ì¹˜ ë°ì´í„° ì €ì¥ (ë¶„ì„ìš©)
            self.save_batch_to_s3(device_aggregations, batch_analysis_results)
            
            return {
                'statusCode': 200,
                'processed_records': processed_records,
                'high_risk_count': len(high_risk_devices),
                'batch_analysis': batch_analysis_results
            }
            
        except Exception as e:
            logger.error(f"Kinesis processing error: {e}")
            return {'statusCode': 500, 'error': str(e)}
    
    def process_kinesis_record(self, payload):
        """ê°œë³„ Kinesis ë ˆì½”ë“œ ì²˜ë¦¬"""
        device_id = payload['device_id']
        risk_score = payload['risk_analysis']['risk_score']
        timestamp = payload['timestamp']
        
        # ì‹œê³„ì—´ íŒ¨í„´ ë¶„ì„
        pattern_analysis = self.analyze_temporal_patterns(device_id, payload)
        
        # ì„¼ì„œ ë°ì´í„° ì´ìƒì¹˜ íƒì§€
        anomaly_analysis = self.detect_sensor_anomalies(payload['sensor_data'])
        
        # ë””ë°”ì´ìŠ¤ ê°„ ìƒê´€ê´€ê³„ ë¶„ì„
        correlation_analysis = self.analyze_device_correlation(device_id, payload)
        
        # ì˜ˆì¸¡ ê²°ê³¼ êµ¬ì„±
        prediction_result = {
            'device_id': device_id,
            'timestamp': timestamp,
            'risk_score': risk_score,
            'pattern_analysis': pattern_analysis,
            'anomaly_analysis': anomaly_analysis,
            'correlation_analysis': correlation_analysis,
            'sensor_summary': self.summarize_sensor_data(payload['sensor_data']),
            'prediction_confidence': self.calculate_prediction_confidence(payload)
        }
        
        # ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥
        self.save_prediction_result(prediction_result)
        
        # CloudWatch ë©”íŠ¸ë¦­ ì „ì†¡
        self.send_cloudwatch_metrics(device_id, payload)
        
        # ì•¡ì…˜ í•„ìš” ì—¬ë¶€ ê²°ì •
        action_required = self.determine_action_required(
            risk_score, 
            pattern_analysis, 
            anomaly_analysis,
            payload['alarm_status']
        )
        
        return {
            'device_id': device_id,
            'risk_score': risk_score,
            'action_required': action_required,
            'prediction_result': prediction_result,
            'raw_payload': payload
        }
    
    def analyze_temporal_patterns(self, device_id, current_payload):
        """ì‹œê³„ì—´ íŒ¨í„´ ë¶„ì„"""
        try:
            # Redisë‚˜ DynamoDBì—ì„œ ìµœê·¼ ë°ì´í„° ì¡°íšŒ (ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ìºì‹œ ì‚¬ìš©)
            historical_data = self.get_recent_kinesis_data(device_id, minutes=60)
            
            if len(historical_data) < 10:
                return {'trend': 'INSUFFICIENT_DATA', 'pattern': 'UNKNOWN'}
            
            # ì˜¨ë„ íŠ¸ë Œë“œ ë¶„ì„
            temperatures = [data['sensor_data']['temperature'] for data in historical_data]
            temp_trend = self.calculate_trend(temperatures)
            temp_velocity = self.calculate_velocity(temperatures)
            
            # ìœ„í—˜ë„ íŠ¸ë Œë“œ ë¶„ì„
            risk_scores = [data['risk_analysis']['risk_score'] for data in historical_data]
            risk_trend = self.calculate_trend(risk_scores)
            
            # ì•„í¬ ë°œìƒ íŒ¨í„´
            arc_counts = [data['sensor_data']['arc_count'] for data in historical_data]
            arc_pattern = self.analyze_arc_pattern(arc_counts)
            
            # ì£¼ê¸°ì„± ë¶„ì„ (FFT ê¸°ë°˜)
            periodicity = self.detect_periodicity(temperatures)
            
            return {
                'temperature_trend': temp_trend,
                'temperature_velocity': temp_velocity,
                'risk_trend': risk_trend,
                'arc_pattern': arc_pattern,
                'trend': self.determine_overall_trend(temp_trend, risk_trend),
                'periodicity': periodicity,
                'data_points': len(historical_data),
                'trend_confidence': self.calculate_trend_confidence(temperatures, risk_scores)
            }
            
        except Exception as e:
            logger.error(f"Pattern analysis error: {e}")
            return {'trend': 'ERROR', 'pattern': 'UNKNOWN'}
    
    def detect_sensor_anomalies(self, sensor_data):
        """ì„¼ì„œ ë°ì´í„° ì´ìƒì¹˜ íƒì§€"""
        anomalies = []
        
        # í†µê³„ì  ì´ìƒì¹˜ íƒì§€ (Z-score ê¸°ë°˜)
        z_scores = {}
        
        # ê° ì„¼ì„œë³„ ê¸°ì¤€ê°’ (ì •ìƒ ë²”ìœ„)
        normal_ranges = {
            'temperature': (20, 60),
            'voltage': (200, 250),
            'current': (0, 120),
            'co_gas': (0, 20),
            'voc': (0, 500),
            'power_factor': (0.8, 1.0)
        }
        
        for sensor, (min_val, max_val) in normal_ranges.items():
            if sensor in sensor_data:
                value = sensor_data[sensor]
                
                # ë²”ìœ„ ì´ˆê³¼ ê²€ì‚¬
                if value < min_val or value > max_val:
                    anomalies.append({
                        'sensor': sensor,
                        'value': value,
                        'type': 'RANGE_VIOLATION',
                        'severity': 'HIGH' if sensor == 'temperature' else 'MEDIUM'
                    })
        
        # ì„¼ì„œ ê°„ ì¼ê´€ì„± ê²€ì‚¬
        consistency_check = self.check_sensor_consistency(sensor_data)
        if not consistency_check['consistent']:
            anomalies.extend(consistency_check['anomalies'])
        
        return {
            'anomalies': anomalies,
            'anomaly_count': len(anomalies),
            'severity_distribution': self.count_anomaly_severity(anomalies)
        }
    
    def check_sensor_consistency(self, sensor_data):
        """ì„¼ì„œ ê°„ ì¼ê´€ì„± ê²€ì‚¬"""
        anomalies = []
        
        # ì „ë ¥ ì¼ê´€ì„± ê²€ì‚¬ (P = V * I * PF)
        if all(k in sensor_data for k in ['voltage', 'current', 'power_factor']):
            calculated_power = (sensor_data['voltage'] * 
                              sensor_data['current'] * 
                              sensor_data['power_factor'])
            
            # ì‹¤ì œ ì „ë ¥ê°’ì´ ìˆë‹¤ë©´ ë¹„êµ
            if 'power' in sensor_data:
                power_diff = abs(calculated_power - sensor_data['power'])
                if power_diff > calculated_power * 0.2:  # 20% ì´ìƒ ì°¨ì´
                    anomalies.append({
                        'sensor': 'power_consistency',
                        'type': 'CALCULATION_MISMATCH',
                        'calculated': calculated_power,
                        'measured': sensor_data['power'],
                        'difference': power_diff,
                        'severity': 'MEDIUM'
                    })
        
        # ì˜¨ë„ ì„¼ì„œ ì¼ê´€ì„± (PCB vs ì„¼ì„œ)
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” payloadì— ë‘ ì˜¨ë„ê°’ì´ ëª¨ë‘ ìˆë‹¤ê³  ê°€ì •
        
        return {
            'consistent': len(anomalies) == 0,
            'anomalies': anomalies
        }
    
    def analyze_device_correlation(self, device_id, payload):
        """ë””ë°”ì´ìŠ¤ ê°„ ìƒê´€ê´€ê³„ ë¶„ì„"""
        try:
            # ë™ì¼ ì§€ì—­/ê±´ë¬¼ì˜ ë‹¤ë¥¸ ë””ë°”ì´ìŠ¤ ë°ì´í„° ì¡°íšŒ
            nearby_devices = self.get_nearby_devices_data(device_id)
            
            if not nearby_devices:
                return {'correlation': 'NO_NEARBY_DEVICES'}
            
            correlations = {}
            current_temp = payload['sensor_data']['temperature']
            current_risk = payload['risk_analysis']['risk_score']
            
            for other_device in nearby_devices:
                # ì˜¨ë„ ìƒê´€ê´€ê³„
                temp_correlation = self.calculate_correlation(
                    current_temp, 
                    other_device['temperature']
                )
                
                # ìœ„í—˜ë„ ìƒê´€ê´€ê³„
                risk_correlation = self.calculate_correlation(
                    current_risk,
                    other_device['risk_score']
                )
                
                correlations[other_device['device_id']] = {
                    'temperature_correlation': temp_correlation,
                    'risk_correlation': risk_correlation,
                    'distance': other_device.get('distance', 'unknown')
                }
            
            # í´ëŸ¬ìŠ¤í„° ìœ„í—˜ë„ ë¶„ì„
            cluster_risk = self.analyze_cluster_risk(device_id, nearby_devices, payload)
            
            return {
                'correlations': correlations,
                'cluster_risk': cluster_risk,
                'nearby_device_count': len(nearby_devices)
            }
            
        except Exception as e:
            logger.error(f"Correlation analysis error: {e}")
            return {'correlation': 'ERROR'}
    
    def perform_batch_analysis(self, device_aggregations):
        """ë°°ì¹˜ ë¶„ì„ ìˆ˜í–‰"""
        batch_results = {}
        
        for device_id, records in device_aggregations.items():
            if len(records) < 2:
                continue
            
            # ë°°ì¹˜ ë‚´ íŠ¸ë Œë“œ ë¶„ì„
            risk_scores = [r['risk_score'] for r in records]
            trend_analysis = {
                'risk_trend': self.calculate_trend(risk_scores),
                'risk_volatility': np.std(risk_scores),
                'max_risk': max(risk_scores),
                'min_risk': min(risk_scores),
                'avg_risk': np.mean(risk_scores)
            }
            
            # ë°°ì¹˜ ë‚´ ì´ìƒì¹˜ íƒì§€
            batch_anomalies = self.detect_batch_anomalies(records)
            
            # ë°°ì¹˜ ì¶”ì²œì‚¬í•­
            recommendations = self.generate_batch_recommendations(
                device_id, 
                trend_analysis, 
                batch_anomalies
            )
            
            batch_results[device_id] = {
                'trend_analysis': trend_analysis,
                'batch_anomalies': batch_anomalies,
                'recommendations': recommendations,
                'record_count': len(records)
            }
        
        return batch_results
    
    def detect_batch_anomalies(self, records):
        """ë°°ì¹˜ ë‚´ ì´ìƒì¹˜ íƒì§€"""
        anomalies = []
        
        # ê¸‰ê²©í•œ ìœ„í—˜ë„ ë³€í™” íƒì§€
        risk_scores = [r['risk_score'] for r in records]
        for i in range(1, len(risk_scores)):
            risk_change = abs(risk_scores[i] - risk_scores[i-1])
            if risk_change > 0.3:  # 30% ì´ìƒ ê¸‰ë³€
                anomalies.append({
                    'type': 'RAPID_RISK_CHANGE',
                    'change': risk_change,
                    'timestamp': records[i]['prediction_result']['timestamp']
                })
        
        # ì§€ì†ì ì¸ ê³ ìœ„í—˜ ìƒíƒœ
        high_risk_streak = 0
        for record in records:
            if record['risk_score'] > 0.7:
                high_risk_streak += 1
            else:
                high_risk_streak = 0
        
        if high_risk_streak >= 3:
            anomalies.append({
                'type': 'SUSTAINED_HIGH_RISK',
                'streak_length': high_risk_streak
            })
        
        return anomalies
    
    def save_batch_to_s3(self, device_aggregations, batch_analysis_results):
        """ë°°ì¹˜ ë°ì´í„°ë¥¼ S3ì— ì €ì¥ (ë¶„ì„ìš©)"""
        try:
            bucket_name = os.environ.get('ANALYTICS_BUCKET')
            if not bucket_name:
                return
            
            # íŒŒì¼ëª… ìƒì„± (íŒŒí‹°ì…˜ ê¸°ë°˜)
            now = datetime.now()
            s3_key = f"kinesis-batch/{now.year}/{now.month:02d}/{now.day:02d}/{now.hour:02d}/{now.minute:02d}-batch.json"
            
            batch_data = {
                'timestamp': now.isoformat(),
                'device_aggregations': device_aggregations,
                'batch_analysis': batch_analysis_results,
                'record_count': sum(len(records) for records in device_aggregations.values())
            }
            
            self.s3.put_object(
                Bucket=bucket_name,
                Key=s3_key,
                Body=json.dumps(batch_data, default=str),
                ContentType='application/json'
            )
            
        except Exception as e:
            logger.error(f"S3 save error: {e}")
    
    def calculate_trend(self, values):
        """ì„ í˜• íŠ¸ë Œë“œ ê³„ì‚°"""
        if len(values) < 3:
            return 0
        
        n = len(values)
        x = np.arange(n)
        slope = np.polyfit(x, values, 1)[0]
        return slope
    
    def calculate_velocity(self, values):
        """ë³€í™” ì†ë„ ê³„ì‚° (1ì°¨ ë¯¸ë¶„)"""
        if len(values) < 2:
            return 0
        
        velocities = np.diff(values)
        return np.mean(velocities)
    
    def analyze_arc_pattern(self, arc_counts):
        """ì•„í¬ ë°œìƒ íŒ¨í„´ ë¶„ì„"""
        total_arcs = sum(arc_counts)
        non_zero_count = sum(1 for count in arc_counts if count > 0)
        
        if total_arcs == 0:
            return {'pattern': 'NO_ARCS', 'frequency': 0}
        
        frequency = non_zero_count / len(arc_counts)
        
        if frequency > 0.5:
            pattern = 'FREQUENT'
        elif frequency > 0.2:
            pattern = 'MODERATE'
        else:
            pattern = 'SPORADIC'
        
        return {
            'pattern': pattern,
            'frequency': frequency,
            'total_arcs': total_arcs,
            'max_single_event': max(arc_counts)
        }
    
    def detect_periodicity(self, values):
        """ì£¼ê¸°ì„± íƒì§€ (FFT ê¸°ë°˜)"""
        if len(values) < 10:
            return {'periodic': False}
        
        try:
            # FFT ê³„ì‚°
            fft_values = np.fft.fft(values)
            frequencies = np.fft.fftfreq(len(values))
            
            # ì£¼ìš” ì£¼íŒŒìˆ˜ ì„±ë¶„ ì°¾ê¸°
            magnitudes = np.abs(fft_values)
            dominant_freq_idx = np.argmax(magnitudes[1:]) + 1  # DC ì„±ë¶„ ì œì™¸
            
            dominant_frequency = frequencies[dominant_freq_idx]
            period = 1 / abs(dominant_frequency) if dominant_frequency != 0 else None
            
            return {
                'periodic': magnitudes[dominant_freq_idx] > np.mean(magnitudes) * 2,
                'period': period,
                'dominant_frequency': dominant_frequency
            }
            
        except Exception as e:
            logger.error(f"Periodicity detection error: {e}")
            return {'periodic': False}
    
    def send_cloudwatch_metrics(self, device_id, payload):
        """CloudWatch ë©”íŠ¸ë¦­ ì „ì†¡"""
        try:
            metrics = []
            
            # ê¸°ë³¸ ë©”íŠ¸ë¦­
            base_metrics = [
                ('DeviceRiskScore', payload['risk_analysis']['risk_score']),
                ('DeviceTemperature', payload['sensor_data']['temperature']),
                ('DeviceCurrent', payload['sensor_data']['current']),
                ('DeviceVoltage', payload['sensor_data']['voltage']),
                ('DeviceArcCount', payload['sensor_data']['arc_count']),
                ('DeviceCOGas', payload['sensor_data']['co_gas']),
                ('DeviceVOC', payload['sensor_data']['voc'])
            ]
            
            for metric_name, value in base_metrics:
                metrics.append({
                    'MetricName': metric_name,
                    'Dimensions': [
                        {'Name': 'DeviceId', 'Value': str(device_id)}
                    ],
                    'Value': float(value),
                    'Unit': 'None',
                    'Timestamp': datetime.now()
                })
            
            # ì•ŒëŒ ìƒíƒœ ë©”íŠ¸ë¦­
            alarm_count = payload['alarm_status']['total_alarm_count']
            metrics.append({
                'MetricName': 'DeviceAlarmCount',
                'Dimensions': [
                    {'Name': 'DeviceId', 'Value': str(device_id)}
                ],
                'Value': alarm_count,
                'Unit': 'Count',
                'Timestamp': datetime.now()
            })
            
            # ë°°ì¹˜ë¡œ ë©”íŠ¸ë¦­ ì „ì†¡
            for i in range(0, len(metrics), 20):  # CloudWatch ì œí•œ: 20ê°œì”©
                batch = metrics[i:i+20]
                self.cloudwatch.put_metric_data(
                    Namespace='FirePrediction/Devices',
                    MetricData=batch
                )
            
        except Exception as e:
            logger.error(f"CloudWatch metrics error: {e}")
    
    def save_prediction_result(self, prediction_result):
        """ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥"""
        try:
            # TTL ì„¤ì • (7ì¼)
            ttl = int((datetime.now().timestamp() + (7 * 24 * 60 * 60)))
            
            item = {
                'device_id': prediction_result['device_id'],
                'timestamp': prediction_result['timestamp'],
                'ttl': ttl,
                'risk_score': Decimal(str(prediction_result['risk_score'])),
                'pattern_analysis': prediction_result['pattern_analysis'],
                'anomaly_analysis': prediction_result['anomaly_analysis'],
                'correlation_analysis': prediction_result['correlation_analysis'],
                'sensor_summary': prediction_result['sensor_summary'],
                'prediction_confidence': Decimal(str(prediction_result['prediction_confidence']))
            }
            
            # Decimal ë³€í™˜
            item = self.convert_to_dynamodb_format(item)
            
            self.predictions_table.put_item(Item=item)
            
        except Exception as e:
            logger.error(f"Prediction save error: {e}")
    
    def convert_to_dynamodb_format(self, data):
        """DynamoDB í˜•ì‹ ë³€í™˜"""
        from decimal import Decimal
        
        if isinstance(data, dict):
            return {k: self.convert_to_dynamodb_format(v) for k, v in data.items()}
        elif isinstance(data, list):
            return [self.convert_to_dynamodb_format(item) for item in data]
        elif isinstance(data, float):
            return Decimal(str(data))
        elif isinstance(data, int) and not isinstance(data, bool):
            return data
        else:
            return data

class KinesisAnalyticsQueries:
    """Kinesis Data Analytics SQL ì¿¼ë¦¬ë“¤"""
    
    # 5ë¶„ ìœˆë„ìš° ì§‘ê³„
    FIVE_MINUTE_AGGREGATION = """
    CREATE OR REPLACE STREAM "DEVICE_5MIN_AGGREGATES" AS
    SELECT 
        device_id,
        AVG(temperature) as avg_temperature,
        MAX(temperature) as max_temperature,
        MIN(temperature) as min_temperature,
        STDDEV_SAMP(temperature) as temp_stddev,
        AVG(risk_score) as avg_risk_score,
        MAX(risk_score) as max_risk_score,
        SUM(arc_count) as total_arcs,
        AVG(current) as avg_current,
        MAX(current) as max_current,
        COUNT(*) as record_count,
        ROWTIME_TO_TIMESTAMP(ROWTIME) as window_end
    FROM SOURCE_SQL_STREAM_001
    GROUP BY device_id, ROWTIME RANGE INTERVAL '5' MINUTE;
    """
    
    # ì˜¨ë„ ì´ìƒì¹˜ íƒì§€
    TEMPERATURE_ANOMALY_DETECTION = """
    CREATE OR REPLACE STREAM "TEMPERATURE_ANOMALIES" AS
    SELECT 
        device_id,
        temperature,
        risk_score,
        ROWTIME_TO_TIMESTAMP(ROWTIME) as detection_time,
        'TEMPERATURE_SPIKE' as anomaly_type
    FROM SOURCE_SQL_STREAM_001
    WHERE temperature > (
        SELECT AVG(temperature) + 2 * STDDEV_SAMP(temperature)
        FROM SOURCE_SQL_STREAM_001 
        WHERE device_id = s.device_id
        GROUP BY device_id, ROWTIME RANGE INTERVAL '1' HOUR
    ) AND temperature > 50;
    """
    
    # ìœ„í—˜ë„ ê¸‰ì¦ íƒì§€
    RISK_SPIKE_DETECTION = """
    CREATE OR REPLACE STREAM "RISK_SPIKES" AS
    SELECT 
        device_id,
        risk_score,
        LAG(risk_score, 1) OVER (
            PARTITION BY device_id 
            ORDER BY ROWTIME RANGE INTERVAL '10' MINUTE
        ) as prev_risk_score,
        risk_score - LAG(risk_score, 1) OVER (
            PARTITION BY device_id 
            ORDER BY ROWTIME RANGE INTERVAL '10' MINUTE
        ) as risk_change,
        ROWTIME_TO_TIMESTAMP(ROWTIME) as detection_time
    FROM SOURCE_SQL_STREAM_001
    WHERE risk_score - LAG(risk_score, 1) OVER (
        PARTITION BY device_id 
        ORDER BY ROWTIME RANGE INTERVAL '10' MINUTE
    ) > 0.3;
    """
    
    # ë‹¤ì¤‘ ë””ë°”ì´ìŠ¤ ìƒê´€ê´€ê³„
    MULTI_DEVICE_CORRELATION = """
    CREATE OR REPLACE STREAM "DEVICE_CORRELATIONS" AS
    SELECT 
        d1.device_id as device1,
        d2.device_id as device2,
        ABS(d1.temperature - d2.temperature) as temp_diff,
        ABS(d1.risk_score - d2.risk_score) as risk_diff,
        (d1.risk_score + d2.risk_score) / 2 as avg_risk,
        CASE 
            WHEN d1.risk_score > 0.7 AND d2.risk_score > 0.7 THEN 'HIGH_RISK_CLUSTER'
            WHEN ABS(d1.temperature - d2.temperature) > 20 THEN 'TEMP_DIVERGENCE'
            ELSE 'NORMAL'
        END as correlation_type,
        ROWTIME_TO_TIMESTAMP(ROWTIME) as analysis_time
    FROM SOURCE_SQL_STREAM_001 d1
    JOIN SOURCE_SQL_STREAM_001 d2 WITHIN INTERVAL '2' MINUTE
    ON d1.device_id != d2.device_id
    WHERE d1.risk_score > 0.5 OR d2.risk_score > 0.5;
    """
    
    # ì•„í¬ íŒ¨í„´ ë¶„ì„
    ARC_PATTERN_ANALYSIS = """
    CREATE OR REPLACE STREAM "ARC_PATTERNS" AS
    SELECT 
        device_id,
        SUM(arc_count) as total_arcs_10min,
        COUNT(CASE WHEN arc_count > 0 THEN 1 END) as arc_events,
        MAX(arc_count) as max_single_arc,
        AVG(CASE WHEN arc_count > 0 THEN arc_count END) as avg_arc_per_event,
        CASE 
            WHEN SUM(arc_count) > 10 THEN 'HIGH_FREQUENCY'
            WHEN SUM(arc_count) > 5 THEN 'MODERATE'
            WHEN SUM(arc_count) > 0 THEN 'LOW'
            ELSE 'NONE'
        END as arc_pattern,
        ROWTIME_TO_TIMESTAMP(ROWTIME) as window_end
    FROM SOURCE_SQL_STREAM_001
    WHERE ROWTIME RANGE INTERVAL '10' MINUTE PRECEDING
    GROUP BY device_id, ROWTIME RANGE INTERVAL '10' MINUTE;
    """

class KinesisOutputProcessor:
    """Kinesis Analytics ì¶œë ¥ ì²˜ë¦¬"""
    
    def __init__(self):
        self.sns = boto3.client('sns')
        self.lambda_client = boto3.client('lambda')
        
    def process_analytics_output(self, event, context):
        """Kinesis Analytics ì¶œë ¥ ì²˜ë¦¬"""
        try:
            for record in event['Records']:
                data = json.loads(base64.b64decode(record['kinesis']['data']))
                
                # ì¶œë ¥ ìŠ¤íŠ¸ë¦¼ë³„ ì²˜ë¦¬
                if 'TEMPERATURE_ANOMALIES' in record['eventSourceARN']:
                    self.handle_temperature_anomaly(data)
                elif 'RISK_SPIKES' in record['eventSourceARN']:
                    self.handle_risk_spike(data)
                elif 'DEVICE_CORRELATIONS' in record['eventSourceARN']:
                    self.handle_device_correlation(data)
                elif 'ARC_PATTERNS' in record['eventSourceARN']:
                    self.handle_arc_pattern(data)
            
            return {'statusCode': 200}
            
        except Exception as e:
            logger.error(f"Analytics output processing error: {e}")
            return {'statusCode': 500}
    
    def handle_temperature_anomaly(self, data):
        """ì˜¨ë„ ì´ìƒì¹˜ ì²˜ë¦¬"""
        if data['temperature'] > 65:  # ë§¤ìš° ë†’ì€ ì˜¨ë„
            self.send_critical_temperature_alert(data)
    
    def handle_risk_spike(self, data):
        """ìœ„í—˜ë„ ê¸‰ì¦ ì²˜ë¦¬"""
        if data['risk_change'] > 0.5:  # 50% ì´ìƒ ê¸‰ì¦
            self.trigger_emergency_response(data)
    
    def handle_device_correlation(self, data):
        """ë””ë°”ì´ìŠ¤ ìƒê´€ê´€ê³„ ì²˜ë¦¬"""
        if data['correlation_type'] == 'HIGH_RISK_CLUSTER':
            self.alert_cluster_risk(data)
    
    def handle_arc_pattern(self, data):
        """ì•„í¬ íŒ¨í„´ ì²˜ë¦¬"""
        if data['arc_pattern'] == 'HIGH_FREQUENCY':
            self.schedule_maintenance_check(data)
    
    def send_critical_temperature_alert(self, data):
        """ê¸´ê¸‰ ì˜¨ë„ ì•Œë¦¼"""
        message = f"ğŸŒ¡ï¸ ê¸´ê¸‰: ë””ë°”ì´ìŠ¤ {data['device_id']} ê³ ì˜¨ ê°ì§€ - {data['temperature']:.1f}Â°C"
        
        self.sns.publish(
            TopicArn=os.environ.get('EMERGENCY_TOPIC_ARN'),
            Message=message,
            Subject="ê¸´ê¸‰: ê³ ì˜¨ ê°ì§€"
        )
    
    def trigger_emergency_response(self, data):
        """ë¹„ìƒ ëŒ€ì‘ íŠ¸ë¦¬ê±°"""
        # Lambda í•¨ìˆ˜ í˜¸ì¶œë¡œ ë¹„ìƒ í”„ë¡œí† ì½œ ì‹¤í–‰
        self.lambda_client.invoke(
            FunctionName=os.environ.get('EMERGENCY_RESPONSE_FUNCTION'),
            InvocationType='Event',
            Payload=json.dumps({
                'device_id': data['device_id'],
                'risk_change': data['risk_change'],
                'current_risk': data['risk_score'],
                'trigger_time': data['detection_time']
            })
        )

# AWS IoT Core í™”ì¬ ì˜ˆì¸¡ ì‹œìŠ¤í…œ - Part 4: SageMaker ML íŒŒì´í”„ë¼ì¸ ë° ê³ ê¸‰ ëª¨ë¸

## 4. Amazon SageMakerë¥¼ í™œìš©í•œ ê³ ê¸‰ ML ëª¨ë¸

### 4.1 SageMaker í•™ìŠµ íŒŒì´í”„ë¼ì¸

```python
import sagemaker
from sagemaker.tensorflow import TensorFlow
from sagemaker.processing import ProcessingInput, ProcessingOutput
from sagemaker.sklearn.processing import SKLearnProcessor
from sagemaker.workflow.pipeline import Pipeline
from sagemaker.workflow.steps import ProcessingStep, TrainingStep
from sagemaker.workflow.parameters import ParameterString, ParameterInteger
import pandas as pd
import numpy as np
import boto3

class SageMakerMLPipeline:
    def __init__(self):
        self.sagemaker_session = sagemaker.Session()
        self.role = sagemaker.get_execution_role()
        self.bucket = self.sagemaker_session.default_bucket()
        self.region = boto3.Session().region_name
        
    def create_preprocessing_script(self):
        """ë°ì´í„° ì „ì²˜ë¦¬ ìŠ¤í¬ë¦½íŠ¸ ìƒì„±"""
        preprocessing_code = 

'''
import argparse
import os
import boto3
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split
import joblib
import json
from datetime import datetime, timedelta

def preprocess_discrete_data():
    """Discrete ë°ì´í„° ì „ì²˜ë¦¬"""
    
    # DynamoDBì—ì„œ ë°ì´í„° ì¡°íšŒ
    dynamodb = boto3.resource('dynamodb')
    raw_data_table = dynamodb.Table('FirePrediction-RawData')
    
    # ìµœê·¼ 30ì¼ ë°ì´í„° ì¡°íšŒ
    end_time = datetime.now()
    start_time = end_time - timedelta(days=30)
    
    # ë°ì´í„° ìˆ˜ì§‘
    all_data = []
    devices = get_all_device_ids()  # ëª¨ë“  ë””ë°”ì´ìŠ¤ ID ì¡°íšŒ
    
    for device_id in devices:
        response = raw_data_table.query(
            KeyConditionExpression=Key('device_id').eq(device_id) & 
                                 Key('timestamp').between(
                                     start_time.isoformat(), 
                                     end_time.isoformat()
                                 )
        )
        all_data.extend(response['Items'])
    
    # DataFrame ë³€í™˜
    df = pd.DataFrame(all_data)
    
    # Discrete í•„ë“œ ê¸°ë°˜ íŠ¹ì§• ì¶”ì¶œ
    features = extract_discrete_features(df)
    
    # ë¼ë²¨ ìƒì„± (í™”ì¬ ë°œìƒ ì˜ˆì¸¡)
    labels = create_fire_labels(df)
    
    # íŠ¹ì§• ì—”ì§€ë‹ˆì–´ë§
    engineered_features = engineer_features(features)
    
    # ë°ì´í„° ë¶„í• 
    X_train, X_test, y_train, y_test = train_test_split(
        engineered_features, labels, test_size=0.2, random_state=42, stratify=labels
    )
    
    # ì •ê·œí™”
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    # ì €ì¥
    save_processed_data(X_train_scaled, X_test_scaled, y_train, y_test, scaler)

def extract_discrete_features(df):
    """Discrete í´ë˜ìŠ¤ ê¸°ë°˜ íŠ¹ì§• ì¶”ì¶œ"""
    features = pd.DataFrame()
    
    # ê¸°ë³¸ ì„¼ì„œ ë°ì´í„°
    sensor_columns = [
        'voltage', 'voltage_low', 'current', 'leakage_current', 
        'resistive_leakage', 'power', 'pcb_temperature', 
        'sensor_temperature', 'co_gas', 'voc', 'power_factor', 'arc_count'
    ]
    
    for col in sensor_columns:
        if col in df.columns:
            features[col] = pd.to_numeric(df[col], errors='coerce')
    
    # ìƒíƒœ ë¹„íŠ¸ ë¶„í•´
    features = add_status_bit_features(features, df['status_bits'])
    
    # ì‹œê°„ ê¸°ë°˜ íŠ¹ì§•
    features = add_temporal_features(features, df['timestamp'])
    
    return features

def add_status_bit_features(features, status_bits):
    """ìƒíƒœ ë¹„íŠ¸ë¥¼ ê°œë³„ íŠ¹ì§•ìœ¼ë¡œ ë¶„í•´"""
    # Discrete í´ë˜ìŠ¤ ì£¼ì„ ê¸°ë°˜ ë¹„íŠ¸ ë¶„í•´
    bit_names = [
        'overvoltage', 'power_outage', 'overcurrent', 'total_leakage',
        'resistive_leakage_alarm', 'power_anomaly', 'pcb_temp_alarm',
        'sensor_temp_alarm', 'co_alarm', 'voc_alarm', 'power_factor_alarm', 'arc_alarm'
    ]
    
    for i, bit_name in enumerate(bit_names):
        features[f'status_{bit_name}'] = status_bits.apply(lambda x: bool(x & (1 << i)))
    
    # í†µì‹  ì˜¤ë¥˜
    features['communication_error'] = status_bits >= 4096
    
    # ì´ ì•ŒëŒ ìˆ˜
    features['total_alarms'] = status_bits.apply(
        lambda x: bin(x & 0xFFF).count('1') if x < 4096 else 0
    )
    
    return features

def add_temporal_features(features, timestamps):
    """ì‹œê°„ ê¸°ë°˜ íŠ¹ì§• ì¶”ê°€"""
    timestamps = pd.to_datetime(timestamps)
    
    features['hour'] = timestamps.dt.hour
    features['day_of_week'] = timestamps.dt.dayofweek
    features['month'] = timestamps.dt.month
    features['is_weekend'] = timestamps.dt.dayofweek.isin([5, 6])
    features['is_night'] = timestamps.dt.hour.isin([22, 23, 0, 1, 2, 3, 4, 5])
    
    return features

def engineer_features(features):
    """ê³ ê¸‰ íŠ¹ì§• ì—”ì§€ë‹ˆì–´ë§"""
    engineered = features.copy()
    
    # ì˜¨ë„ ê´€ë ¨ íŠ¹ì§•
    engineered['max_temperature'] = features[['pcb_temperature', 'sensor_temperature']].max(axis=1)
    engineered['temp_difference'] = abs(features['pcb_temperature'] - features['sensor_temperature'])
    engineered['temp_ratio'] = features['pcb_temperature'] / (features['sensor_temperature'] + 1e-6)
    
    # ì „ê¸°ì  íŠ¹ì§•
    engineered['power_calculated'] = (features['voltage'] * features['current'] * 
                                    features['power_factor'])
    engineered['power_discrepancy'] = abs(engineered['power_calculated'] - features['power'])
    engineered['resistance'] = features['voltage'] / (features['current'] + 1e-6)
    engineered['current_density'] = features['current'] / (features['voltage'] + 1e-6)
    
    # ëˆ„ì „ ê´€ë ¨ íŠ¹ì§•
    engineered['total_leakage'] = features['leakage_current'] + features['resistive_leakage']
    engineered['leakage_ratio'] = (features['resistive_leakage'] / 
                                 (features['leakage_current'] + 1e-6))
    
    # ê°€ìŠ¤ ê´€ë ¨ íŠ¹ì§•
    engineered['gas_index'] = features['co_gas'] * 0.6 + features['voc'] * 0.4
    engineered['gas_risk_score'] = (
        (features['co_gas'] > 17).astype(int) * 0.7 +
        (features['voc'] > 400).astype(int) * 0.3
    )
    
    # ë³µí•© ìœ„í—˜ ì§€í‘œ
    engineered['electrical_risk'] = (
        engineered['status_overcurrent'].astype(int) * 0.3 +
        engineered['status_total_leakage'].astype(int) * 0.4 +
        (engineered['arc_count'] > 0).astype(int) * 0.3
    )
    
    engineered['thermal_risk'] = (
        (engineered['max_temperature'] > 60).astype(int) * 0.6 +
        (engineered['temp_difference'] > 15).astype(int) * 0.4
    )
    
    return engineered

def create_fire_labels(df):
    """í™”ì¬ ë¼ë²¨ ìƒì„±"""
    # ì‹¤ì œ í™”ì¬ ë°œìƒ ë°ì´í„°ê°€ ìˆë‹¤ë©´ ì‚¬ìš©
    # ì—¬ê¸°ì„œëŠ” ì‹œë®¬ë ˆì´ì…˜ëœ ë¼ë²¨ ìƒì„±
    
    fire_conditions = (
        (df['pcb_temperature'] > 70) |
        (df['sensor_temperature'] > 70) |
        (df['arc_count'] > 5) |
        ((df['co_gas'] > 25) & (df['voc'] > 600)) |
        (df['total_alarms'] > 4)
    )
    
    # í™”ì¬ ë°œìƒ ì „ 10ë¶„ê°„ë„ ì–‘ì„±ìœ¼ë¡œ í‘œì‹œ
    labels = fire_conditions.astype(int)
    
    # ì‹œê³„ì—´ í™•ì¥ (í–¥í›„ ì˜ˆì¸¡)
    for i in range(len(labels) - 10):
        if any(labels[i+1:i+11]):  # í–¥í›„ 10ë¶„ ë‚´ í™”ì¬ ë°œìƒ
            labels[i] = 1
    
    return labels

if __name__ == "__main__":
    preprocess_discrete_data()
'''
        
        # ì „ì²˜ë¦¬ ìŠ¤í¬ë¦½íŠ¸ë¥¼ S3ì— ì €ì¥
        with open('/tmp/preprocessing.py', 'w') as f:
            f.write(preprocessing_code)
        
        s3_client = boto3.client('s3')
        s3_client.upload_file(
            '/tmp/preprocessing.py',
            self.bucket,
            'scripts/preprocessing.py'
        )
        
        return f's3://{self.bucket}/scripts/preprocessing.py'
    
    def create_lstm_training_script(self):
        """LSTM ëª¨ë¸ í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸"""
        training_code = 
        
'''
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout, Conv1D, MaxPooling1D
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau
from tensorflow.keras.optimizers import Adam
import numpy as np
import pandas as pd
import argparse
import os
import joblib

def create_sequences(X, y, sequence_length=30):
    """ì‹œê³„ì—´ ì‹œí€€ìŠ¤ ìƒì„±"""
    sequences = []
    targets = []
    
    for i in range(len(X) - sequence_length):
        sequences.append(X[i:i+sequence_length])
        targets.append(y[i+sequence_length])
    
    return np.array(sequences), np.array(targets)

def build_advanced_lstm_model(input_shape):
    """ê³ ê¸‰ LSTM ëª¨ë¸ êµ¬ì¶•"""
    model = Sequential([
        # CNN íŠ¹ì§• ì¶”ì¶œ ë ˆì´ì–´
        Conv1D(64, 3, activation='relu', input_shape=input_shape),
        Conv1D(64, 3, activation='relu'),
        MaxPooling1D(2),
        Dropout(0.25),
        
        # LSTM ë ˆì´ì–´ë“¤
        LSTM(128, return_sequences=True, dropout=0.2, recurrent_dropout=0.2),
        LSTM(64, return_sequences=True, dropout=0.2, recurrent_dropout=0.2),
        LSTM(32, dropout=0.2, recurrent_dropout=0.2),
        
        # Dense ë ˆì´ì–´ë“¤
        Dense(64, activation='relu'),
        Dropout(0.5),
        Dense(32, activation='relu'),
        Dropout(0.3),
        Dense(16, activation='relu'),
        Dense(1, activation='sigmoid')
    ])
    
    # ì»¤ìŠ¤í…€ ì˜µí‹°ë§ˆì´ì €
    optimizer = Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999)
    
    model.compile(
        optimizer=optimizer,
        loss='binary_crossentropy',
        metrics=['accuracy', 'precision', 'recall', 'AUC']
    )
    
    return model

def train_model():
    """ëª¨ë¸ í•™ìŠµ ì‹¤í–‰"""
    # ì „ì²˜ë¦¬ëœ ë°ì´í„° ë¡œë“œ
    X_train = np.load('/opt/ml/input/data/train/X_train.npy')
    X_test = np.load('/opt/ml/input/data/train/X_test.npy')
    y_train = np.load('/opt/ml/input/data/train/y_train.npy')
    y_test = np.load('/opt/ml/input/data/train/y_test.npy')
    
    # ì‹œí€€ìŠ¤ ìƒì„±
    sequence_length = 30
    X_train_seq, y_train_seq = create_sequences(X_train, y_train, sequence_length)
    X_test_seq, y_test_seq = create_sequences(X_test, y_test, sequence_length)
    
    # ëª¨ë¸ ìƒì„±
    model = build_advanced_lstm_model((sequence_length, X_train.shape[1]))
    
    # ì½œë°± ì„¤ì •
    callbacks = [
        EarlyStopping(
            monitor='val_loss',
            patience=15,
            restore_best_weights=True,
            verbose=1
        ),
        ModelCheckpoint(
            '/opt/ml/model/best_model.h5',
            monitor='val_auc',
            save_best_only=True,
            mode='max',
            verbose=1
        ),
        ReduceLROnPlateau(
            monitor='val_loss',
            factor=0.5,
            patience=8,
            min_lr=1e-7,
            verbose=1
        )
    ]
    
    # í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ê³„ì‚° (ë¶ˆê· í˜• ë°ì´í„° ì²˜ë¦¬)
    class_weight = {
        0: 1.0,
        1: len(y_train_seq) / (2 * np.sum(y_train_seq))
    }
    
    # ëª¨ë¸ í•™ìŠµ
    history = model.fit(
        X_train_seq, y_train_seq,
        epochs=100,
        batch_size=32,
        validation_data=(X_test_seq, y_test_seq),
        callbacks=callbacks,
        class_weight=class_weight,
        verbose=1
    )
    
    # ëª¨ë¸ ì €ì¥
    model.save('/opt/ml/model/lstm_fire_prediction_model.h5')
    
    # í•™ìŠµ ì´ë ¥ ì €ì¥
    import pickle
    with open('/opt/ml/model/training_history.pkl', 'wb') as f:
        pickle.dump(history.history, f)
    
    # ëª¨ë¸ í‰ê°€
    test_loss, test_accuracy, test_precision, test_recall, test_auc = model.evaluate(
        X_test_seq, y_test_seq, verbose=0
    )
    
    print(f"Test Results:")
    print(f"Loss: {test_loss:.4f}")
    print(f"Accuracy: {test_accuracy:.4f}")
    print(f"Precision: {test_precision:.4f}")
    print(f"Recall: {test_recall:.4f}")
    print(f"AUC: {test_auc:.4f}")

if __name__ == "__main__":
    train_model()
'''
        # í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ë¥¼ S3ì— ì €ì¥
        with open('/tmp/train_lstm.py', 'w') as f:
            f.write(training_code)
        
        s3_client = boto3.client('s3')
        s3_client.upload_file(
            '/tmp/train_lstm.py',
            self.bucket,
            'scripts/train_lstm.py'
        )
        
        return f's3://{self.bucket}/scripts/train_lstm.py'

    def create_sagemaker_pipeline(self):
        """SageMaker íŒŒì´í”„ë¼ì¸ ìƒì„±"""
        
        # íŒŒë¼ë¯¸í„° ì •ì˜
        processing_instance_type = ParameterString(
            name="ProcessingInstanceType",
            default_value="ml.m5.xlarge"
        )
        
        training_instance_type = ParameterString(
            name="TrainingInstanceType", 
            default_value="ml.p3.2xlarge"
        )
        
        model_approval_status = ParameterString(
            name="ModelApprovalStatus",
            default_value="PendingManualApproval"
        )
        
        # ì „ì²˜ë¦¬ ìŠ¤í…
        sklearn_processor = SKLearnProcessor(
            framework_version="0.23-1",
            instance_type=processing_instance_type,
            instance_count=1,
            base_job_name="fire-prediction-preprocessing",
            role=self.role,
        )
        
        preprocessing_step = ProcessingStep(
            name="PreprocessDiscreteData",
            processor=sklearn_processor,
            inputs=[
                ProcessingInput(
                    source=self.create_preprocessing_script(),
                    destination="/opt/ml/processing/input/code",
                    input_name="code"
                )
            ],
            outputs=[
                ProcessingOutput(
                    output_name="train",
                    source="/opt/ml/processing/output/train",
                    destination=f"s3://{self.bucket}/fire-prediction/train"
                ),
                ProcessingOutput(
                    output_name="test", 
                    source="/opt/ml/processing/output/test",
                    destination=f"s3://{self.bucket}/fire-prediction/test"
                )
            ],
            code=self.create_preprocessing_script()
        )
        
        # LSTM í•™ìŠµ ìŠ¤í…
        tensorflow_estimator = TensorFlow(
            entry_point="train_lstm.py",
            source_dir=f"s3://{self.bucket}/scripts/",
            role=self.role,
            instance_count=1,
            instance_type=training_instance_type,
            framework_version="2.8",
            py_version="py39",
            script_mode=True,
            hyperparameters={
                'epochs': 100,
                'batch_size': 32,
                'learning_rate': 0.001
            }
        )
        
        training_step = TrainingStep(
            name="TrainLSTMModel",
            estimator=tensorflow_estimator,
            inputs={
                "train": TrainingInput(
                    s3_data=preprocessing_step.properties.ProcessingOutputConfig.Outputs[
                        "train"
                    ].S3Output.S3Uri,
                    content_type="application/x-npy"
                )
            }
        )
        
        # íŒŒì´í”„ë¼ì¸ ì •ì˜
        pipeline = Pipeline(
            name="fire-prediction-pipeline",
            parameters=[
                processing_instance_type,
                training_instance_type,
                model_approval_status
            ],
            steps=[preprocessing_step, training_step],
            sagemaker_session=self.sagemaker_session
        )
        
        return pipeline

class RealTimeInferenceEndpoint:
    """ì‹¤ì‹œê°„ ì¶”ë¡  ì—”ë“œí¬ì¸íŠ¸"""
    
    def __init__(self):
        self.sagemaker_client = boto3.client('sagemaker-runtime')
        self.endpoint_name = 'fire-prediction-lstm-endpoint'
        
    def create_inference_script(self):
        """ì¶”ë¡  ìŠ¤í¬ë¦½íŠ¸ ìƒì„±"""
        inference_code = '''
import json
import numpy as np
import tensorflow as tf
from tensorflow.keras.models import load_model
import joblib
import os

def model_fn(model_dir):
    """ëª¨ë¸ ë¡œë“œ"""
    model = load_model(os.path.join(model_dir, 'lstm_fire_prediction_model.h5'))
    scaler = joblib.load(os.path.join(model_dir, 'scaler.pkl'))
    return {'model': model, 'scaler': scaler}

def input_fn(request_body, content_type='application/json'):
    """ì…ë ¥ ë°ì´í„° ì „ì²˜ë¦¬"""
    if content_type == 'application/json':
        input_data = json.loads(request_body)
        
        # Discrete ë°ì´í„° í˜•ì‹ ë³€í™˜
        features = []
        
        # ê¸°ë³¸ ì„¼ì„œ ë°ì´í„°
        sensor_values = [
            input_data.get('voltage', 0),
            input_data.get('voltage_low', 0), 
            input_data.get('current', 0),
            input_data.get('leakage_current', 0),
            input_data.get('resistive_leakage', 0),
            input_data.get('power', 0),
            input_data.get('pcb_temperature', 0),
            input_data.get('sensor_temperature', 0),
            input_data.get('co_gas', 0),
            input_data.get('voc', 0),
            input_data.get('power_factor', 0),
            input_data.get('arc_count', 0)
        ]
        
        # ìƒíƒœ ë¹„íŠ¸ ë¶„í•´
        status_bits = input_data.get('status_bits', 0)
        status_features = []
        for i in range(12):
            status_features.append(int(bool(status_bits & (1 << i))))
        
        # ì—”ì§€ë‹ˆì–´ë“œ íŠ¹ì§• ê³„ì‚°
        max_temp = max(sensor_values[6], sensor_values[7])  # pcb_temp, sensor_temp
        temp_diff = abs(sensor_values[6] - sensor_values[7])
        power_calc = sensor_values[0] * sensor_values[2] * sensor_values[10]  # V*I*PF
        
        engineered_features = [
            max_temp,
            temp_diff,
            power_calc,
            abs(power_calc - sensor_values[5]),  # power discrepancy
            sensor_values[0] / (sensor_values[2] + 1e-6),  # resistance
            sensor_values[3] + sensor_values[4],  # total leakage
            sensor_values[8] * 0.6 + sensor_values[9] * 0.4  # gas index
        ]
        
        # ì „ì²´ íŠ¹ì§• ë²¡í„°
        all_features = sensor_values + status_features + engineered_features
        
        return np.array(all_features).reshape(1, -1)
    
    else:
        raise ValueError(f"Unsupported content type: {content_type}")

def predict_fn(input_data, model_dict):
    """ì˜ˆì¸¡ ìˆ˜í–‰"""
    model = model_dict['model']
    scaler = model_dict['scaler']
    
    # ë°ì´í„° ì •ê·œí™”
    input_scaled = scaler.transform(input_data)
    
    # ì‹œí€€ìŠ¤ í˜•íƒœë¡œ ë³€í™˜ (ë‹¨ì¼ ì‹œì  ì˜ˆì¸¡ì„ ìœ„í•´ ë³µì œ)
    sequence_length = 30
    input_sequence = np.repeat(input_scaled, sequence_length, axis=0)
    input_sequence = input_sequence.reshape(1, sequence_length, -1)
    
    # ì˜ˆì¸¡
    prediction = model.predict(input_sequence)
    probability = float(prediction[0][0])
    
    # ìœ„í—˜ ìˆ˜ì¤€ ê²°ì •
    if probability >= 0.8:
        risk_level = 'CRITICAL'
    elif probability >= 0.6:
        risk_level = 'HIGH'
    elif probability >= 0.4:
        risk_level = 'MEDIUM'
    elif probability >= 0.2:
        risk_level = 'LOW'
    else:
        risk_level = 'NORMAL'
    
    return {
        'fire_probability': probability,
        'risk_level': risk_level,
        'confidence': min(1.0, abs(probability - 0.5) * 2)  # 0.5ì—ì„œ ë©€ìˆ˜ë¡ ë†’ì€ ì‹ ë¢°ë„
    }

def output_fn(prediction, accept='application/json'):
    """ì¶œë ¥ í˜•ì‹ ë³€í™˜"""
    if accept == 'application/json':
        return json.dumps(prediction)
    else:
        raise ValueError(f"Unsupported accept type: {accept}")
'''
        
        return inference_code
    
    def deploy_model(self, model_artifacts_s3_path):
        """ëª¨ë¸ ë°°í¬"""
        from sagemaker.tensorflow import TensorFlowModel
        
        # ì¶”ë¡  ì½”ë“œ ìƒì„±
        inference_code = self.create_inference_script()
        
        # ì¶”ë¡  ìŠ¤í¬ë¦½íŠ¸ ì €ì¥
        with open('/tmp/inference.py', 'w') as f:
            f.write(inference_code)
        
        # TensorFlow ëª¨ë¸ ìƒì„±
        tensorflow_model = TensorFlowModel(
            model_data=model_artifacts_s3_path,
            role=sagemaker.get_execution_role(),
            entry_point='inference.py',
            source_dir='/tmp/',
            framework_version='2.8',
            py_version='py39'
        )
        
        # ì—”ë“œí¬ì¸íŠ¸ ë°°í¬
        predictor = tensorflow_model.deploy(
            initial_instance_count=1,
            instance_type='ml.m5.large',
            endpoint_name=self.endpoint_name
        )
        
        return predictor
    
    def predict_fire_risk(self, discrete_data):
        """ì‹¤ì‹œê°„ í™”ì¬ ìœ„í—˜ ì˜ˆì¸¡"""
        try:
            # ë°ì´í„° í˜•ì‹ ë³€í™˜
            payload = {
                'voltage': discrete_data.get('volt', 0),
                'voltage_low': discrete_data.get('voltLow', 0),
                'current': discrete_data.get('ct', 0),
                'leakage_current': discrete_data.get('zct', 0),
                'resistive_leakage': discrete_data.get('rzct', 0),
                'power': discrete_data.get('pwr', 0),
                'pcb_temperature': discrete_data.get('pcbTemp', 0),
                'sensor_temperature': discrete_data.get('sensorTemp', 0),
                'co_gas': discrete_data.get('co', 0),
                'voc': discrete_data.get('voc', 0),
                'power_factor': discrete_data.get('pFct', 0),
                'arc_count': discrete_data.get('aArc', 0),
                'status_bits': discrete_data.get('status', 0)
            }
            
            # SageMaker ì—”ë“œí¬ì¸íŠ¸ í˜¸ì¶œ
            response = self.sagemaker_client.invoke_endpoint(
                EndpointName=self.endpoint_name,
                ContentType='application/json',
                Body=json.dumps(payload)
            )
            
            # ê²°ê³¼ íŒŒì‹±
            result = json.loads(response['Body'].read().decode())
            
            return result
            
        except Exception as e:
            logger.error(f"SageMaker prediction error: {e}")
            return {
                'fire_probability': 0.5,
                'risk_level': 'UNKNOWN',
                'confidence': 0.0,
                'error': str(e)
            }

class BatchTransformJob:
    """ë°°ì¹˜ ë³€í™˜ ì‘ì—…"""
    
    def __init__(self):
        self.sagemaker_client = boto3.client('sagemaker')
        self.s3_client = boto3.client('s3')
        
    def create_batch_prediction_job(self, model_name, input_s3_path, output_s3_path):
        """ë°°ì¹˜ ì˜ˆì¸¡ ì‘ì—… ìƒì„±"""
        
        transform_job_name = f"fire-prediction-batch-{int(datetime.now().timestamp())}"
        
        try:
            response = self.sagemaker_client.create_transform_job(
                TransformJobName=transform_job_name,
                ModelName=model_name,
                BatchStrategy='MultiRecord',
                TransformInput={
                    'DataSource': {
                        'S3DataSource': {
                            'S3DataType': 'S3Prefix',
                            'S3Uri': input_s3_path
                        }
                    },
                    'ContentType': 'application/json',
                    'SplitType': 'Line'
                },
                TransformOutput={
                    'S3OutputPath': output_s3_path,
                    'Accept': 'application/json'
                },
                TransformResources={
                    'InstanceType': 'ml.m5.large',
                    'InstanceCount': 1
                }
            )
            
            return transform_job_name
            
        except Exception as e:
            logger.error(f"Batch transform job creation error: {e}")
            return None
    
    def monitor_batch_job(self, job_name):
        """ë°°ì¹˜ ì‘ì—… ëª¨ë‹ˆí„°ë§"""
        while True:
            response = self.sagemaker_client.describe_transform_job(
                TransformJobName=job_name
            )
            
            status = response['TransformJobStatus']
            
            if status == 'Completed':
                print(f"Batch job {job_name} completed successfully")
                return True
            elif status == 'Failed':
                print(f"Batch job {job_name} failed: {response.get('FailureReason', 'Unknown')}")
                return False
            elif status == 'Stopping' or status == 'Stopped':
                print(f"Batch job {job_name} was stopped")
                return False
            
            print(f"Batch job {job_name} status: {status}")
            time.sleep(30)

class ModelMonitoring:
    """ëª¨ë¸ ëª¨ë‹ˆí„°ë§ ë° ì„±ëŠ¥ ì¶”ì """
    
    def __init__(self):
        self.cloudwatch = boto3.client('cloudwatch')
        self.dynamodb = boto3.resource('dynamodb')
        self.model_metrics_table = self.dynamodb.Table('FirePrediction-ModelMetrics')
        
    def log_prediction_metrics(self, device_id, prediction_result, actual_outcome=None):
        """ì˜ˆì¸¡ ë©”íŠ¸ë¦­ ë¡œê¹…"""
        try:
            # CloudWatch ë©”íŠ¸ë¦­
            metrics = [
                {
                    'MetricName': 'PredictionConfidence',
                    'Dimensions': [
                        {'Name': 'DeviceId', 'Value': str(device_id)},
                        {'Name': 'Model', 'Value': 'LSTM'}
                    ],
                    'Value': prediction_result['confidence'],
                    'Unit': 'None'
                },
                {
                    'MetricName': 'FireProbability',
                    'Dimensions': [
                        {'Name': 'DeviceId', 'Value': str(device_id)},
                        {'Name': 'Model', 'Value': 'LSTM'}
                    ],
                    'Value': prediction_result['fire_probability'],
                    'Unit': 'None'
                }
            ]
            
            self.cloudwatch.put_metric_data(
                Namespace='FirePrediction/ModelPerformance',
                MetricData=metrics
            )
            
            # DynamoDBì— ìƒì„¸ ë©”íŠ¸ë¦­ ì €ì¥
            item = {
                'device_id': str(device_id),
                'timestamp': datetime.now().isoformat(),
                'prediction_probability': Decimal(str(prediction_result['fire_probability'])),
                'predicted_risk_level': prediction_result['risk_level'],
                'confidence': Decimal(str(prediction_result['confidence'])),
                'model_version': 'LSTM_v1.0',
                'ttl': int((datetime.now().timestamp() + (90 * 24 * 60 * 60)))  # 90ì¼
            }
            
            if actual_outcome is not None:
                item['actual_outcome'] = actual_outcome
                item['prediction_accuracy'] = 1 if (
                    (prediction_result['fire_probability'] > 0.5) == actual_outcome
                ) else 0
            
            self.model_metrics_table.put_item(Item=item)
            
        except Exception as e:
            logger.error(f"Model metrics logging error: {e}")
    
    def calculate_model_performance(self, days_back=7):
        """ëª¨ë¸ ì„±ëŠ¥ ê³„ì‚°"""
        try:
            # ìµœê·¼ Nì¼ê°„ ì˜ˆì¸¡ ê²°ê³¼ ì¡°íšŒ
            end_time = datetime.now()
            start_time = end_time - timedelta(days=days_back)
            
            # ì‹¤ì œ ê²°ê³¼ê°€ ìˆëŠ” ì˜ˆì¸¡ë“¤ë§Œ ì¡°íšŒ
            response = self.model_metrics_table.scan(
                FilterExpression='attribute_exists(actual_outcome) AND #ts BETWEEN :start AND :end',
                ExpressionAttributeNames={'#ts': 'timestamp'},
                ExpressionAttributeValues={
                    ':start': start_time.isoformat(),
                    ':end': end_time.isoformat()
                }
            )
            
            predictions = response['Items']
            
            if len(predictions) < 10:  # ìµœì†Œ 10ê°œ ì˜ˆì¸¡ í•„ìš”
                return {'status': 'insufficient_data', 'count': len(predictions)}
            
            # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê³„ì‚°
            accuracies = [float(p['prediction_accuracy']) for p in predictions]
            confidences = [float(p['confidence']) for p in predictions]
            
            true_positives = sum(1 for p in predictions 
                               if p['actual_outcome'] == 1 and float(p['prediction_probability']) > 0.5)
            false_positives = sum(1 for p in predictions 
                                if p['actual_outcome'] == 0 and float(p['prediction_probability']) > 0.5)
            true_negatives = sum(1 for p in predictions 
                               if p['actual_outcome'] == 0 and float(p['prediction_probability']) <= 0.5)
            false_negatives = sum(1 for p in predictions 
                                if p['actual_outcome'] == 1 and float(p['prediction_probability']) <= 0.5)
            
            # ë©”íŠ¸ë¦­ ê³„ì‚°
            accuracy = np.mean(accuracies)
            precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0
            recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0
            f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
            avg_confidence = np.mean(confidences)
            
            performance_metrics = {
                'accuracy': accuracy,
                'precision': precision,
                'recall': recall,
                'f1_score': f1_score,
                'average_confidence': avg_confidence,
                'total_predictions': len(predictions),
                'true_positives': true_positives,
                'false_positives': false_positives,
                'true_negatives': true_negatives,
                'false_negatives': false_negatives,
                'evaluation_period_days': days_back
            }
            
            # CloudWatchì— ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì „ì†¡
            self.send_performance_metrics_to_cloudwatch(performance_metrics)
            
            return performance_metrics
            
        except Exception as e:
            logger.error(f"Model performance calculation error: {e}")
            return {'status': 'error', 'error': str(e)}
    
    def send_performance_metrics_to_cloudwatch(self, metrics):
        """ì„±ëŠ¥ ë©”íŠ¸ë¦­ì„ CloudWatchì— ì „ì†¡"""
        try:
            cloudwatch_metrics = [
                ('ModelAccuracy', metrics['accuracy']),
                ('ModelPrecision', metrics['precision']),
                ('ModelRecall', metrics['recall']),
                ('ModelF1Score', metrics['f1_score']),
                ('ModelAverageConfidence', metrics['average_confidence'])
            ]
            
            metric_data = []
            for metric_name, value in cloudwatch_metrics:
                metric_data.append({
                    'MetricName': metric_name,
                    'Dimensions': [
                        {'Name': 'Model', 'Value': 'LSTM'},
                        {'Name': 'Environment', 'Value': 'Production'}
                    ],
                    'Value': value,
                    'Unit': 'None'
                })
            
            self.cloudwatch.put_metric_data(
                Namespace='FirePrediction/ModelPerformance',
                MetricData=metric_data
            )
            
        except Exception as e:
            logger.error(f"CloudWatch performance metrics error: {e}")

# í†µí•© ML ì„œë¹„ìŠ¤ í´ë˜ìŠ¤
class FirePredictionMLService:
    """í™”ì¬ ì˜ˆì¸¡ ML ì„œë¹„ìŠ¤ í†µí•© í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.pipeline = SageMakerMLPipeline()
        self.inference_endpoint = RealTimeInferenceEndpoint()
        self.batch_processor = BatchTransformJob()
        self.model_monitor = ModelMonitoring()
        
    def initialize_ml_pipeline(self):
        """ML íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”"""
        # íŒŒì´í”„ë¼ì¸ ìƒì„± ë° ì‹¤í–‰
        pipeline = self.pipeline.create_sagemaker_pipeline()
        execution = pipeline.start()
        
        return execution
    
    def deploy_production_model(self, model_artifacts_path):
        """í”„ë¡œë•ì…˜ ëª¨ë¸ ë°°í¬"""
        predictor = self.inference_endpoint.deploy_model(model_artifacts_path)
        return predictor
    
    def predict_with_monitoring(self, device_id, discrete_data):
        """ëª¨ë‹ˆí„°ë§ê³¼ í•¨ê»˜ ì˜ˆì¸¡ ìˆ˜í–‰"""
        # ì˜ˆì¸¡ ì‹¤í–‰
        prediction = self.inference_endpoint.predict_fire_risk(discrete_data)
        
        # ë©”íŠ¸ë¦­ ë¡œê¹…
        self.model_monitor.log_prediction_metrics(device_id, prediction)
        
        return prediction

# AWS IoT Core í™”ì¬ ì˜ˆì¸¡ ì‹œìŠ¤í…œ - Part 5: ëŒ€ì‹œë³´ë“œ, ëª¨ë‹ˆí„°ë§ ë° ë°°í¬

# 5. ëŒ€ì‹œë³´ë“œ ë° ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ
# 5.1 CloudWatch ëŒ€ì‹œë³´ë“œ

python

import boto3

import json
from datetime import datetime, timedelta

class CloudWatchDashboardManager:
    def __init__(self):
        self.cloudwatch = boto3.client('cloudwatch')
        
    def create_fire_prediction_dashboard(self):
        """í™”ì¬ ì˜ˆì¸¡ ì‹œìŠ¤í…œ ëŒ€ì‹œë³´ë“œ ìƒì„±"""
        
        dashboard_body = {
            "widgets": [
                {
                    "type": "metric",
                    "x": 0, "y": 0, "width": 12, "height": 6,
                    "properties": {
                        "metrics": [
                            ["FirePrediction/Devices", "DeviceRiskScore", "DeviceId", "ALL"],
                            [".", "DeviceTemperature", ".", "."],
                            [".", "DeviceAlarmCount", ".", "."]
                        ],
                        "period": 300,
                        "stat": "Average",
                        "region": "us-east-1",
                        "title": "ì „ì²´ ë””ë°”ì´ìŠ¤ ìœ„í—˜ë„ í˜„í™©",
                        "yAxis": {
                            "left": {"min": 0, "max": 1}
                        }
                    }
                },
                {
                    "type": "metric",
                    "x": 12, "y": 0, "width": 12, "height": 6,
                    "properties": {
                        "metrics": [
                            ["FirePrediction/ModelPerformance", "ModelAccuracy", "Model", "LSTM"],
                            [".", "ModelPrecision", ".", "."],
                            [".", "ModelRecall", ".", "."],
                            [".", "ModelF1Score", ".", "."]
                        ],
                        "period": 3600,
                        "stat": "Average",
                        "region": "us-east-1",
                        "title": "ëª¨ë¸ ì„±ëŠ¥ ë©”íŠ¸ë¦­",
                        "yAxis": {
                            "left": {"min": 0, "max": 1}
                        }
                    }
                },
                {
                    "type": "log",
                    "x": 0, "y": 6, "width": 24, "height": 6,
                    "properties": {
                        "query": "SOURCE '/aws/lambda/fire-prediction-processor'\n| fields @timestamp, device_id, risk_level, risk_score\n| filter risk_level in [\"HIGH\", \"CRITICAL\"]\n| sort @timestamp desc\n| limit 20",
                        "region": "us-east-1",
                        "title": "ìµœê·¼ ê³ ìœ„í—˜ ì•Œë¦¼",
                        "view": "table"
                    }
                },
                {
                    "type": "metric",
                    "x": 0, "y": 12, "width": 8, "height": 6,
                    "properties": {
                        "metrics": [
                            ["AWS/Lambda", "Duration", "FunctionName", "fire-prediction-processor"],
                            [".", "Errors", ".", "."],
                            [".", "Invocations", ".", "."]
                        ],
                        "period": 300,
                        "stat": "Sum",
                        "region": "us-east-1",
                        "title": "Lambda ì„±ëŠ¥"
                    }
                },
                {
                    "type": "metric",
                    "x": 8, "y": 12, "width": 8, "height": 6,
                    "properties": {
                        "metrics": [
                            ["AWS/Kinesis", "IncomingRecords", "StreamName", "fire-prediction-stream"],
                            [".", "OutgoingRecords", ".", "."],
                            [".", "WriteProvisionedThroughputExceeded", ".", "."]
                        ],
                        "period": 300,
                        "stat": "Sum",
                        "region": "us-east-1",
                        "title": "Kinesis ìŠ¤íŠ¸ë¦¼ ìƒíƒœ"
                    }
                },
                {
                    "type": "metric",
                    "x": 16, "y": 12, "width": 8, "height": 6,
                    "properties": {
                        "metrics": [
                            ["AWS/DynamoDB", "ConsumedReadCapacityUnits", "TableName", "FirePrediction-RawData"],
                            [".", "ConsumedWriteCapacityUnits", ".", "."],
                            [".", "ThrottledRequests", ".", "."]
                        ],
                        "period": 300,
                        "stat": "Sum",
                        "region": "us-east-1",
                        "title": "DynamoDB ì‚¬ìš©ëŸ‰"
                    }
                }
            ]
        }
        
        try:
            response = self.cloudwatch.put_dashboard(
                DashboardName='FirePredictionSystem',
                DashboardBody=json.dumps(dashboard_body)
            )
            print("Dashboard created successfully")
            return response
        except Exception as e:
            print(f"Dashboard creation error: {e}")
            return None

class AlarmManager:
    """CloudWatch ì•ŒëŒ ê´€ë¦¬"""
    
    def __init__(self):
        self.cloudwatch = boto3.client('cloudwatch')
        self.sns = boto3.client('sns')
        
    def create_system_alarms(self):
        """ì‹œìŠ¤í…œ ì•ŒëŒ ìƒì„±"""
        
        alarms = [
            {
                'AlarmName': 'HighRiskDeviceCount',
                'ComparisonOperator': 'GreaterThanThreshold',
                'EvaluationPeriods': 2,
                'MetricName': 'DeviceRiskScore',
                'Namespace': 'FirePrediction/Devices',
                'Period': 300,
                'Statistic': 'Average',
                'Threshold': 0.7,
                'ActionsEnabled': True,
                'AlarmActions': [
                    os.environ.get('HIGH_RISK_SNS_TOPIC_ARN')
                ],
                'AlarmDescription': 'ê³ ìœ„í—˜ ë””ë°”ì´ìŠ¤ ê°ì§€',
                'Unit': 'None'
            },
            {
                'AlarmName': 'ModelAccuracyDrop',
                'ComparisonOperator': 'LessThanThreshold',
                'EvaluationPeriods': 3,
                'MetricName': 'ModelAccuracy',
                'Namespace': 'FirePrediction/ModelPerformance',
                'Period': 3600,
                'Statistic': 'Average',
                'Threshold': 0.85,
                'ActionsEnabled': True,
                'AlarmActions': [
                    os.environ.get('MODEL_ALERT_SNS_TOPIC_ARN')
                ],
                'AlarmDescription': 'ëª¨ë¸ ì •í™•ë„ í•˜ë½',
                'Unit': 'None'
            },
            {
                'AlarmName': 'LambdaErrorRate',
                'ComparisonOperator': 'GreaterThanThreshold',
                'EvaluationPeriods': 2,
                'MetricName': 'Errors',
                'Namespace': 'AWS/Lambda',
                'Period': 300,
                'Statistic': 'Sum',
                'Threshold': 5,
                'ActionsEnabled': True,
                'AlarmActions': [
                    os.environ.get('SYSTEM_ALERT_SNS_TOPIC_ARN')
                ],
                'AlarmDescription': 'Lambda í•¨ìˆ˜ ì˜¤ë¥˜ìœ¨ ì¦ê°€',
                'Dimensions': [
                    {'Name': 'FunctionName', 'Value': 'fire-prediction-processor'}
                ]
            },
            {
                'AlarmName': 'KinesisThrottling',
                'ComparisonOperator': 'GreaterThanThreshold',
                'EvaluationPeriods': 1,
                'MetricName': 'WriteProvisionedThroughputExceeded',
                'Namespace': 'AWS/Kinesis',
                'Period': 300,
                'Statistic': 'Sum',
                'Threshold': 0,
                'ActionsEnabled': True,
                'AlarmActions': [
                    os.environ.get('SYSTEM_ALERT_SNS_TOPIC_ARN')
                ],
                'AlarmDescription': 'Kinesis ìŠ¤íŠ¸ë¦¼ ìŠ¤ë¡œí‹€ë§ ë°œìƒ',
                'Dimensions': [
                    {'Name': 'StreamName', 'Value': 'fire-prediction-stream'}
                ]
            },
            {
                'AlarmName': 'DynamoDBThrottling',
                'ComparisonOperator': 'GreaterThanThreshold',
                'EvaluationPeriods': 1,
                'MetricName': 'ThrottledRequests',
                'Namespace': 'AWS/DynamoDB',
                'Period': 300,
                'Statistic': 'Sum',
                'Threshold': 0,
                'ActionsEnabled': True,
                'AlarmActions': [
                    os.environ.get('SYSTEM_ALERT_SNS_TOPIC_ARN')
                ],
                'AlarmDescription': 'DynamoDB ìŠ¤ë¡œí‹€ë§ ë°œìƒ',
                'Dimensions': [
                    {'Name': 'TableName', 'Value': 'FirePrediction-RawData'}
                ]
            },
            {
                'AlarmName': 'HighTemperatureDevices',
                'ComparisonOperator': 'GreaterThanThreshold',
                'EvaluationPeriods': 1,
                'MetricName': 'DeviceTemperature',
                'Namespace': 'FirePrediction/Devices',
                'Period': 300,
                'Statistic': 'Maximum',
                'Threshold': 65,
                'ActionsEnabled': True,
                'AlarmActions': [
                    os.environ.get('CRITICAL_ALERT_TOPIC_ARN')
                ],
                'AlarmDescription': '65ë„ ì´ìƒ ê³ ì˜¨ ë””ë°”ì´ìŠ¤ ê°ì§€',
                'Unit': 'None'
            }
        ]
        
        for alarm in alarms:
            try:
                self.cloudwatch.put_metric_alarm(**alarm)
                print(f"Alarm created: {alarm['AlarmName']}")
            except Exception as e:
                print(f"Error creating alarm {alarm['AlarmName']}: {e}")
    
    def create_composite_alarm(self):
        """ë³µí•© ì•ŒëŒ ìƒì„± - ì—¬ëŸ¬ ì¡°ê±´ ì¡°í•©"""
        try:
            composite_alarm = {
                'AlarmName': 'FirePredictionSystemHealth',
                'AlarmDescription': 'í™”ì¬ ì˜ˆì¸¡ ì‹œìŠ¤í…œ ì „ì²´ ê±´ê°•ë„',
                'ActionsEnabled': True,
                'AlarmActions': [
                    os.environ.get('SYSTEM_ALERT_SNS_TOPIC_ARN')
                ],
                'AlarmRule': (
                    "ALARM('HighRiskDeviceCount') OR "
                    "ALARM('LambdaErrorRate') OR "
                    "ALARM('KinesisThrottling') OR "
                    "ALARM('DynamoDBThrottling')"
                )
            }
            
            self.cloudwatch.put_composite_alarm(**composite_alarm)
            print("Composite alarm created successfully")
            
        except Exception as e:
            print(f"Error creating composite alarm: {e}")

# 5.2 ì›¹ ëŒ€ì‹œë³´ë“œ êµ¬í˜„

# python

from flask import Flask, render_template, jsonify, request
import boto3
from datetime import datetime, timedelta
import json

app = Flask(__name__)

class WebDashboard:
    def __init__(self):
        self.dynamodb = boto3.resource('dynamodb')
        self.cloudwatch = boto3.client('cloudwatch')
        self.raw_data_table = self.dynamodb.Table('FirePrediction-RawData')
        self.device_state_table = self.dynamodb.Table('FirePrediction-DeviceState')
        self.alerts_table = self.dynamodb.Table('FirePrediction-Alerts')

@app.route('/')
def dashboard():
    """ë©”ì¸ ëŒ€ì‹œë³´ë“œ"""
    return render_template('dashboard.html')

@app.route('/api/overview')
def get_overview():
    """ì‹œìŠ¤í…œ ê°œìš” API"""
    dashboard = WebDashboard()
    
    try:
        # í™œì„± ë””ë°”ì´ìŠ¤ ìˆ˜
        response = dashboard.device_state_table.scan()
        all_devices = response['Items']
        
        # ìµœê·¼ 5ë¶„ ë‚´ í™œì„± ë””ë°”ì´ìŠ¤
        cutoff_time = (datetime.now() - timedelta(minutes=5)).isoformat()
        active_devices = [d for d in all_devices 
                         if d.get('last_seen', '') > cutoff_time]
        
        # ìœ„í—˜ë„ë³„ ë¶„í¬
        risk_distribution = {'NORMAL': 0, 'LOW': 0, 'MEDIUM': 0, 'HIGH': 0, 'CRITICAL': 0}
        for device in active_devices:
            risk_level = device.get('current_risk_level', 'NORMAL')
            risk_distribution[risk_level] += 1
        
        # ìµœê·¼ ì•Œë¦¼
        recent_alerts_response = dashboard.alerts_table.scan(
            FilterExpression='alert_timestamp > :cutoff',
            ExpressionAttributeValues={':cutoff': cutoff_time},
            Limit=10
        )
        
        return jsonify({
            'total_devices': len(all_devices),
            'active_devices': len(active_devices),
            'risk_distribution': risk_distribution,
            'recent_alerts': recent_alerts_response['Items'][:5],
            'system_status': 'HEALTHY' if len(active_devices) > len(all_devices) * 0.8 else 'DEGRADED'
        })
        
    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/api/device/<device_id>')
def get_device_details(device_id):
    """íŠ¹ì • ë””ë°”ì´ìŠ¤ ìƒì„¸ ì •ë³´"""
    dashboard = WebDashboard()
    
    try:
        # ë””ë°”ì´ìŠ¤ ìƒíƒœ
        device_response = dashboard.device_state_table.get_item(
            Key={'device_id': device_id}
        )
        
        if 'Item' not in device_response:
            return jsonify({'error': 'Device not found'}), 404
        
        device_info = device_response['Item']
        
        # ìµœê·¼ 24ì‹œê°„ ë°ì´í„°
        end_time = datetime.now()
        start_time = end_time - timedelta(hours=24)
        
        history_response = dashboard.raw_data_table.query(
            KeyConditionExpression=Key('device_id').eq(device_id) & 
                                 Key('timestamp').between(
                                     start_time.isoformat(),
                                     end_time.isoformat()
                                 ),
            ScanIndexForward=False,
            Limit=100
        )
        
        history_data = history_response['Items']
        
        # ì‹œê³„ì—´ ë°ì´í„° ì¤€ë¹„
        timeseries = []
        for item in reversed(history_data):
            timeseries.append({
                'timestamp': item['timestamp'],
                'temperature': max(float(item.get('pcb_temperature', 0)), 
                                 float(item.get('sensor_temperature', 0))),
                'risk_score': float(item.get('risk_indicators', {}).get('total_risk_score', 0)),
                'arc_count': int(item.get('arc_count', 0)),
                'voltage': float(item.get('voltage', 0)),
                'current': float(item.get('current', 0))
            })
        
        # ë””ë°”ì´ìŠ¤ë³„ ì•Œë¦¼
        alerts_response = dashboard.alerts_table.query(
            KeyConditionExpression=Key('device_id').eq(device_id),
            ScanIndexForward=False,
            Limit=20
        )
        
        return jsonify({
            'device_info': device_info,
            'timeseries_data': timeseries,
            'recent_alerts': alerts_response['Items'],
            'recommendations': generate_device_recommendations(device_info, history_data)
        })
        
    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/api/realtime/<device_id>')
def get_realtime_data(device_id):
    """ì‹¤ì‹œê°„ ë°ì´í„° ìŠ¤íŠ¸ë¦¼"""
    dashboard = WebDashboard()
    
    try:
        # ìµœê·¼ 1ë¶„ ë°ì´í„°
        end_time = datetime.now()
        start_time = end_time - timedelta(minutes=1)
        
        response = dashboard.raw_data_table.query(
            KeyConditionExpression=Key('device_id').eq(device_id) & 
                                 Key('timestamp').gte(start_time.isoformat()),
            ScanIndexForward=False,
            Limit=5
        )
        
        latest_data = response['Items'][0] if response['Items'] else None
        
        if latest_data:
            return jsonify({
                'timestamp': latest_data['timestamp'],
                'temperature': max(float(latest_data.get('pcb_temperature', 0)),
                                 float(latest_data.get('sensor_temperature', 0))),
                'risk_score': float(latest_data.get('risk_indicators', {}).get('total_risk_score', 0)),
                'voltage': float(latest_data.get('voltage', 0)),
                'current': float(latest_data.get('current', 0)),
                'arc_count': int(latest_data.get('arc_count', 0)),
                'alarm_status': latest_data.get('alarm_status', {}),
                'status_bits': int(latest_data.get('status_bits', 0))
            })
        else:
            return jsonify({'error': 'No recent data'}), 404
            
    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/api/metrics')
def get_system_metrics():
    """ì‹œìŠ¤í…œ ë©”íŠ¸ë¦­ API"""
    dashboard = WebDashboard()
    
    try:
        end_time = datetime.now()
        start_time = end_time - timedelta(hours=1)
        
        # CloudWatch ë©”íŠ¸ë¦­ ì¡°íšŒ
        metrics = {}
        
        # Lambda ë©”íŠ¸ë¦­
        lambda_response = dashboard.cloudwatch.get_metric_statistics(
            Namespace='AWS/Lambda',
            MetricName='Invocations',
            Dimensions=[{'Name': 'FunctionName', 'Value': 'fire-prediction-processor'}],
            StartTime=start_time,
            EndTime=end_time,
            Period=300,
            Statistics=['Sum']
        )
        
        metrics['lambda_invocations'] = lambda_response['Datapoints']
        
        # Kinesis ë©”íŠ¸ë¦­
        kinesis_response = dashboard.cloudwatch.get_metric_statistics(
            Namespace='AWS/Kinesis',
            MetricName='IncomingRecords',
            Dimensions=[{'Name': 'StreamName', 'Value': 'fire-prediction-stream'}],
            StartTime=start_time,
            EndTime=end_time,
            Period=300,
            Statistics=['Sum']
        )
        
        metrics['kinesis_records'] = kinesis_response['Datapoints']
        
        return jsonify(metrics)
        
    except Exception as e:
        return jsonify({'error': str(e)}), 500

def generate_device_recommendations(device_info, history_data):
    """ë””ë°”ì´ìŠ¤ë³„ ê¶Œì¥ì‚¬í•­ ìƒì„±"""
    recommendations = []
    
    current_risk = float(device_info.get('current_risk_score', 0))
    last_temp = float(device_info.get('last_temperature', 0))
    total_alarms = int(device_info.get('total_alarms', 0))
    
    if current_risk > 0.7:
        recommendations.append({
            'priority': 'HIGH',
            'type': 'IMMEDIATE_INSPECTION',
            'message': 'ì¦‰ì‹œ í˜„ì¥ ì ê²€ì´ í•„ìš”í•©ë‹ˆë‹¤.',
            'action': 'INSPECT'
        })
    
    if last_temp > 60:
        recommendations.append({
            'priority': 'HIGH',
            'type': 'COOLING_CHECK',
            'message': 'ëƒ‰ê° ì‹œìŠ¤í…œ ì ê²€ ë° í™˜ê¸° í™•ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤.',
            'action': 'MAINTENANCE'
        })
    
    if total_alarms > 10:
        recommendations.append({
            'priority': 'MEDIUM',
            'type': 'ALARM_ANALYSIS',
            'message': 'ë°˜ë³µì ì¸ ì•ŒëŒ ë°œìƒ ì›ì¸ ë¶„ì„ì´ í•„ìš”í•©ë‹ˆë‹¤.',
            'action': 'ANALYZE'
        })
    
    # íˆìŠ¤í† ë¦¬ ê¸°ë°˜ ê¶Œì¥ì‚¬í•­
    if len(history_data) > 10:
        arc_counts = [int(item.get('arc_count', 0)) for item in history_data[-10:]]
        if sum(arc_counts) > 5:
            recommendations.append({
                'priority': 'MEDIUM',
                'type': 'ELECTRICAL_CHECK',
                'message': 'ì „ê¸° ì—°ê²°ë¶€ ë° ì ˆì—° ìƒíƒœ ì ê²€ì„ ê¶Œì¥í•©ë‹ˆë‹¤.',
                'action': 'MAINTENANCE'
            })
    
    return recommendations

### 5.3 HTML í…œí”Œë¦¿ë“¤

#### dashboard.html (ë©”ì¸ ëŒ€ì‹œë³´ë“œ)

```html
<!DOCTYPE html>
<html lang="ko">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>í™”ì¬ ì˜ˆì¸¡ ì‹œìŠ¤í…œ ëŒ€ì‹œë³´ë“œ</title>
    <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
    <script src="https://code.jquery.com/jquery-3.6.0.min.js"></script>
    <style>
        body { font-family: Arial, sans-serif; margin: 0; padding: 20px; background: #f5f5f5; }
        .header { background: #2c3e50; color: white; padding: 20px; border-radius: 8px; margin-bottom: 20px; }
        .dashboard-grid { display: grid; grid-template-columns: repeat(auto-fit, minmax(300px, 1fr)); gap: 20px; }
        .card { background: white; padding: 20px; border-radius: 8px; box-shadow: 0 2px 4px rgba(0,0,0,0.1); }
        .metric { text-align: center; margin: 10px 0; }
        .metric-value { font-size: 2em; font-weight: bold; }
        .metric-label { color: #666; }
        .risk-high { color: #e74c3c; }
        .risk-medium { color: #f39c12; }
        .risk-low { color: #27ae60; }
        .alert-item { padding: 10px; margin: 5px 0; border-left: 4px solid #e74c3c; background: #f8f9fa; }
        .device-list { max-height: 400px; overflow-y: auto; }
        .device-item { padding: 10px; border-bottom: 1px solid #eee; cursor: pointer; }
        .device-item:hover { background: #f8f9fa; }
        .status-indicator { width: 10px; height: 10px; border-radius: 50%; display: inline-block; margin-right: 10px; }
        .status-normal { background: #27ae60; }
        .status-warning { background: #f39c12; }
        .status-critical { background: #e74c3c; }
    </style>
</head>
<body>
    <div class="header">
        <h1>ğŸ”¥ í™”ì¬ ì˜ˆì¸¡ ì‹œìŠ¤í…œ ëŒ€ì‹œë³´ë“œ</h1>
        <p>ì‹¤ì‹œê°„ IoT ë””ë°”ì´ìŠ¤ ëª¨ë‹ˆí„°ë§ ë° í™”ì¬ ìœ„í—˜ ì˜ˆì¸¡</p>
    </div>

    <div class="dashboard-grid">
        <!-- ì‹œìŠ¤í…œ ê°œìš” -->
        <div class="card">
            <h3>ì‹œìŠ¤í…œ ê°œìš”</h3>
            <div class="metric">
                <div class="metric-value" id="total-devices">-</div>
                <div class="metric-label">ì´ ë””ë°”ì´ìŠ¤</div>
            </div>
            <div class="metric">
                <div class="metric-value" id="active-devices">-</div>
                <div class="metric-label">í™œì„± ë””ë°”ì´ìŠ¤</div>
            </div>
            <div class="metric">
                <div class="metric-value" id="system-status">-</div>
                <div class="metric-label">ì‹œìŠ¤í…œ ìƒíƒœ</div>
            </div>
        </div>

        <!-- ìœ„í—˜ë„ ë¶„í¬ -->
        <div class="card">
            <h3>ìœ„í—˜ë„ ë¶„í¬</h3>
            <canvas id="risk-distribution-chart" width="400" height="200"></canvas>
        </div>

        <!-- ìµœê·¼ ì•Œë¦¼ -->
        <div class="card">
            <h3>ìµœê·¼ ì•Œë¦¼</h3>
            <div id="recent-alerts" class="alert-list">
                <!-- ì•Œë¦¼ ëª©ë¡ì´ ì—¬ê¸°ì— í‘œì‹œë©ë‹ˆë‹¤ -->
            </div>
        </div>

        <!-- ë””ë°”ì´ìŠ¤ ëª©ë¡ -->
        <div class="card">
            <h3>ë””ë°”ì´ìŠ¤ ëª©ë¡</h3>
            <div id="device-list" class="device-list">
                <!-- ë””ë°”ì´ìŠ¤ ëª©ë¡ì´ ì—¬ê¸°ì— í‘œì‹œë©ë‹ˆë‹¤ -->
            </div>
        </div>

        <!-- ì‹¤ì‹œê°„ ë©”íŠ¸ë¦­ -->
        <div class="card">
            <h3>ì‹¤ì‹œê°„ ë©”íŠ¸ë¦­</h3>
            <canvas id="realtime-metrics-chart" width="400" height="200"></canvas>
        </div>

        <!-- ì‹œìŠ¤í…œ ì„±ëŠ¥ -->
        <div class="card">
            <h3>ì‹œìŠ¤í…œ ì„±ëŠ¥</h3>
            <div class="metric">
                <div class="metric-value" id="lambda-invocations">-</div>
                <div class="metric-label">Lambda í˜¸ì¶œ/ì‹œê°„</div>
            </div>
            <div class="metric">
                <div class="metric-value" id="kinesis-records">-</div>
                <div class="metric-label">Kinesis ë ˆì½”ë“œ/ì‹œê°„</div>
            </div>
        </div>
    </div>

    <script>
        // ì°¨íŠ¸ ì´ˆê¸°í™”
        let riskChart, metricsChart;
        
        function initCharts() {
            // ìœ„í—˜ë„ ë¶„í¬ ì°¨íŠ¸
            const riskCtx = document.getElementById('risk-distribution-chart').getContext('2d');
            riskChart = new Chart(riskCtx, {
                type: 'doughnut',
                data: {
                    labels: ['ì •ìƒ', 'ë‚®ìŒ', 'ë³´í†µ', 'ë†’ìŒ', 'ìœ„í—˜'],
                    datasets: [{
                        data: [0, 0, 0, 0, 0],
                        backgroundColor: ['#27ae60', '#2ecc71', '#f39c12', '#e67e22', '#e74c3c']
                    }]
                },
                options: {
                    responsive: true,
                    maintainAspectRatio: false
                }
            });

            // ì‹¤ì‹œê°„ ë©”íŠ¸ë¦­ ì°¨íŠ¸
            const metricsCtx = document.getElementById('realtime-metrics-chart').getContext('2d');
            metricsChart = new Chart(metricsCtx, {
                type: 'line',
                data: {
                    labels: [],
                    datasets: [{
                        label: 'í‰ê·  ìœ„í—˜ë„',
                        data: [],
                        borderColor: '#e74c3c',
                        tension: 0.1
                    }]
                },
                options: {
                    responsive: true,
                    maintainAspectRatio: false,
                    scales: {
                        y: {
                            beginAtZero: true,
                            max: 1
                        }
                    }
                }
            });
        }

        function updateOverview() {
            $.get('/api/overview', function(data) {
                $('#total-devices').text(data.total_devices);
                $('#active-devices').text(data.active_devices);
                $('#system-status').text(data.system_status);
                
                // ìœ„í—˜ë„ ë¶„í¬ ì—…ë°ì´íŠ¸
                const distribution = data.risk_distribution;
                riskChart.data.datasets[0].data = [
                    distribution.NORMAL,
                    distribution.LOW,
                    distribution.MEDIUM,
                    distribution.HIGH,
                    distribution.CRITICAL
                ];
                riskChart.update();
                
                // ìµœê·¼ ì•Œë¦¼ ì—…ë°ì´íŠ¸
                const alertsHtml = data.recent_alerts.map(alert => `
                    <div class="alert-item">
                        <strong>ë””ë°”ì´ìŠ¤ ${alert.device_id}</strong> - ${alert.alert_level}<br>
                        <small>${new Date(alert.alert_timestamp).toLocaleString()}</small>
                    </div>
                `).join('');
                $('#recent-alerts').html(alertsHtml);
            });
        }

        function updateMetrics() {
            $.get('/api/metrics', function(data) {
                if (data.lambda_invocations && data.lambda_invocations.length > 0) {
                    const latest = data.lambda_invocations[data.lambda_invocations.length - 1];
                    $('#lambda-invocations').text(latest.Sum || 0);
                }
                
                if (data.kinesis_records && data.kinesis_records.length > 0) {
                    const latest = data.kinesis_records[data.kinesis_records.length - 1];
                    $('#kinesis-records').text(latest.Sum || 0);
                }
            });
        }

        // ì´ˆê¸°í™” ë° ì£¼ê¸°ì  ì—…ë°ì´íŠ¸
        $(document).ready(function() {
            initCharts();
            updateOverview();
            updateMetrics();
            
            // 5ì´ˆë§ˆë‹¤ ì—…ë°ì´íŠ¸
            setInterval(updateOverview, 5000);
            setInterval(updateMetrics, 10000);
        });
    </script>
</body>
</html>

# 6. ì‹œìŠ¤í…œ ë°°í¬ ë° Infrastructure as Code

# 6.1 CloudFormation í…œí”Œë¦¿
# yaml

AWSTemplateFormatVersion: '2010-09-09'
Description: 'AWS IoT Core ê¸°ë°˜ í™”ì¬ ì˜ˆì¸¡ ì‹œìŠ¤í…œ'

Parameters:
  Environment:
    Type: String
    Default: 'dev'
    AllowedValues: ['dev', 'staging', 'prod']
  
  NotificationEmail:
    Type: String
    Description: 'ì•Œë¦¼ì„ ë°›ì„ ì´ë©”ì¼ ì£¼ì†Œ'

Resources:
  # IAM ì—­í• 
  IoTFirePredictionRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: [lambda.amazonaws.com, iot.amazonaws.com, sagemaker.amazonaws.com]
            Action: sts:AssumeRole
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole
        - arn:aws:iam::aws:policy/AmazonDynamoDBFullAccess
        - arn:aws:iam::aws:policy/AmazonKinesisFullAccess
        - arn:aws:iam::aws:policy/AmazonSNSFullAccess
        - arn:aws:iam::aws:policy/CloudWatchFullAccess
        - arn:aws:iam::aws:policy/AmazonSageMakerFullAccess

  # DynamoDB í…Œì´ë¸”ë“¤
  RawDataTable:
    Type: AWS::DynamoDB::Table
    Properties:
      TableName: !Sub 'FirePrediction-RawData-${Environment}'
      BillingMode: PAY_PER_REQUEST
      AttributeDefinitions:
        - AttributeName: device_id
          AttributeType: S
        - AttributeName: timestamp
          AttributeType: S
      KeySchema:
        - AttributeName: device_id
          KeyType: HASH
        - AttributeName: timestamp
          KeyType: RANGE
      TimeToLiveSpecification:
        AttributeName: ttl
        Enabled: true
      StreamSpecification:
        StreamViewType: NEW_AND_OLD_IMAGES

  DeviceStateTable:
    Type: AWS::DynamoDB::Table
    Properties:
      TableName: !Sub 'FirePrediction-DeviceState-${Environment}'
      BillingMode: PAY_PER_REQUEST
      AttributeDefinitions:
        - AttributeName: device_id
          AttributeType: S
      KeySchema:
        - AttributeName: device_id
          KeyType: HASH

  AlertsTable:
    Type: AWS::DynamoDB::Table
    Properties:
      TableName: !Sub 'FirePrediction-Alerts-${Environment}'
      BillingMode: PAY_PER_REQUEST
      AttributeDefinitions:
        - AttributeName: device_id
          AttributeType: S
        - AttributeName: alert_timestamp
          AttributeType: S
      KeySchema:
        - AttributeName: device_id
          KeyType: HASH
        - AttributeName: alert_timestamp
          KeyType: RANGE
      TimeToLiveSpecification:
        AttributeName: ttl
        Enabled: true

  # Kinesis Stream
  FirePredictionStream:
    Type: AWS::Kinesis::Stream
    Properties:
      Name: !Sub 'fire-prediction-stream-${Environment}'
      ShardCount: 2
      RetentionPeriodHours: 24

  # SNS í† í”½ë“¤
  CriticalAlertTopic:
    Type: AWS::SNS::Topic
    Properties:
      TopicName: !Sub 'fire-prediction-critical-alerts-${Environment}'
      Subscription:
        - Endpoint: !Ref NotificationEmail
          Protocol: email

  HighRiskAlertTopic:
    Type: AWS::SNS::Topic
    Properties:
      TopicName: !Sub 'fire-prediction-high-risk-alerts-${Environment}'
      Subscription:
        - Endpoint: !Ref NotificationEmail
          Protocol: email

  # Lambda í•¨ìˆ˜
  FirePredictionProcessor:
    Type: AWS::Lambda::Function
    Properties:
      FunctionName: !Sub 'fire-prediction-processor-${Environment}'
      Runtime: python3.9
      Handler: lambda_function.lambda_handler
      Role: !GetAtt IoTFirePredictionRole.Arn
      Timeout: 300
      MemorySize: 512
      Environment:
        Variables:
          KINESIS_STREAM_NAME: !Ref FirePredictionStream
          CRITICAL_ALERT_TOPIC_ARN: !Ref CriticalAlertTopic
          HIGH_RISK_ALERT_TOPIC_ARN: !Ref HighRiskAlertTopic
          ENVIRONMENT: !Ref Environment
      Code:
        ZipFile: |
          # Lambda ì½”ë“œê°€ ì—¬ê¸°ì— í¬í•¨ë©ë‹ˆë‹¤
          def lambda_handler(event, context):
              return {'statusCode': 200}

  # IoT Rule
  FirePredictionIoTRule:
    Type: AWS::IoT::TopicRule
    Properties:
      RuleName: !Sub 'FirePredictionRule${Environment}'
      TopicRulePayload:
        Sql: !Sub |
          SELECT deviceId, idx, status, volt, voltLow, ct as current, 
                 zct as leakage_current, rzct as resistive_leakage, 
                 pwr as power, pcbTemp, sensorTemp, co, voc, 
                 pFct as power_factor, aArc as arc_count, 
                 timestamp() as timestamp
          FROM 'fire-prediction/device/+/data'
          WHERE status < 4096
        Actions:
          - Lambda:
              FunctionArn: !GetAtt FirePredictionProcessor.Arn

  # Lambda ê¶Œí•œ (IoT Ruleì—ì„œ í˜¸ì¶œ)
  LambdaInvokePermission:
    Type: AWS::Lambda::Permission
    Properties:
      FunctionName: !Ref FirePredictionProcessor
      Action: lambda:InvokeFunction
      Principal: iot.amazonaws.com
      SourceArn: !GetAtt FirePredictionIoTRule.Arn

Outputs:
  StreamName:
    Description: 'Kinesis Stream Name'
    Value: !Ref FirePredictionStream
    Export:
      Name: !Sub '${AWS::StackName}-StreamName'
  
  ProcessorFunctionArn:
    Description: 'Lambda Function ARN'
    Value: !GetAtt FirePredictionProcessor.Arn
    Export:
      Name: !Sub '${AWS::StackName}-ProcessorFunctionArn'

# 6.2 ë°°í¬ ìŠ¤í¬ë¦½íŠ¸

# bash

#!/bin/bash
# deploy.sh - í™”ì¬ ì˜ˆì¸¡ ì‹œìŠ¤í…œ ë°°í¬ ìŠ¤í¬ë¦½íŠ¸

set -e

ENVIRONMENT=${1:-dev}
REGION=${2:-us-east-1}
EMAIL=${3:-admin@example.com}
STACK_NAME="fire-prediction-system-${ENVIRONMENT}"

echo "ğŸš€ í™”ì¬ ì˜ˆì¸¡ ì‹œìŠ¤í…œ ë°°í¬ ì‹œì‘ - í™˜ê²½: ${ENVIRONMENT}"

# 1. CloudFormation ìŠ¤íƒ ë°°í¬
echo "ğŸ“¦ CloudFormation ìŠ¤íƒ ë°°í¬ ì¤‘..."
aws cloudformation deploy \
  --template-file cloudformation.yaml \
  --stack-name ${STACK_NAME} \
  --parameter-overrides \
    Environment=${ENVIRONMENT} \
    NotificationEmail=${EMAIL} \
  --capabilities CAPABILITY_IAM \
  --region ${REGION}

# 2. Lambda í•¨ìˆ˜ ì½”ë“œ íŒ¨í‚¤ì§• ë° ì—…ë°ì´íŠ¸
echo "ğŸ“ Lambda í•¨ìˆ˜ ì½”ë“œ ì—…ë°ì´íŠ¸ ì¤‘..."
cd lambda
pip install -r requirements.txt -t .
zip -r ../lambda-deployment.zip .
cd ..

aws lambda update-function-code \
  --function-name fire-prediction-processor-${ENVIRONMENT} \
  --zip-file fileb://lambda-deployment.zip \
  --region ${REGION}

# 3. SageMaker ëª¨ë¸ ë°°í¬ (ì„ íƒì )
if [ "${ENVIRONMENT}" = "prod" ]; then
    echo "ğŸ¤– SageMaker ëª¨ë¸ ë°°í¬ ì¤‘..."
    python3 deploy_sagemaker.py --environment ${ENVIRONMENT}
fi

# 4. ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ ì„¤ì •
echo "ğŸ“Š CloudWatch ëŒ€ì‹œë³´ë“œ ì„¤ì • ì¤‘..."
python3 setup_dashboard.py --environment ${ENVIRONMENT}

# 5. ì´ˆê¸° í…ŒìŠ¤íŠ¸ ë°ì´í„° ì „ì†¡
echo "ğŸ§ª ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì¤‘..."
python3 test_system.py --environment ${ENVIRONMENT}

echo "âœ… ë°°í¬ ì™„ë£Œ!"
echo "ğŸ“ˆ ëŒ€ì‹œë³´ë“œ: https://console.aws.amazon.com/cloudwatch/home?region=${REGION}#dashboards:name=FirePredictionSystem"
echo "ğŸ“‹ ë¡œê·¸: https://console.aws.amazon.com/cloudwatch/home?region=${REGION}#logsV2:log-groups/log-group/%2Faws%2Flambda%2Ffire-prediction-processor-${ENVIRONMENT}"

# 6.3 ìš´ì˜ ê°€ì´ë“œ

# markdown

# í™”ì¬ ì˜ˆì¸¡ ì‹œìŠ¤í…œ ìš´ì˜ ê°€ì´ë“œ

## ì¼ì¼ ì²´í¬ë¦¬ìŠ¤íŠ¸

### ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸
- [ ] CloudWatch ëŒ€ì‹œë³´ë“œì—ì„œ ì „ì²´ ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸
- [ ] í™œì„± ë””ë°”ì´ìŠ¤ ìˆ˜ê°€ ì˜ˆìƒ ë²”ìœ„ ë‚´ì¸ì§€ í™•ì¸
- [ ] Lambda í•¨ìˆ˜ ì˜¤ë¥˜ìœ¨ì´ 5% ë¯¸ë§Œì¸ì§€ í™•ì¸
- [ ] Kinesis ìŠ¤íŠ¸ë¦¼ì— ë°ì´í„°ê°€ ì •ìƒ ìœ ì…ë˜ëŠ”ì§€ í™•ì¸

### ì•Œë¦¼ ë° ê²½ê³  ê²€í† 
- [ ] ì§€ë‚œ 24ì‹œê°„ ë‚´ ë°œìƒí•œ CRITICAL/HIGH ì•Œë¦¼ ê²€í† 
- [ ] ì•Œë¦¼ì˜ ì ì •ì„± í‰ê°€ (False Positive ì—¬ë¶€)
- [ ] ì¡°ì¹˜ê°€ í•„ìš”í•œ ë””ë°”ì´ìŠ¤ ì‹ë³„ ë° í˜„ì¥íŒ€ ì „ë‹¬
- [ ] ì•Œë¦¼ íŒ¨í„´ ë¶„ì„ (íŠ¹ì • ì‹œê°„ëŒ€/ìœ„ì¹˜ ì§‘ì¤‘ ì—¬ë¶€)

### ì„±ëŠ¥ ë©”íŠ¸ë¦­ ëª¨ë‹ˆí„°ë§
- [ ] ëª¨ë¸ ì •í™•ë„ê°€ 85% ì´ìƒ ìœ ì§€ë˜ëŠ”ì§€ í™•ì¸
- [ ] í‰ê·  ì‘ë‹µ ì‹œê°„ì´ 2ì´ˆ ì´í•˜ì¸ì§€ í™•ì¸
- [ ] DynamoDB ì½ê¸°/ì“°ê¸° ìš©ëŸ‰ ì‚¬ìš©ë¥  í™•ì¸
- [ ] ë¹„ìš© ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§

## ì£¼ê°„ ì²´í¬ë¦¬ìŠ¤íŠ¸

### ëª¨ë¸ ì„±ëŠ¥ ë¶„ì„
- [ ] ì£¼ê°„ ëª¨ë¸ ì„±ëŠ¥ ë¦¬í¬íŠ¸ ìƒì„±
- [ ] Precision, Recall, F1-Score íŠ¸ë Œë“œ ë¶„ì„
- [ ] ì˜¤íƒ(False Positive) ë° ë¯¸íƒ(False Negative) ì‚¬ë¡€ ë¶„ì„
- [ ] í•„ìš”ì‹œ ëª¨ë¸ ì¬í•™ìŠµ ê³„íš ìˆ˜ë¦½

### ì‹œìŠ¤í…œ ìµœì í™”
- [ ] ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰ ë¶„ì„ ë° ìµœì í™” ê¸°íšŒ ì‹ë³„
- [ ] ë¡œê·¸ ë¶„ì„ì„ í†µí•œ ì„±ëŠ¥ ë³‘ëª© ì§€ì  íŒŒì•…
- [ ] ë””ë°”ì´ìŠ¤ë³„ ë°ì´í„° í’ˆì§ˆ í‰ê°€
- [ ] ì•Œë¦¼ ì„ê³„ê°’ ì¡°ì • í•„ìš”ì„± ê²€í† 

### ë°ì´í„° í’ˆì§ˆ ê´€ë¦¬
- [ ] ì„¼ì„œ ë°ì´í„° ì´ìƒì¹˜ íŒ¨í„´ ë¶„ì„
- [ ] í†µì‹  ì˜¤ë¥˜ ë¹ˆë„ ë¶„ì„
- [ ] ë””ë°”ì´ìŠ¤ë³„ ë°ì´í„° ì™„ê²°ì„± ê²€ì‚¬
- [ ] ìƒˆë¡œìš´ ì„¼ì„œ ìœ í˜• ë˜ëŠ” íŒ¨í„´ ì‹ë³„

## ì›”ê°„ ì²´í¬ë¦¬ìŠ¤íŠ¸

### ì‹œìŠ¤í…œ ìœ ì§€ë³´ìˆ˜
- [ ] ì „ì²´ ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜ ê²€í† 
- [ ] ë³´ì•ˆ ì·¨ì•½ì  ì ê²€ ë° íŒ¨ì¹˜ ì ìš©
- [ ] ë°±ì—… ë° ë³µêµ¬ ì ˆì°¨ í…ŒìŠ¤íŠ¸
- [ ] ìš©ëŸ‰ ê³„íš ë° í™•ì¥ì„± ê²€í† 

### ë¹„ì¦ˆë‹ˆìŠ¤ ê°€ì¹˜ í‰ê°€
- [ ] í™”ì¬ ì˜ˆë°© íš¨ê³¼ ì¸¡ì •
- [ ] ROI(íˆ¬ì ìˆ˜ìµë¥ ) ê³„ì‚°
- [ ] ì‚¬ìš©ì í”¼ë“œë°± ìˆ˜ì§‘ ë° ë¶„ì„
- [ ] ì‹œìŠ¤í…œ ê°œì„  ìš°ì„ ìˆœìœ„ ì„¤ì •

## ì¥ì•  ëŒ€ì‘ ì ˆì°¨

### Level 1: ê²½ë¯¸í•œ ë¬¸ì œ
**ì¦ìƒ**: ì¼ë¶€ ë””ë°”ì´ìŠ¤ ì˜¤í”„ë¼ì¸, ê°€ë” ë°œìƒí•˜ëŠ” Lambda ì˜¤ë¥˜
**ëŒ€ì‘**:
1. CloudWatch ë¡œê·¸ì—ì„œ ì˜¤ë¥˜ íŒ¨í„´ í™•ì¸
2. ì˜í–¥ë°›ëŠ” ë””ë°”ì´ìŠ¤ ì¬ì‹œì‘ ì‹œë„
3. 24ì‹œê°„ ëª¨ë‹ˆí„°ë§ í›„ ìë™ ë³µêµ¬ë˜ì§€ ì•Šìœ¼ë©´ Level 2ë¡œ escalate

### Level 2: ì¤‘ê°„ ìˆ˜ì¤€ ë¬¸ì œ
**ì¦ìƒ**: ë‹¤ìˆ˜ ë””ë°”ì´ìŠ¤ ì˜¤í”„ë¼ì¸, Lambda ì˜¤ë¥˜ìœ¨ > 10%, ëª¨ë¸ ì •í™•ë„ ê¸‰ê²©í•œ í•˜ë½
**ëŒ€ì‘**:
1. ì¦‰ì‹œ ìš´ì˜íŒ€ì— ì•Œë¦¼
2. ì‹œìŠ¤í…œ ìƒíƒœ ìƒì„¸ ì§„ë‹¨
3. í•„ìš”ì‹œ ë°±ì—… ì‹œìŠ¤í…œìœ¼ë¡œ ì „í™˜
4. ê·¼ë³¸ ì›ì¸ ë¶„ì„ ë° ì„ì‹œ í•´ê²°ì±… ì ìš©

### Level 3: ì‹¬ê°í•œ ì¥ì• 
**ì¦ìƒ**: ì‹œìŠ¤í…œ ì „ì²´ ë‹¤ìš´, ë°ì´í„° ìœ ì‹¤, ë³´ì•ˆ ì¹¨í•´
**ëŒ€ì‘**:
1. ì¦‰ì‹œ ëª¨ë“  ì´í•´ê´€ê³„ìì—ê²Œ ê¸´ê¸‰ ì•Œë¦¼
2. ì¥ì•  ë³µêµ¬íŒ€ ì†Œì§‘
3. ë¹„ìƒ ì ˆì°¨ í™œì„±í™”
4. ì™¸ë¶€ ì§€ì› ìš”ì²­ ê²€í† 

## ëª¨ë‹ˆí„°ë§ ë©”íŠ¸ë¦­ ê¸°ì¤€ê°’

### ì‹œìŠ¤í…œ ì„±ëŠ¥
- Lambda í•¨ìˆ˜ ì‘ë‹µ ì‹œê°„: < 2ì´ˆ (ëª©í‘œ), < 5ì´ˆ (ê²½ê³ )
- Kinesis ì²˜ë¦¬ ì§€ì—°: < 1ë¶„ (ëª©í‘œ), < 5ë¶„ (ê²½ê³ )
- DynamoDB ì‘ë‹µ ì‹œê°„: < 100ms (ëª©í‘œ), < 500ms (ê²½ê³ )

### ëª¨ë¸ ì„±ëŠ¥
- ì •í™•ë„(Accuracy): > 90% (ëª©í‘œ), > 85% (ìµœì†Œ)
- ì •ë°€ë„(Precision): > 85% (ëª©í‘œ), > 80% (ìµœì†Œ)
- ì¬í˜„ìœ¨(Recall): > 90% (ëª©í‘œ), > 85% (ìµœì†Œ)

### ë¹„ì¦ˆë‹ˆìŠ¤ ë©”íŠ¸ë¦­
- í—ˆìœ„ ê²½ë³´ìœ¨: < 5% (ëª©í‘œ), < 10% (í—ˆìš©)
- ì‹œìŠ¤í…œ ê°€ìš©ì„±: > 99.9% (ëª©í‘œ), > 99.5% (ìµœì†Œ)
- í‰ê·  ê°ì§€ ì‹œê°„: < 5ë¶„ (ëª©í‘œ), < 15ë¶„ (ìµœëŒ€)

## ë°±ì—… ë° ë³µêµ¬

### ë°ì´í„° ë°±ì—…
- DynamoDB: Point-in-time recovery í™œì„±í™”
- S3: Cross-region replication ì„¤ì •
- ëª¨ë¸ ì•„í‹°íŒ©íŠ¸: ë‹¤ì¤‘ ë¦¬ì „ ë°±ì—…

### ë³µêµ¬ ì ˆì°¨
1. **RTO (Recovery Time Objective)**: 30ë¶„
2. **RPO (Recovery Point Objective)**: 5ë¶„
3. **ë³µêµ¬ ìš°ì„ ìˆœìœ„**: ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ > ì•Œë¦¼ ì‹œìŠ¤í…œ > ë¶„ì„ ê¸°ëŠ¥

## ë³´ì•ˆ ê°€ì´ë“œë¼ì¸

### ì ‘ê·¼ ì œì–´
- IAM ì—­í•  ê¸°ë°˜ ìµœì†Œ ê¶Œí•œ ì›ì¹™ ì ìš©
- ì •ê¸°ì ì¸ ê¶Œí•œ ê²€í†  ë° ê°ì‚¬
- Multi-factor authentication í•„ìˆ˜

### ë°ì´í„° ë³´í˜¸
- ì „ì†¡ ì¤‘ ì•”í˜¸í™” (TLS 1.2+)
- ì €ì¥ ì¤‘ ì•”í˜¸í™” (AES-256)
- ê°œì¸ì •ë³´ ìµœì†Œ ìˆ˜ì§‘ ì›ì¹™

### ê·œì • ì¤€ìˆ˜
- ê´€ë ¨ ì‚°ì—… í‘œì¤€ ë° ê·œì • ì¤€ìˆ˜
- ì •ê¸°ì ì¸ ë³´ì•ˆ ê°ì‚¬ ì‹¤ì‹œ
- ì‚¬ê³  ëŒ€ì‘ ì ˆì°¨ ë¬¸ì„œí™”

## ì„±ëŠ¥ ìµœì í™” ê°€ì´ë“œ

### ë¹„ìš© ìµœì í™”

```python
# ë¹„ìš© ëª¨ë‹ˆí„°ë§ ìŠ¤í¬ë¦½íŠ¸
import boto3
from datetime import datetime, timedelta

def monitor_costs():
    ce_client = boto3.client('ce')
    
    end_date = datetime.now()
    start_date = end_date - timedelta(days=30)
    
    response = ce_client.get_cost_and_usage(
        TimePeriod={
            'Start': start_date.strftime('%Y-%m-%d'),
            'End': end_date.strftime('%Y-%m-%d')
        },
        Granularity='DAILY',
        Metrics=['BlendedCost'],
        GroupBy=[
            {'Type': 'DIMENSION', 'Key': 'SERVICE'}
        ]
    )
    
    # ì„œë¹„ìŠ¤ë³„ ë¹„ìš© ë¶„ì„
    for result in response['ResultsByTime']:
        print(f"Date: {result['TimePeriod']['Start']}")
        for group in result['Groups']:
            service = group['Keys'][0]
            cost = group['Metrics']['BlendedCost']['Amount']
            print(f"  {service}: ${cost}")

if __name__ == "__main__":
    monitor_costs()

# ìë™ ìŠ¤ì¼€ì¼ë§ ì„¤ì •

# yaml

# CloudWatch ê¸°ë°˜ ìë™ ìŠ¤ì¼€ì¼ë§
AutoScalingPolicy:
  Type: AWS::ApplicationAutoScaling::ScalingPolicy
  Properties:
    PolicyName: FirePredictionAutoScaling
    PolicyType: TargetTrackingScaling
    TargetTrackingScalingPolicyConfiguration:
      TargetValue: 70.0
      PredefinedMetricSpecification:
        PredefinedMetricType: DynamoDBReadCapacityUtilization

# ë¬¸ì œ í•´ê²° ê°€ì´ë“œ

# ìì£¼ ë°œìƒí•˜ëŠ” ë¬¸ì œë“¤

# 1. Lambda í•¨ìˆ˜ íƒ€ì„ì•„ì›ƒ
- ì›ì¸: ëŒ€ëŸ‰ ë°ì´í„° ì²˜ë¦¬, DynamoDB ì‘ë‹µ ì§€ì—°
- í•´ê²°ì±…:

- íƒ€ì„ì•„ì›ƒ ì‹œê°„ ì¦ê°€ (ìµœëŒ€ 15ë¶„)
- ë°°ì¹˜ í¬ê¸° ì¡°ì •
- DynamoDB ì½ê¸°/ì“°ê¸° ìš©ëŸ‰ ì¦ê°€

# 2. Kinesis ìŠ¤íŠ¸ë¦¼ ì§€ì—°
ì›ì¸: ìƒ¤ë“œ ìˆ˜ ë¶€ì¡±, ì²˜ë¦¬ ì§€ì—°
í•´ê²°ì±…:

ìƒ¤ë“œ ìˆ˜ ì¦ê°€
ë³‘ë ¬ ì²˜ë¦¬ ë¡œì§ ìµœì í™”
ë°°ì¹˜ í¬ê¸° ì¡°ì •

# 3. ëª¨ë¸ ì„±ëŠ¥ ì €í•˜
ì›ì¸: ë°ì´í„° ë“œë¦¬í”„íŠ¸, ê°œë… ë“œë¦¬í”„íŠ¸
í•´ê²°ì±…:

ìƒˆë¡œìš´ ë°ì´í„°ë¡œ ëª¨ë¸ ì¬í•™ìŠµ
íŠ¹ì§• ì—”ì§€ë‹ˆì–´ë§ ê°œì„ 
ì•™ìƒë¸” ëª¨ë¸ ì ìš©

# 4. ë†’ì€ ì˜¤íƒë¥ 
ì›ì¸: ì„ê³„ê°’ ì„¤ì • ë¶€ì ì ˆ, í™˜ê²½ ë³€í™”
í•´ê²°ì±…:

ì„ê³„ê°’ ë™ì  ì¡°ì •
ì»¨í…ìŠ¤íŠ¸ ì •ë³´ ì¶”ê°€ ê³ ë ¤
ë‹¤ë‹¨ê³„ ê²€ì¦ ë¡œì§ ì ìš©

# ì—°ë½ì²˜ ë° ì§€ì›
# ê¸´ê¸‰ ìƒí™©

24/7 ì§€ì› í•«ë¼ì¸: 1588-0000
ì´ë©”ì¼: emergency@company.com
Slack: #fire-prediction-emergency

ì¼ë°˜ ì§€ì›

ê¸°ìˆ  ì§€ì›: tech-support@company.com
ë¬¸ì„œ ë° êµìœ¡: docs@company.com
í”¼ë“œë°±: feedback@company.com

ê°œë°œíŒ€

ì‹œìŠ¤í…œ ì•„í‚¤í…íŠ¸: architect@company.com
ML ì—”ì§€ë‹ˆì–´: ml-team@company.com
DevOps: devops@company.com


## ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ë° ê²€ì¦

### ë‹¨ìœ„ í…ŒìŠ¤íŠ¸

```python
import unittest
import json
from unittest.mock import patch, MagicMock
from lambda_function import lambda_handler, DiscreteDataProcessor

class TestFirePredictionSystem(unittest.TestCase):
    
    def setUp(self):
        self.test_discrete_data = {
            'deviceId': 'TEST001',
            'idx': 1,
            'status': 0,  # ì •ìƒ ìƒíƒœ
            'volt': 220.0,
            'voltLow': 220.0,
            'ct': 5.0,
            'zct': 0.5,
            'rzct': 0.3,
            'pwr': 1.1,
            'pcbTemp': 35.0,
            'sensorTemp': 37.0,
            'co': 5.0,
            'voc': 50.0,
            'pFct': 0.95,
            'aArc': 0,
            'timestamp': '2024-01-01T12:00:00Z'
        }
    
    def test_discrete_data_parsing(self):
        """Discrete ë°ì´í„° íŒŒì‹± í…ŒìŠ¤íŠ¸"""
        processor = DiscreteDataProcessor()
        result = processor.parse_discrete_data(self.test_discrete_data)
        
        self.assertIsNotNone(result)
        self.assertEqual(result['device_id'], 'TEST001')
        self.assertEqual(result['voltage'], 220.0)
        self.assertEqual(result['current'], 5.0)
    
    def test_status_bit_parsing(self):
        """ìƒíƒœ ë¹„íŠ¸ íŒŒì‹± í…ŒìŠ¤íŠ¸"""
        processor = DiscreteDataProcessor()
        
        # ê³¼ì „ì•• ì•ŒëŒ í…ŒìŠ¤íŠ¸ (bit 0)
        test_data = self.test_discrete_data.copy()
        test_data['status'] = 1  # bit 0 set
        
        result = processor.parse_discrete_data(test_data)
        self.assertTrue(result['alarm_status']['overvoltage'])
        self.assertFalse(result['alarm_status']['overcurrent'])
    
    def test_risk_calculation(self):
        """ìœ„í—˜ë„ ê³„ì‚° í…ŒìŠ¤íŠ¸"""
        processor = DiscreteDataProcessor()
        
        # ê³ ì˜¨ ìƒí™© í…ŒìŠ¤íŠ¸
        high_temp_data = self.test_discrete_data.copy()
        high_temp_data['pcbTemp'] = 70.0
        high_temp_data['sensorTemp'] = 72.0
        
        result = processor.parse_discrete_data(high_temp_data)
        self.assertGreater(
            result['risk_indicators']['total_risk_score'], 
            0.5
        )
    
    @patch('boto3.resource')
    def test_lambda_handler(self, mock_boto3):
        """Lambda í•¸ë“¤ëŸ¬ í…ŒìŠ¤íŠ¸"""
        # Mock DynamoDB
        mock_dynamodb = MagicMock()
        mock_boto3.return_value = mock_dynamodb
        
        response = lambda_handler(self.test_discrete_data, {})
        
        self.assertEqual(response['statusCode'], 200)
        body = json.loads(response['body'])
        self.assertEqual(body['device_id'], 'TEST001')

class TestMLModel(unittest.TestCase):
    
    def test_model_prediction_range(self):
        """ëª¨ë¸ ì˜ˆì¸¡ê°’ ë²”ìœ„ í…ŒìŠ¤íŠ¸"""
        from FirePredictionMLService import RealTimeInferenceEndpoint
        
        endpoint = RealTimeInferenceEndpoint()
        
        test_data = {
            'volt': 220.0,
            'ct': 5.0,
            'pcbTemp': 45.0,
            'sensorTemp': 47.0,
            'co': 10.0,
            'voc': 100.0,
            'aArc': 0,
            'status': 0
        }
        
        with patch.object(endpoint, 'sagemaker_client') as mock_client:
            mock_response = {
                'Body': MagicMock()
            }
            mock_response['Body'].read.return_value.decode.return_value = json.dumps({
                'fire_probability': 0.3,
                'risk_level': 'MEDIUM',
                'confidence': 0.8
            })
            mock_client.invoke_endpoint.return_value = mock_response
            
            result = endpoint.predict_fire_risk(test_data)
            
            self.assertGreaterEqual(result['fire_probability'], 0.0)
            self.assertLessEqual(result['fire_probability'], 1.0)
            self.assertIn(result['risk_level'], 
                         ['NORMAL', 'LOW', 'MEDIUM', 'HIGH', 'CRITICAL'])

if __name__ == '__main__':
    unittest.main()

# í†µí•© í…ŒìŠ¤íŠ¸

# python

import boto3

import json
import time
from datetime import datetime

class IntegrationTest:
    def __init__(self, environment='dev'):
        self.environment = environment
        self.iot_client = boto3.client('iot-data')
        self.dynamodb = boto3.resource('dynamodb')
        self.kinesis = boto3.client('kinesis')
        
    def test_end_to_end_flow(self):
        """ì „ì²´ ì‹œìŠ¤í…œ End-to-End í…ŒìŠ¤íŠ¸"""
        print("ğŸ§ª E2E í…ŒìŠ¤íŠ¸ ì‹œì‘...")
        
        # 1. í…ŒìŠ¤íŠ¸ ë°ì´í„° ì „ì†¡
        test_device_id = f"TEST_{int(time.time())}"
        test_data = {
            'deviceId': test_device_id,
            'idx': 1,
            'status': 2048,  # ì•„í¬ ì•ŒëŒ
            'volt': 225.0,
            'voltLow': 225.0,
            'ct': 15.0,
            'zct': 2.0,
            'rzct': 1.5,
            'pwr': 3.375,
            'pcbTemp': 55.0,
            'sensorTemp': 58.0,
            'co': 12.0,
            'voc': 200.0,
            'pFct': 0.92,
            'aArc': 3
        }
        
        # IoT Coreë¡œ ë°ì´í„° ì „ì†¡
        topic = f'fire-prediction/device/{test_device_id}/data'
        self.iot_client.publish(
            topic=topic,
            qos=1,
            payload=json.dumps(test_data)
        )
        
        print(f"âœ… í…ŒìŠ¤íŠ¸ ë°ì´í„° ì „ì†¡ ì™„ë£Œ: {test_device_id}")
        
        # 2. ì²˜ë¦¬ ê²°ê³¼ í™•ì¸ (30ì´ˆ ëŒ€ê¸°)
        time.sleep(30)
        
        # DynamoDBì—ì„œ ê²°ê³¼ í™•ì¸
        raw_data_table = self.dynamodb.Table(f'FirePrediction-RawData-{self.environment}')
        device_state_table = self.dynamodb.Table(f'FirePrediction-DeviceState-{self.environment}')
        
        # ì›ì‹œ ë°ì´í„° ì €ì¥ í™•ì¸
        response = raw_data_table.query(
            KeyConditionExpression=Key('device_id').eq(test_device_id),
            ScanIndexForward=False,
            Limit=1
        )
        
        assert len(response['Items']) > 0, "ì›ì‹œ ë°ì´í„°ê°€ ì €ì¥ë˜ì§€ ì•ŠìŒ"
        print("âœ… ì›ì‹œ ë°ì´í„° ì €ì¥ í™•ì¸")
        
        # ë””ë°”ì´ìŠ¤ ìƒíƒœ ì—…ë°ì´íŠ¸ í™•ì¸
        device_response = device_state_table.get_item(
            Key={'device_id': test_device_id}
        )
        
        assert 'Item' in device_response, "ë””ë°”ì´ìŠ¤ ìƒíƒœê°€ ì—…ë°ì´íŠ¸ë˜ì§€ ì•ŠìŒ"
        print("âœ… ë””ë°”ì´ìŠ¤ ìƒíƒœ ì—…ë°ì´íŠ¸ í™•ì¸")
        
        # 3. Kinesis ìŠ¤íŠ¸ë¦¼ í™•ì¸
        stream_name = f'fire-prediction-stream-{self.environment}'
        
        # ê°„ë‹¨í•œ Kinesis ìƒíƒœ í™•ì¸
        stream_desc = self.kinesis.describe_stream(StreamName=stream_name)
        assert stream_desc['StreamDescription']['StreamStatus'] == 'ACTIVE'
        print("âœ… Kinesis ìŠ¤íŠ¸ë¦¼ í™œì„± ìƒíƒœ í™•ì¸")
        
        print("ğŸ‰ E2E í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        
        return test_device_id
    
    def test_high_load_scenario(self, num_devices=100):
        """ê³ ë¶€í•˜ ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸"""
        print(f"âš¡ ê³ ë¶€í•˜ í…ŒìŠ¤íŠ¸ ì‹œì‘ ({num_devices}ê°œ ë””ë°”ì´ìŠ¤)...")
        
        import concurrent.futures
        import random
        
        def send_test_data(device_num):
            test_data = {
                'deviceId': f'LOAD_TEST_{device_num}',
                'idx': random.randint(1, 1000),
                'status': random.choice([0, 1, 2, 4, 8, 16, 32, 64]),
                'volt': random.uniform(200, 240),
                'voltLow': random.uniform(180, 200),
                'ct': random.uniform(0, 50),
                'zct': random.uniform(0, 10),
                'rzct': random.uniform(0, 5),
                'pwr': random.uniform(0, 10),
                'pcbTemp': random.uniform(20, 80),
                'sensorTemp': random.uniform(20, 80),
                'co': random.uniform(0, 30),
                'voc': random.uniform(0, 1000),
                'pFct': random.uniform(0.7, 1.0),
                'aArc': random.randint(0, 10)
            }
            
            topic = f'fire-prediction/device/LOAD_TEST_{device_num}/data'
            self.iot_client.publish(
                topic=topic,
                qos=1,
                payload=json.dumps(test_data)
            )
            
            return device_num
        
        # ë³‘ë ¬ë¡œ ë°ì´í„° ì „ì†¡
        with concurrent.futures.ThreadPoolExecutor(max_workers=20) as executor:
            futures = [executor.submit(send_test_data, i) for i in range(num_devices)]
            
            for future in concurrent.futures.as_completed(futures):
                device_num = future.result()
                if device_num % 10 == 0:
                    print(f"  ğŸ“¤ {device_num}/{num_devices} ì „ì†¡ ì™„ë£Œ")
        
        print("âœ… ê³ ë¶€í•˜ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì „ì†¡ ì™„ë£Œ")
        
        # ì‹œìŠ¤í…œ ì‘ë‹µ ì‹œê°„ ëª¨ë‹ˆí„°ë§
        time.sleep(60)  # 1ë¶„ ëŒ€ê¸°
        
        # CloudWatch ë©”íŠ¸ë¦­ í™•ì¸
        cloudwatch = boto3.client('cloudwatch')
        
        end_time = datetime.now()
        start_time = end_time - timedelta(minutes=5)
        
        # Lambda ì˜¤ë¥˜ìœ¨ í™•ì¸
        response = cloudwatch.get_metric_statistics(
            Namespace='AWS/Lambda',
            MetricName='Errors',
            Dimensions=[
                {'Name': 'FunctionName', 'Value': f'fire-prediction-processor-{self.environment}'}
            ],
            StartTime=start_time,
            EndTime=end_time,
            Period=300,
            Statistics=['Sum']
        )
        
        total_errors = sum(point['Sum'] for point in response['Datapoints'])
        error_rate = (total_errors / num_devices) * 100
        
        assert error_rate < 5, f"ì˜¤ë¥˜ìœ¨ì´ ë„ˆë¬´ ë†’ìŒ: {error_rate}%"
        print(f"âœ… ì˜¤ë¥˜ìœ¨ í™•ì¸: {error_rate:.2f}%")
        
        print("ğŸ‰ ê³ ë¶€í•˜ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")

if __name__ == "__main__":
    test = IntegrationTest('dev')
    test.test_end_to_end_flow()
    test.test_high_load_scenario(50)

# ì´ì œ AWS IoT Core ê¸°ë°˜ í™”ì¬ ì˜ˆì¸¡ ì‹œìŠ¤í…œì˜ ì „ì²´ êµ¬í˜„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. 
# ì‹œìŠ¤í…œì€ Discrete í´ë˜ìŠ¤ì˜ ë°ì´í„° êµ¬ì¡°ë¥¼ ì™„ë²½í•˜ê²Œ ì§€ì›í•˜ë©°, ì‹¤ì‹œê°„ ë°ì´í„° ì²˜ë¦¬, ë¨¸ì‹ ëŸ¬ë‹ ê¸°ë°˜ ì˜ˆì¸¡, ëª¨ë‹ˆí„°ë§ ë° ì•Œë¦¼ ê¸°ëŠ¥ì„ ëª¨ë‘ í¬í•¨í•©ë‹ˆë‹¤.